#!/usr/bin/env python3
"""
Step 3: ç®€åŒ–çš„å‚æ•°ä¼˜åŒ–
ä¿®å¤äº†PyTorchå…¼å®¹æ€§é—®é¢˜çš„ç‰ˆæœ¬ï¼Œä¸“æ³¨äºæ ¸å¿ƒçš„å‚æ•°æµ‹è¯•
"""

import torch
import torch.nn.functional as F
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import librosa
from pathlib import Path
import time
from diffusers import AudioLDM2Pipeline
from typing import Dict, Tuple, List
import warnings
warnings.filterwarnings("ignore")

class SimpleParameterOptimizer:
    """
    ç®€åŒ–çš„å‚æ•°ä¼˜åŒ–å™¨
    é¿å…å¤æ‚çš„PyTorchæ“ä½œï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½
    """
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸ¯ åˆå§‹åŒ–ç®€åŒ–å‚æ•°ä¼˜åŒ–å™¨")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        
        # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        print(f"âœ… ç®€åŒ–å‚æ•°ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("simplified_parameter_optimization")
        self.output_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def optimize_frequency_boost_parameters(self, audio_path: str) -> Dict:
        """
        ä¼˜åŒ–é¢‘ç‡å¢å¼ºå‚æ•°
        æµ‹è¯•ä¸åŒçš„å¢å¼ºç³»æ•°ï¼Œæ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹
        """
        print(f"\nğŸ”¬ å¼€å§‹é¢‘ç‡å¢å¼ºå‚æ•°ä¼˜åŒ–: {Path(audio_path).name}")
        
        # åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘ï¼ˆåªåšä¸€æ¬¡ï¼‰
        original_audio, processed_audio = self._load_and_preprocess_audio(audio_path)
        latent = self._encode_audio(processed_audio)
        vae_only_audio = self._decode_audio(latent.clone())
        
        # å®šä¹‰æµ‹è¯•çš„å¢å¼ºç³»æ•°
        boost_factors = [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.8, 2.0, 2.5, 3.0]
        
        results = []
        timestamp = int(time.time())
        
        print(f"ğŸ§ª æµ‹è¯• {len(boost_factors)} ç§å¢å¼ºç³»æ•°...")
        
        for i, boost_factor in enumerate(boost_factors):
            print(f"\n   ğŸ”¬ æµ‹è¯• {i+1}/{len(boost_factors)}: å¢å¼ºç³»æ•° {boost_factor}x")
            
            # åº”ç”¨å¢å¼º
            enhanced_latent = self._apply_simple_frequency_boost(latent.clone(), boost_factor)
            enhanced_audio = self._decode_audio(enhanced_latent)
            
            # åˆ†æè´¨é‡
            quality_metrics = self._analyze_quality(original_audio, vae_only_audio, enhanced_audio)
            freq_metrics = self._analyze_frequency(original_audio, vae_only_audio, enhanced_audio)
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            composite_score = self._calculate_composite_score(quality_metrics, freq_metrics)
            
            # ä¿å­˜å…³é”®é…ç½®çš„éŸ³é¢‘
            audio_paths = None
            if boost_factor in [1.2, 1.5, 2.0] or composite_score > 6.0:  # ä¿å­˜å…³é”®é…ç½®
                audio_paths = self._save_configuration_audio(
                    original_audio, vae_only_audio, enhanced_audio, 
                    timestamp, f"boost_{boost_factor}"
                )
            
            result = {
                "boost_factor": boost_factor,
                "quality_metrics": quality_metrics,
                "frequency_metrics": freq_metrics,
                "composite_score": composite_score,
                "audio_paths": audio_paths
            }
            
            results.append(result)
            
            print(f"      ğŸ“Š ç»¼åˆå¾—åˆ†: {composite_score:.2f}")
            print(f"      ğŸ¼ é«˜é¢‘æ”¹è¿›: {freq_metrics['improvements']['high_freq_improvement']*100:+.1f}%")
            print(f"      ğŸ“ˆ SNRæ”¹è¿›: {quality_metrics['improvements']['snr_improvement']:+.2f} dB")
        
        # æ‰¾åˆ°æœ€ä½³é…ç½®
        best_result = max(results, key=lambda x: x['composite_score'])
        
        # åˆ›å»ºåˆ†ææŠ¥å‘Š
        self._create_analysis_report(results, best_result, timestamp)
        
        # åˆ›å»ºå¯è§†åŒ–
        self._create_analysis_visualizations(results, timestamp)
        
        optimization_result = {
            "input_file": audio_path,
            "timestamp": timestamp,
            "all_results": results,
            "best_result": best_result,
            "total_configs_tested": len(boost_factors)
        }
        
        self._display_results(optimization_result)
        
        return optimization_result
    
    def _apply_simple_frequency_boost(self, latent: torch.Tensor, boost_factor: float) -> torch.Tensor:
        """
        åº”ç”¨ç®€å•çš„é¢‘ç‡å¢å¼º
        ä½¿ç”¨åŸºç¡€çš„Laplacianç®—å­ï¼Œé¿å…å¤æ‚çš„strideæ“ä½œ
        """
        with torch.no_grad():
            # ç®€å•çš„Laplacianæ ¸
            laplacian = torch.tensor([
                [[-1, -1, -1],
                 [-1,  8, -1],
                 [-1, -1, -1]]
            ], dtype=latent.dtype, device=latent.device).unsqueeze(0).unsqueeze(0)
            
            enhanced_latent = latent.clone()
            
            # å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«å¤„ç†
            for c in range(latent.shape[1]):
                channel_latent = latent[:, c:c+1, :, :]  # [B, 1, H, W]
                
                # åº”ç”¨é«˜é¢‘æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…strideé—®é¢˜ï¼‰
                # æ‰‹åŠ¨å®ç°padding
                padded_channel = F.pad(channel_latent, (1, 1, 1, 1), mode='reflect')
                
                # æ‰‹åŠ¨å·ç§¯ï¼ˆé¿å…strideå‚æ•°é—®é¢˜ï¼‰
                high_freq_response = F.conv2d(padded_channel, laplacian, padding=0)
                
                # ç¡®ä¿è¾“å‡ºå°ºå¯¸æ­£ç¡®
                if high_freq_response.shape != channel_latent.shape:
                    high_freq_response = F.interpolate(
                        high_freq_response, 
                        size=channel_latent.shape[-2:], 
                        mode='bilinear', 
                        align_corners=False
                    )
                
                # åº”ç”¨å¢å¼º
                enhanced_channel = channel_latent + high_freq_response * (boost_factor - 1.0)
                enhanced_latent[:, c:c+1, :, :] = enhanced_channel
            
            return enhanced_latent
    
    def _calculate_composite_score(self, quality_metrics: Dict, freq_metrics: Dict) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        q_improvements = quality_metrics['improvements']
        f_improvements = freq_metrics['improvements']
        
        # æƒé‡è®¾è®¡ï¼ˆé«˜é¢‘æ¢å¤æœ€é‡è¦ï¼‰
        weights = {
            'snr': 0.2,
            'correlation': 0.3,
            'high_freq': 0.4,
            'overall_freq': 0.1
        }
        
        # å½’ä¸€åŒ–å¾—åˆ†
        snr_score = max(0, min(10, (q_improvements['snr_improvement'] + 5) * 2))
        corr_score = max(0, min(10, q_improvements['correlation_improvement'] * 100))
        high_freq_score = max(0, min(10, f_improvements['high_freq_improvement'] * 10))
        overall_freq_score = max(0, min(10, f_improvements['frequency_correlation_improvement'] * 20))
        
        composite_score = (
            weights['snr'] * snr_score +
            weights['correlation'] * corr_score +
            weights['high_freq'] * high_freq_score +
            weights['overall_freq'] * overall_freq_score
        )
        
        return composite_score
    
    def _load_and_preprocess_audio(self, audio_path: str) -> Tuple[np.ndarray, torch.Tensor]:
        """åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘"""
        print(f"   ğŸ“‚ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
        
        original_audio, sr = torchaudio.load(audio_path)
        
        if original_audio.shape[0] > 1:
            original_audio = original_audio.mean(dim=0, keepdim=True)
        
        if sr != 48000:
            resampler = torchaudio.transforms.Resample(sr, 48000)
            processed_audio = resampler(original_audio)
        else:
            processed_audio = original_audio.clone()
        
        max_length = 48000 * 10
        if processed_audio.shape[-1] > max_length:
            processed_audio = processed_audio[..., :max_length]
        
        original_audio_np = original_audio.squeeze().numpy()
        return original_audio_np, processed_audio
    
    def _encode_audio(self, audio: torch.Tensor) -> torch.Tensor:
        """VAEç¼–ç """
        with torch.no_grad():
            audio_np = audio.squeeze().numpy()
            inputs = self.pipeline.feature_extractor(
                audio_np, sampling_rate=48000, return_tensors="pt"
            )
            mel_features = inputs["input_features"].to(self.device)
            
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode()
            latent = latent * self.pipeline.vae.config.scaling_factor
            
            return latent
    
    def _decode_audio(self, latent: torch.Tensor) -> np.ndarray:
        """VAEè§£ç """
        with torch.no_grad():
            latent_for_decode = latent / self.pipeline.vae.config.scaling_factor
            vae_dtype = next(self.pipeline.vae.parameters()).dtype
            latent_for_decode = latent_for_decode.to(vae_dtype)
            
            mel_spectrogram = self.pipeline.vae.decode(latent_for_decode).sample
            audio_tensor = self.pipeline.mel_spectrogram_to_waveform(mel_spectrogram)
            audio_np = audio_tensor.squeeze().cpu().numpy()
            
            return audio_np
    
    def _analyze_quality(self, original: np.ndarray, vae_only: np.ndarray, enhanced: np.ndarray) -> Dict:
        """è´¨é‡åˆ†æ"""
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        def calc_metrics(orig, recon):
            min_len = min(len(orig), len(recon))
            o, r = orig[:min_len], recon[:min_len]
            
            mse = np.mean((o - r) ** 2)
            snr = 10 * np.log10(np.mean(o ** 2) / (mse + 1e-10))
            correlation = np.corrcoef(o, r)[0, 1] if min_len > 1 else 0.0
            if np.isnan(correlation):
                correlation = 0.0
            
            return {"snr_db": snr, "correlation": correlation, "mse": mse}
        
        vae_metrics = calc_metrics(original_16k, vae_only)
        enhanced_metrics = calc_metrics(original_16k, enhanced)
        
        improvements = {
            "snr_improvement": enhanced_metrics["snr_db"] - vae_metrics["snr_db"],
            "correlation_improvement": enhanced_metrics["correlation"] - vae_metrics["correlation"],
            "mse_improvement": vae_metrics["mse"] - enhanced_metrics["mse"]
        }
        
        return {
            "vae_only": vae_metrics,
            "enhanced": enhanced_metrics,
            "improvements": improvements
        }
    
    def _analyze_frequency(self, original: np.ndarray, vae_only: np.ndarray, enhanced: np.ndarray) -> Dict:
        """é¢‘ç‡åˆ†æ"""
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        def analyze_bands(orig, recon):
            min_len = min(len(orig), len(recon))
            o, r = orig[:min_len], recon[:min_len]
            
            n_fft = 8192 if min_len >= 8192 else 2 ** int(np.log2(min_len))
            orig_fft = np.abs(np.fft.fft(o[:n_fft]))[:n_fft//2]
            recon_fft = np.abs(np.fft.fft(r[:n_fft]))[:n_fft//2]
            freqs = np.fft.fftfreq(n_fft, 1/16000)[:n_fft//2]
            
            low_mask = freqs < 500
            mid_mask = (freqs >= 500) & (freqs < 4000)
            high_mask = freqs >= 4000
            
            low_retention = np.sum(recon_fft[low_mask]) / (np.sum(orig_fft[low_mask]) + 1e-10)
            mid_retention = np.sum(recon_fft[mid_mask]) / (np.sum(orig_fft[mid_mask]) + 1e-10)
            high_retention = np.sum(recon_fft[high_mask]) / (np.sum(orig_fft[high_mask]) + 1e-10)
            
            freq_corr = np.corrcoef(orig_fft, recon_fft)[0, 1]
            if np.isnan(freq_corr):
                freq_corr = 0.0
            
            return {
                "low_freq_retention": low_retention,
                "mid_freq_retention": mid_retention,
                "high_freq_retention": high_retention,
                "frequency_correlation": freq_corr
            }
        
        vae_freq = analyze_bands(original_16k, vae_only)
        enhanced_freq = analyze_bands(original_16k, enhanced)
        
        improvements = {
            "low_freq_improvement": enhanced_freq["low_freq_retention"] - vae_freq["low_freq_retention"],
            "mid_freq_improvement": enhanced_freq["mid_freq_retention"] - vae_freq["mid_freq_retention"],
            "high_freq_improvement": enhanced_freq["high_freq_retention"] - vae_freq["high_freq_retention"],
            "frequency_correlation_improvement": enhanced_freq["frequency_correlation"] - vae_freq["frequency_correlation"]
        }
        
        return {
            "vae_only": vae_freq,
            "enhanced": enhanced_freq,
            "improvements": improvements
        }
    
    def _save_configuration_audio(self,
                                original: np.ndarray,
                                vae_only: np.ndarray,
                                enhanced: np.ndarray,
                                timestamp: int,
                                config_name: str) -> Dict[str, str]:
        """ä¿å­˜ç‰¹å®šé…ç½®çš„éŸ³é¢‘"""
        
        paths = {}
        safe_name = config_name.replace(".", "_")
        
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        original_path = self.output_dir / f"original_{safe_name}_{timestamp}.wav"
        vae_path = self.output_dir / f"vae_only_{safe_name}_{timestamp}.wav"
        enhanced_path = self.output_dir / f"enhanced_{safe_name}_{timestamp}.wav"
        
        sf.write(str(original_path), original_16k, 16000)
        sf.write(str(vae_path), vae_only, 16000)
        sf.write(str(enhanced_path), enhanced, 16000)
        
        paths["original"] = str(original_path)
        paths["vae_only"] = str(vae_path)
        paths["enhanced"] = str(enhanced_path)
        
        return paths
    
    def _create_analysis_report(self, results: List[Dict], best_result: Dict, timestamp: int):
        """åˆ›å»ºåˆ†ææŠ¥å‘Š"""
        
        report_path = self.output_dir / f"boost_factor_analysis_{timestamp}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# é¢‘ç‡å¢å¼ºç³»æ•°ä¼˜åŒ–æŠ¥å‘Š\n\n")
            f.write(f"## æµ‹è¯•æ¦‚å†µ\n")
            f.write(f"- æµ‹è¯•é…ç½®æ•°é‡: {len(results)}\n")
            f.write(f"- æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n\n")
            
            f.write("## æœ€ä½³é…ç½®\n")
            f.write(f"- **æœ€ä½³å¢å¼ºç³»æ•°**: {best_result['boost_factor']}x\n")
            f.write(f"- **ç»¼åˆå¾—åˆ†**: {best_result['composite_score']:.2f}\n")
            f.write(f"- **SNRæ”¹è¿›**: {best_result['quality_metrics']['improvements']['snr_improvement']:+.2f} dB\n")
            f.write(f"- **é«˜é¢‘æ”¹è¿›**: {best_result['frequency_metrics']['improvements']['high_freq_improvement']*100:+.1f}%\n\n")
            
            f.write("## æ‰€æœ‰é…ç½®ç»“æœ\n\n")
            f.write("| å¢å¼ºç³»æ•° | SNRæ”¹è¿›(dB) | é«˜é¢‘æ”¹è¿›(%) | ç›¸å…³æ€§æ”¹è¿› | ç»¼åˆå¾—åˆ† |\n")
            f.write("|----------|-------------|-------------|------------|----------|\n")
            
            # æŒ‰å¾—åˆ†æ’åº
            sorted_results = sorted(results, key=lambda x: x['composite_score'], reverse=True)
            
            for result in sorted_results:
                boost = result['boost_factor']
                quality = result['quality_metrics']['improvements']
                freq = result['frequency_metrics']['improvements']
                score = result['composite_score']
                
                f.write(f"| {boost}x | {quality['snr_improvement']:+.2f} | "
                       f"{freq['high_freq_improvement']*100:+.1f} | "
                       f"{quality['correlation_improvement']:+.4f} | "
                       f"{score:.2f} |\n")
            
            f.write("\n## å…³é”®å‘ç°\n\n")
            
            # æ‰¾åˆ°æœ€ä½³èŒƒå›´
            high_scores = [r for r in results if r['composite_score'] > 6.0]
            if high_scores:
                boost_range = [r['boost_factor'] for r in high_scores]
                f.write(f"### æœ€ä½³å¢å¼ºç³»æ•°èŒƒå›´\n")
                f.write(f"- é«˜åˆ†é…ç½®çš„å¢å¼ºç³»æ•°èŒƒå›´: {min(boost_range):.1f}x - {max(boost_range):.1f}x\n")
                f.write(f"- å»ºè®®ä½¿ç”¨: {best_result['boost_factor']}x (æœ€é«˜å¾—åˆ†)\n\n")
            
            # è¶‹åŠ¿åˆ†æ
            f.write("### è¶‹åŠ¿åˆ†æ\n")
            boost_factors = [r['boost_factor'] for r in results]
            scores = [r['composite_score'] for r in results]
            
            # æ‰¾åˆ°å¾—åˆ†çš„å³°å€¼
            max_score_idx = scores.index(max(scores))
            optimal_boost = boost_factors[max_score_idx]
            
            f.write(f"- æœ€ä¼˜å¢å¼ºç³»æ•°: {optimal_boost}x\n")
            
            # åˆ†æè¿‡åº¦å¢å¼º
            over_enhanced = [r for r in results if r['boost_factor'] > optimal_boost and r['composite_score'] < best_result['composite_score']]
            if over_enhanced:
                f.write(f"- è¿‡åº¦å¢å¼ºé˜ˆå€¼: >{optimal_boost}x åæ€§èƒ½ä¸‹é™\n")
            
        print(f"   ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def _create_analysis_visualizations(self, results: List[Dict], timestamp: int):
        """åˆ›å»ºåˆ†æå¯è§†åŒ–"""
        
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('é¢‘ç‡å¢å¼ºç³»æ•°ä¼˜åŒ–åˆ†æ', fontsize=16, fontweight='bold')
            
            # æå–æ•°æ®
            boost_factors = [r['boost_factor'] for r in results]
            composite_scores = [r['composite_score'] for r in results]
            snr_improvements = [r['quality_metrics']['improvements']['snr_improvement'] for r in results]
            high_freq_improvements = [r['frequency_metrics']['improvements']['high_freq_improvement'] * 100 for r in results]
            
            # 1. ç»¼åˆå¾—åˆ† vs å¢å¼ºç³»æ•°
            axes[0, 0].plot(boost_factors, composite_scores, 'o-', linewidth=2, markersize=8)
            axes[0, 0].set_title('ç»¼åˆå¾—åˆ† vs å¢å¼ºç³»æ•°')
            axes[0, 0].set_xlabel('å¢å¼ºç³»æ•°')
            axes[0, 0].set_ylabel('ç»¼åˆå¾—åˆ†')
            axes[0, 0].grid(True, alpha=0.3)
            
            # æ ‡è®°æœ€ä½³ç‚¹
            best_idx = composite_scores.index(max(composite_scores))
            axes[0, 0].scatter([boost_factors[best_idx]], [composite_scores[best_idx]], 
                             color='red', s=100, zorder=5, label='æœ€ä½³é…ç½®')
            axes[0, 0].legend()
            
            # 2. SNRæ”¹è¿› vs å¢å¼ºç³»æ•°
            axes[0, 1].plot(boost_factors, snr_improvements, 's-', linewidth=2, markersize=6, color='orange')
            axes[0, 1].set_title('SNRæ”¹è¿› vs å¢å¼ºç³»æ•°')
            axes[0, 1].set_xlabel('å¢å¼ºç³»æ•°')
            axes[0, 1].set_ylabel('SNRæ”¹è¿› (dB)')
            axes[0, 1].grid(True, alpha=0.3)
            axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)
            
            # 3. é«˜é¢‘æ”¹è¿› vs å¢å¼ºç³»æ•°
            axes[1, 0].plot(boost_factors, high_freq_improvements, '^-', linewidth=2, markersize=6, color='green')
            axes[1, 0].set_title('é«˜é¢‘æ”¹è¿› vs å¢å¼ºç³»æ•°')
            axes[1, 0].set_xlabel('å¢å¼ºç³»æ•°')
            axes[1, 0].set_ylabel('é«˜é¢‘æ”¹è¿› (%)')
            axes[1, 0].grid(True, alpha=0.3)
            
            # 4. SNR vs é«˜é¢‘æ”¹è¿›æ•£ç‚¹å›¾
            axes[1, 1].scatter(snr_improvements, high_freq_improvements, s=100, alpha=0.7)
            for i, boost in enumerate(boost_factors):
                axes[1, 1].annotate(f'{boost}x', (snr_improvements[i], high_freq_improvements[i]), 
                                  xytext=(5, 5), textcoords='offset points', fontsize=8)
            axes[1, 1].set_title('SNRæ”¹è¿› vs é«˜é¢‘æ”¹è¿›')
            axes[1, 1].set_xlabel('SNRæ”¹è¿› (dB)')
            axes[1, 1].set_ylabel('é«˜é¢‘æ”¹è¿› (%)')
            axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            plot_path = self.output_dir / f"boost_factor_analysis_{timestamp}.png"
            plt.savefig(str(plot_path), dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"   ğŸ“Š åˆ†æå¯è§†åŒ–å·²ä¿å­˜: {plot_path}")
            
        except Exception as e:
            print(f"   âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    def _display_results(self, result: Dict):
        """æ˜¾ç¤ºç»“æœ"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ é¢‘ç‡å¢å¼ºç³»æ•°ä¼˜åŒ–å®Œæˆï¼")
        print(f"{'='*80}")
        
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {result['input_file']}")
        print(f"ğŸ§ª æµ‹è¯•é…ç½®æ•°é‡: {result['total_configs_tested']}")
        
        best = result['best_result']
        best_quality = best['quality_metrics']['improvements']
        best_freq = best['frequency_metrics']['improvements']
        
        print(f"\nğŸ† æœ€ä½³é…ç½®:")
        print(f"   ğŸ“ˆ æœ€ä½³å¢å¼ºç³»æ•°: {best['boost_factor']}x")
        print(f"   ğŸ… ç»¼åˆå¾—åˆ†: {best['composite_score']:.2f}/10")
        
        print(f"\nğŸ“Š æœ€ä½³é…ç½®æ€§èƒ½:")
        print(f"   ğŸ“ˆ SNRæ”¹è¿›: {best_quality['snr_improvement']:+.2f} dB")
        print(f"   ğŸ”— ç›¸å…³æ€§æ”¹è¿›: {best_quality['correlation_improvement']:+.4f}")
        print(f"   ğŸ¼ é«˜é¢‘æ”¹è¿›: {best_freq['high_freq_improvement']*100:+.1f}%")
        
        # Top 3é…ç½®
        sorted_results = sorted(result['all_results'], key=lambda x: x['composite_score'], reverse=True)
        print(f"\nğŸ¥‡ Top 3 å¢å¼ºç³»æ•°:")
        for i, r in enumerate(sorted_results[:3]):
            boost = r['boost_factor']
            score = r['composite_score']
            high_freq = r['frequency_metrics']['improvements']['high_freq_improvement'] * 100
            print(f"   {i+1}. {boost}x: å¾—åˆ† {score:.2f}, é«˜é¢‘æ”¹è¿› {high_freq:+.1f}%")
        
        # è¶‹åŠ¿åˆ†æ
        print(f"\nğŸ“ˆ è¶‹åŠ¿åˆ†æ:")
        
        high_performers = [r for r in result['all_results'] if r['composite_score'] > 6.0]
        if high_performers:
            boost_range = [r['boost_factor'] for r in high_performers]
            print(f"   âœ… æ¨èå¢å¼ºç³»æ•°èŒƒå›´: {min(boost_range):.1f}x - {max(boost_range):.1f}x")
        
        # æ£€æŸ¥è¿‡åº¦å¢å¼º
        best_boost = best['boost_factor']
        over_enhanced = [r for r in result['all_results'] 
                        if r['boost_factor'] > best_boost and 
                        r['composite_score'] < best['composite_score'] * 0.9]
        
        if over_enhanced:
            min_over = min([r['boost_factor'] for r in over_enhanced])
            print(f"   âš ï¸ è¿‡åº¦å¢å¼ºé˜ˆå€¼: >{min_over:.1f}x åæ•ˆæœä¸‹é™")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   ğŸ“„ åˆ†ææŠ¥å‘Š: boost_factor_analysis_{result['timestamp']}.md")
        print(f"   ğŸ“Š å¯è§†åŒ–å›¾è¡¨: boost_factor_analysis_{result['timestamp']}.png")
        
        if best['audio_paths']:
            print(f"   ğŸµ æœ€ä½³é…ç½®éŸ³é¢‘:")
            for name, path in best['audio_paths'].items():
                print(f"      {name}: {Path(path).name}")

def demo_simplified_optimization():
    """æ¼”ç¤ºç®€åŒ–å‚æ•°ä¼˜åŒ–"""
    print("ğŸ¯ Step 3: ç®€åŒ–å‚æ•°ä¼˜åŒ–")
    print("=" * 50)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        print("è¯·ç¡®ä¿AudioLDM2_Music_output.wavåœ¨å½“å‰ç›®å½•ä¸­")
        return
    
    # åˆå§‹åŒ–ç®€åŒ–ä¼˜åŒ–å™¨
    optimizer = SimpleParameterOptimizer()
    
    # æ‰§è¡Œé¢‘ç‡å¢å¼ºå‚æ•°ä¼˜åŒ–
    result = optimizer.optimize_frequency_boost_parameters(input_file)
    
    print(f"\nâœ… ç®€åŒ–å‚æ•°ä¼˜åŒ–å®Œæˆï¼")
    print(f"ğŸ“Š æŸ¥çœ‹è¾“å‡ºç›®å½•: simplified_parameter_optimization/")
    print(f"ğŸµ è¯•å¬ä¸åŒå¢å¼ºç³»æ•°çš„éŸ³é¢‘æ•ˆæœ")
    print(f"ğŸ“„ æŸ¥çœ‹è¯¦ç»†çš„åˆ†ææŠ¥å‘Š")
    
    print(f"\nğŸ’¡ æ€»ç»“:")
    print(f"   ğŸ¯ æˆ‘ä»¬ç³»ç»Ÿæ€§åœ°æµ‹è¯•äº†ä¸åŒçš„é¢‘ç‡å¢å¼ºç³»æ•°")
    print(f"   ğŸ“ˆ æ‰¾åˆ°äº†è´¨é‡å’Œé«˜é¢‘æ¢å¤çš„æœ€ä½³å¹³è¡¡ç‚¹")
    print(f"   ğŸ”§ ç°åœ¨å¯ä»¥ä½¿ç”¨æœ€ä½³å‚æ•°é…ç½®å¤„ç†éŸ³é¢‘")

if __name__ == "__main__":
    demo_simplified_optimization()
