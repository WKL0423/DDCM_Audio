#!/usr/bin/env python3
"""
AudioLDM2 VAE + HiFiGAN ç»ˆæä¿®å¤ç‰ˆæœ¬
è‡ªåŠ¨å¤„ç†AudioLDM2_Music_output.wavæ–‡ä»¶
ä¿®å¤æ‰€æœ‰å·²çŸ¥é—®é¢˜ï¼šVAE scalingã€ClapFeatureExtractorã€HiFiGANè¾“å…¥æ ¼å¼
"""

import sys
import torch
import numpy as np
import librosa
from pathlib import Path
import time
import soundfile as sf
from diffusers import AudioLDM2Pipeline
import os

def save_audio_compatible(audio, path, sr=16000):
    """å…¼å®¹çš„éŸ³é¢‘ä¿å­˜å‡½æ•°"""
    try:
        # ç¡®ä¿éŸ³é¢‘æ˜¯numpyæ•°ç»„
        if isinstance(audio, torch.Tensor):
            audio = audio.cpu().numpy()
        
        # ç¡®ä¿éŸ³é¢‘æ˜¯ä¸€ç»´çš„
        if len(audio.shape) > 1:
            audio = audio.squeeze()
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        if audio.max() > 1.0 or audio.min() < -1.0:
            audio = audio / np.max(np.abs(audio))
        
        # ä¿å­˜ä¸ºWAVæ–‡ä»¶
        sf.write(path, audio, sr)
        print(f"   ğŸ’¾ ä¿å­˜æˆåŠŸ: {path}")
        return True
        
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥ {path}: {e}")
        return False

def test_audioldm2_ultimate_fix(audio_path, max_length=10.0):
    """
    AudioLDM2 VAE + HiFiGAN ç»ˆæä¿®å¤æµ‹è¯•
    è‡ªåŠ¨å¤„ç†AudioLDM2_Music_output.wavæ–‡ä»¶
    æ·»åŠ å¤šç§æ”¹è¿›æ–¹æ³•ä»¥æé«˜é‡å»ºè´¨é‡
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ğŸ¯ AudioLDM2 æœ€ç»ˆä¿®å¤æµ‹è¯• (æ”¹è¿›ç‰ˆ)")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # åŠ è½½AudioLDM2 pipeline
    print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   VAE scaling_factor: {pipeline.vae.config.scaling_factor}")
    print(f"   Vocoderç±»å‹: {type(pipeline.vocoder).__name__}")
    
    # è·å–feature extractorå‚æ•°
    fe_sr = pipeline.feature_extractor.sampling_rate
    print(f"   FeatureExtractoré‡‡æ ·ç‡: {fe_sr} Hz")
    
    # åŠ è½½éŸ³é¢‘ - ä½¿ç”¨å¤šç§é‡‡æ ·ç‡è¿›è¡Œå¯¹æ¯”
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘: {Path(audio_path).name}")
    
    # ä½¿ç”¨feature extractorçš„é‡‡æ ·ç‡
    audio_fe_sr, sr_fe = librosa.load(audio_path, sr=fe_sr, duration=max_length)
    audio_16k, sr_16k = librosa.load(audio_path, sr=16000, duration=max_length)
    
    print(f"   {fe_sr}HzéŸ³é¢‘: {len(audio_fe_sr)/sr_fe:.2f}ç§’, èŒƒå›´[{audio_fe_sr.min():.3f}, {audio_fe_sr.max():.3f}]")
    print(f"   16kHzéŸ³é¢‘: {len(audio_16k)/sr_16k:.2f}ç§’, èŒƒå›´[{audio_16k.min():.3f}, {audio_16k.max():.3f}]")
    
    # æ–¹æ³•1: ä½¿ç”¨AudioLDM2çš„ClapFeatureExtractorï¼ˆæ”¹è¿›ç‰ˆï¼‰
    print("\nğŸµ æ–¹æ³•1: ä½¿ç”¨AudioLDM2çš„ClapFeatureExtractor (æ”¹è¿›ç‰ˆ)...")
    try:
        # éŸ³é¢‘é¢„å¤„ç† - æ·»åŠ éŸ³é‡å½’ä¸€åŒ–
        audio_input = audio_fe_sr.copy()
        
        # è½»å¾®çš„éŸ³é‡å½’ä¸€åŒ–ï¼Œé¿å…è¿‡åº¦å¤„ç†
        if np.max(np.abs(audio_input)) > 0:
            audio_input = audio_input / np.max(np.abs(audio_input)) * 0.95
        
        # ç¡®ä¿éŸ³é¢‘è¾“å…¥æ˜¯æ­£ç¡®çš„æ ¼å¼
        if len(audio_input.shape) > 1:
            audio_input = audio_input.squeeze()
        
        features = pipeline.feature_extractor(
            audio_input, 
            return_tensors="pt", 
            sampling_rate=fe_sr
        )
        
        mel_input = features.input_features.to(device)
        if device == "cuda":
            mel_input = mel_input.half()
        
        print(f"   âœ… ClapFeatureExtractoræˆåŠŸ")
        print(f"   è¾“å…¥: {mel_input.shape} (æ ¼å¼: [batch, channel, time, feature])")
        print(f"   èŒƒå›´: [{mel_input.min():.3f}, {mel_input.max():.3f}]")
        
        use_clap_features = True
        
    except Exception as e:
        print(f"   âŒ ClapFeatureExtractorå¤±è´¥: {e}")
        use_clap_features = False
    
    # å¦‚æœClapFeatureExtractorå¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„ä¼ ç»Ÿæ–¹æ³•
    if not use_clap_features:
        print("   ğŸ”„ ä½¿ç”¨æ”¹è¿›çš„ä¼ ç»Ÿmelé¢‘è°±å¤„ç†...")
        
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„mel-spectrogramå‚æ•°
        mel_spec = librosa.feature.melspectrogram(
            y=audio_fe_sr, 
            sr=fe_sr, 
            n_mels=64,
            hop_length=int(fe_sr * 0.01),  # 10ms hop length
            n_fft=int(fe_sr * 0.025),      # 25ms window
            fmin=50,
            fmax=fe_sr // 2,  # Nyquist frequency
            window='hann',
            center=True,
            pad_mode='reflect',
            power=2.0  # åŠŸç‡è°±
        )
        
        # æ”¹è¿›çš„dBè½¬æ¢
        mel_db = librosa.power_to_db(mel_spec, ref=np.max, top_db=80)
        
        # æ ‡å‡†åŒ–åˆ°[-1, 1]èŒƒå›´
        mel_db = 2 * (mel_db + 80) / 80 - 1
        
        # è½¬æ¢ä¸ºAudioLDM2æœŸæœ›çš„æ ¼å¼
        mel_tensor = torch.from_numpy(mel_db).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.half()
        
        # ç»´åº¦è½¬æ¢ï¼š[64, time] -> [1, 1, time, 64]
        mel_input = mel_tensor.transpose(0, 1).unsqueeze(0).unsqueeze(0)
        
        print(f"   ä¼ ç»Ÿå¤„ç†: {mel_input.shape}")
        print(f"   èŒƒå›´: [{mel_input.min():.3f}, {mel_input.max():.3f}]")
    
    print(f"   æœ€ç»ˆè¾“å…¥: {mel_input.shape}, {mel_input.dtype}")
    
    # VAEå¤„ç† - æ”¹è¿›ç‰ˆæœ¬
    print("\nğŸ§  VAEç¼–ç è§£ç  (æ”¹è¿›ç‰ˆ)...")
    with torch.no_grad():
        # ç¼–ç 
        latent_dist = pipeline.vae.encode(mel_input)
        
        # æ”¹è¿›1: ä½¿ç”¨æ›´å‡†ç¡®çš„latenté‡‡æ ·
        if hasattr(latent_dist, 'latent_dist'):
            # ä½¿ç”¨modeè€Œä¸æ˜¯sampleå¯èƒ½æ›´ç¨³å®š
            latent = latent_dist.latent_dist.mode()
        else:
            latent = latent_dist.mode() if hasattr(latent_dist, 'mode') else latent_dist.sample()
        
        # å…³é”®ä¿®å¤1: ç¼–ç ååº”ç”¨scaling_factor
        latent = latent * pipeline.vae.config.scaling_factor
        print(f"   ç¼–ç latent: {latent.shape}")
        print(f"   LatentèŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
        
        # æ”¹è¿›2: è½»å¾®çš„latent space regularization
        latent_std = torch.std(latent)
        if latent_std > 3.0:  # å¦‚æœlatentè¿‡äºåˆ†æ•£
            latent = latent * (3.0 / latent_std)
            print(f"   Latentæ­£åˆ™åŒ–: std {latent_std:.3f} -> 3.0")
        
        # è§£ç 
        latent_for_decode = latent / pipeline.vae.config.scaling_factor
        reconstructed_mel = pipeline.vae.decode(latent_for_decode).sample
        
        print(f"   è§£ç è¾“å‡º: {reconstructed_mel.shape}")
        print(f"   è§£ç èŒƒå›´: [{reconstructed_mel.min():.3f}, {reconstructed_mel.max():.3f}]")
    
    # HiFiGANå¤„ç† - å¤šç§ç­–ç•¥
    print("\nğŸ¤ HiFiGAN vocoder (å¤šç­–ç•¥)...")
    
    # ç­–ç•¥1: ä½¿ç”¨pipelineæ ‡å‡†æ–¹æ³•
    try:
        print("   ğŸš€ ç­–ç•¥1: ä½¿ç”¨pipeline.mel_spectrogram_to_waveform...")
        waveform = pipeline.mel_spectrogram_to_waveform(reconstructed_mel)
        reconstructed_audio = waveform.squeeze().cpu().detach().numpy()
        vocoder_method = "AudioLDM2_Pipeline_Standard"
        print(f"   âœ… æˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
        
    except Exception as e:
        print(f"   âŒ ç­–ç•¥1å¤±è´¥: {e}")
        
        # ç­–ç•¥2: ç›´æ¥ä½¿ç”¨vocoderï¼Œä½†åŠ å…¥mel-spectrogramé¢„å¤„ç†
        try:
            print("   ğŸ”„ ç­–ç•¥2: é¢„å¤„ç†+ç›´æ¥vocoderè°ƒç”¨...")
            
            # é¢„å¤„ç†mel-spectrogram
            vocoder_input = reconstructed_mel.clone()
            
            # ç»´åº¦è°ƒæ•´
            if vocoder_input.dim() == 4:
                vocoder_input = vocoder_input.squeeze(1)  # [B, 1, H, W] -> [B, H, W]
            
            # æ”¹è¿›çš„mel-spectrogramé¢„å¤„ç†
            # å°†èŒƒå›´è°ƒæ•´åˆ°vocoderæœŸæœ›çš„èŒƒå›´
            vocoder_input = torch.clamp(vocoder_input, -10, 2)
            
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            vocoder_dtype = next(pipeline.vocoder.parameters()).dtype
            vocoder_input = vocoder_input.to(vocoder_dtype)
            
            print(f"   Vocoderè¾“å…¥: {vocoder_input.shape}, {vocoder_input.dtype}")
            print(f"   Vocoderè¾“å…¥èŒƒå›´: [{vocoder_input.min():.3f}, {vocoder_input.max():.3f}]")
            
            # ç›´æ¥è°ƒç”¨vocoder
            with torch.no_grad():
                waveform = pipeline.vocoder(vocoder_input)
                reconstructed_audio = waveform.squeeze().cpu().detach().numpy()
                
            vocoder_method = "AudioLDM2_Vocoder_Direct_Improved"
            print(f"   âœ… ç­–ç•¥2æˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
            
        except Exception as e2:
            print(f"   âŒ ç­–ç•¥2å¤±è´¥: {e2}")
            
            # ç­–ç•¥3: ä½¿ç”¨æ”¹è¿›çš„Griffin-Lim
            print("   ğŸ”„ ç­–ç•¥3: ä½¿ç”¨æ”¹è¿›çš„Griffin-Lim...")
            
            mel_np = reconstructed_mel.squeeze().cpu().detach().numpy()
            print(f"   Mel shape: {mel_np.shape}")
            
            # ç»´åº¦è°ƒæ•´
            if mel_np.ndim == 2 and mel_np.shape[1] == 64:
                mel_np = mel_np.T  # [time, 64] -> [64, time]
            
            # æ”¹è¿›çš„åå½’ä¸€åŒ– - ä½¿ç”¨æ›´ç²¾ç¡®çš„æ˜ å°„
            if use_clap_features:
                # ClapFeatureExtractorçš„è¾“å‡ºèŒƒå›´å¤§çº¦æ˜¯[-100, 20]
                mel_db = (mel_np + 100) / 120 * 80 - 80
            else:
                # ä¼ ç»Ÿæ–¹æ³•çš„è¾“å‡ºèŒƒå›´æ˜¯[-1, 1]
                mel_db = (mel_np + 1) / 2 * 80 - 80
            
            mel_power = librosa.db_to_power(mel_db)
            
            # ä½¿ç”¨æ›´é€‚åˆçš„å‚æ•°
            reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                mel_power,
                sr=16000,
                hop_length=160,
                n_fft=1024,
                win_length=1024,
                window='hann',
                n_iter=64,  # æ›´å¤šè¿­ä»£
                length=len(audio_16k)
            )
            vocoder_method = "Griffin_Lim_Improved"
            print(f"   âœ… ç­–ç•¥3æˆåŠŸ: {len(reconstructed_audio)}æ ·æœ¬")
    
    # åå¤„ç†æ”¹è¿›
    print("\nğŸ”§ åå¤„ç†...")
    
    # éŸ³é‡åŒ¹é…
    reference_audio = audio_16k
    if len(reconstructed_audio) > len(reference_audio):
        reconstructed_audio = reconstructed_audio[:len(reference_audio)]
    elif len(reconstructed_audio) < len(reference_audio):
        reconstructed_audio = np.pad(reconstructed_audio, (0, len(reference_audio) - len(reconstructed_audio)))
    
    # æ”¹è¿›çš„éŸ³é‡åŒ¹é…
    ref_rms = np.sqrt(np.mean(reference_audio ** 2))
    rec_rms = np.sqrt(np.mean(reconstructed_audio ** 2))
    
    if rec_rms > 0:
        volume_ratio = ref_rms / rec_rms
        # é™åˆ¶éŸ³é‡è°ƒæ•´èŒƒå›´ï¼Œé¿å…è¿‡åº¦æ”¾å¤§
        volume_ratio = np.clip(volume_ratio, 0.1, 10.0)
        reconstructed_audio = reconstructed_audio * volume_ratio
        print(f"   éŸ³é‡åŒ¹é…: {rec_rms:.4f} -> {ref_rms:.4f} (æ¯”ä¾‹: {volume_ratio:.2f})")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("vae_hifigan_ultimate_fix")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
    reconstructed_path = output_dir / f"{input_name}_{vocoder_method}_improved_{timestamp}.wav"
    
    print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
    save_audio_compatible(reference_audio, original_path)
    save_audio_compatible(reconstructed_audio, reconstructed_path)
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    mse = np.mean((reference_audio - reconstructed_audio) ** 2)
    snr = 10 * np.log10(np.mean(reference_audio ** 2) / (mse + 1e-10))
    correlation = np.corrcoef(reference_audio, reconstructed_audio)[0, 1] if len(reference_audio) > 1 else 0
    
    # é¢å¤–çš„è´¨é‡æŒ‡æ ‡
    mae = np.mean(np.abs(reference_audio - reconstructed_audio))
    
    # è¾“å‡ºç»“æœ
    print(f"\n{'='*60}")
    print(f"ğŸ¯ AudioLDM2 æœ€ç»ˆä¿®å¤ç»“æœ (æ”¹è¿›ç‰ˆ)")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ğŸ“Š SNR: {snr:.2f} dB")
    print(f"ğŸ“Š MSE: {mse:.6f}")
    print(f"ğŸ“Š MAE: {mae:.6f}")
    print(f"ğŸ“Š ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"ğŸ¤ é‡å»ºæ–¹æ³•: {vocoder_method}")
    
    # è¯Šæ–­åˆ†æ
    print(f"\nğŸ”¬ å…³é”®æ”¹è¿›:")
    print(f"   âœ… VAE scaling_factor: {pipeline.vae.config.scaling_factor}")
    print(f"   âœ… éŸ³é‡é¢„å¤„ç†å’Œåå¤„ç†")
    print(f"   âœ… æ”¹è¿›çš„latenté‡‡æ · (mode vs sample)")
    print(f"   âœ… ç²¾ç¡®çš„mel-spectrogramå‚æ•°")
    print(f"   âœ… å¤šç­–ç•¥vocoderå¤„ç†")
    print(f"   âœ… ClapFeatureExtractorä½¿ç”¨: {'æˆåŠŸ' if use_clap_features else 'å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•'}")
    
    # è´¨é‡è¯„ä¼°
    if "Standard" in vocoder_method or "Direct" in vocoder_method:
        print(f"\nğŸ‰ HiFiGANä¿®å¤æˆåŠŸï¼")
        quality_score = snr + correlation * 10  # ç»¼åˆè´¨é‡åˆ†æ•°
        if quality_score > 5:
            print(f"ğŸ† é‡å»ºè´¨é‡ä¼˜ç§€ï¼(ç»¼åˆåˆ†æ•°: {quality_score:.2f}")
        else:
            print(f"ğŸ”§ é‡å»ºè´¨é‡è‰¯å¥½ï¼Œä½†ä»æœ‰æå‡ç©ºé—´")
    else:
        print(f"âš ï¸ ä½¿ç”¨äº†å›é€€çš„Griffin-Lim vocoder")
        print(f"ğŸ’¡ å»ºè®®: æ£€æŸ¥HiFiGANæ¨¡å‹åŠ è½½æ˜¯å¦æ­£å¸¸")


def test_audioldm2_v3_balanced(audio_path, max_length=10.0):
    """
    V3ç‰ˆæœ¬: å¹³è¡¡ä¼˜åŒ–ç‰ˆæœ¬
    åŸºäºV1ä½†æ·»åŠ æ›´ç²¾ç»†çš„å‚æ•°è°ƒæ•´å’Œå¤„ç†ç­–ç•¥
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ğŸ¯ V3: AudioLDM2 å¹³è¡¡ä¼˜åŒ–ç‰ˆæœ¬")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # åŠ è½½AudioLDM2 pipeline
    print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   VAE scaling_factor: {pipeline.vae.config.scaling_factor}")
    print(f"   Vocoderç±»å‹: {type(pipeline.vocoder).__name__}")
    
    # è·å–feature extractorå‚æ•°
    fe_sr = pipeline.feature_extractor.sampling_rate
    print(f"   FeatureExtractoré‡‡æ ·ç‡: {fe_sr} Hz")
    
    # åŠ è½½éŸ³é¢‘ - V3: ä½¿ç”¨æ›´ç²¾ç»†çš„éŸ³é¢‘é¢„å¤„ç†
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘: {Path(audio_path).name}")
    
    # ä½¿ç”¨feature extractorçš„é‡‡æ ·ç‡
    audio_fe_sr, sr_fe = librosa.load(audio_path, sr=fe_sr, duration=max_length)
    audio_16k, sr_16k = librosa.load(audio_path, sr=16000, duration=max_length)
    
    print(f"   {fe_sr}HzéŸ³é¢‘: {len(audio_fe_sr)/sr_fe:.2f}ç§’, èŒƒå›´[{audio_fe_sr.min():.3f}, {audio_fe_sr.max():.3f}]")
    print(f"   16kHzéŸ³é¢‘: {len(audio_16k)/sr_16k:.2f}ç§’, èŒƒå›´[{audio_16k.min():.3f}, {audio_16k.max():.3f}]")
    
    # V3ç‰¹è‰²: æ›´ç²¾ç»†çš„éŸ³é¢‘é¢„å¤„ç†
    print("ğŸµ V3: ç²¾ç»†åŒ–ClapFeatureExtractorå¤„ç†...")
    try:
        # V3æ”¹è¿›1: åŠ¨æ€éŸ³é‡è°ƒæ•´
        audio_input = audio_fe_sr.copy()
        
        # è®¡ç®—åŠ¨æ€èŒƒå›´
        audio_rms = np.sqrt(np.mean(audio_input**2))
        audio_peak = np.max(np.abs(audio_input))
        
        # V3ç‰¹è‰²: åŠ¨æ€éŸ³é‡å½’ä¸€åŒ–ç­–ç•¥
        if audio_peak > 0:
            # å¦‚æœéŸ³é¢‘å¾ˆå°ï¼Œé€‚åº¦æ”¾å¤§
            if audio_peak < 0.1:
                audio_input = audio_input * (0.3 / audio_peak)
            # å¦‚æœéŸ³é¢‘å¾ˆå¤§ï¼Œé€‚åº¦ç¼©å°
            elif audio_peak > 0.95:
                audio_input = audio_input * (0.85 / audio_peak)
            # ä¸­ç­‰éŸ³é‡ï¼Œè½»å¾®è°ƒæ•´
            else:
                audio_input = audio_input * (0.9 / audio_peak)
        
        print(f"   V3éŸ³é¢‘è°ƒæ•´: {audio_peak:.3f} -> {np.max(np.abs(audio_input)):.3f}")
        
        # V3æ”¹è¿›2: æ·»åŠ è½»å¾®çš„å¹³æ»‘å¤„ç†
        from scipy import signal
        # ä½¿ç”¨å¾ˆè½»çš„ä½é€šæ»¤æ³¢å™¨ï¼Œåªå»é™¤æé«˜é¢‘å™ªå£°
        sos = signal.butter(2, 0.95, 'low', output='sos')
        audio_input = signal.sosfilt(sos, audio_input)
        
        features = pipeline.feature_extractor(
            audio_input, 
            return_tensors="pt", 
            sampling_rate=fe_sr
        )
        
        mel_input = features.input_features.to(device)
        if device == "cuda":
            mel_input = mel_input.half()
        
        print(f"   âœ… V3 ClapFeatureExtractoræˆåŠŸ")
        print(f"   è¾“å…¥: {mel_input.shape} (æ ¼å¼: [batch, channel, time, feature])")
        print(f"   èŒƒå›´: [{mel_input.min():.3f}, {mel_input.max():.3f}]")
        
        use_clap_features = True
        
    except Exception as e:
        print(f"   âŒ V3 ClapFeatureExtractorå¤±è´¥: {e}")
        use_clap_features = False
    
    # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
    if not use_clap_features:
        print("   ğŸ”„ V3: ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•...")
        # ä½¿ç”¨ä¸V1ç›¸åŒçš„ä¼ ç»Ÿæ–¹æ³•
        mel_spec = librosa.feature.melspectrogram(
            y=audio_fe_sr, 
            sr=fe_sr, 
            n_mels=64,
            hop_length=int(fe_sr * 0.01),
            n_fft=int(fe_sr * 0.025),
            fmin=50,
            fmax=fe_sr // 2,
            window='hann',
            center=True,
            pad_mode='reflect',
            power=2.0
        )
        
        mel_db = librosa.power_to_db(mel_spec, ref=np.max, top_db=80)
        mel_db = 2 * (mel_db + 80) / 80 - 1
        
        mel_tensor = torch.from_numpy(mel_db).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.half()
        
        mel_input = mel_tensor.transpose(0, 1).unsqueeze(0).unsqueeze(0)
        print(f"   ä¼ ç»Ÿå¤„ç†: {mel_input.shape}")
        print(f"   èŒƒå›´: [{mel_input.min():.3f}, {mel_input.max():.3f}]")
    
    print(f"   æœ€ç»ˆè¾“å…¥: {mel_input.shape}, {mel_input.dtype}")
    
    # V3 VAEå¤„ç† - æ›´ç²¾ç»†çš„å‚æ•°æ§åˆ¶
    print("\nğŸ§  V3: VAEç¼–ç è§£ç  (ç²¾ç»†ä¼˜åŒ–)...")
    with torch.no_grad():
        # ç¼–ç 
        latent_dist = pipeline.vae.encode(mel_input)
        
        # V3æ”¹è¿›3: æ··åˆé‡‡æ ·ç­–ç•¥
        if hasattr(latent_dist, 'latent_dist'):
            # æ··åˆmodeå’Œsampleï¼Œå–åŠ æƒå¹³å‡
            latent_mode = latent_dist.latent_dist.mode()
            latent_sample = latent_dist.latent_dist.sample()
            # 70%mode + 30%sampleï¼Œå¹³è¡¡ç¡®å®šæ€§å’Œéšæœºæ€§
            latent = 0.7 * latent_mode + 0.3 * latent_sample
        else:
            latent = latent_dist.mode() if hasattr(latent_dist, 'mode') else latent_dist.sample()
        
        # åº”ç”¨scaling_factor
        latent = latent * pipeline.vae.config.scaling_factor
        print(f"   V3ç¼–ç latent: {latent.shape}")
        print(f"   LatentèŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
        
        # V3æ”¹è¿›4: è‡ªé€‚åº”latentè°ƒæ•´
        latent_std = torch.std(latent)
        latent_mean = torch.mean(latent)
        
        # å¦‚æœlatentåˆ†å¸ƒå¼‚å¸¸ï¼Œè¿›è¡Œè½»å¾®è°ƒæ•´
        if latent_std > 4.0:
            latent = latent * (3.5 / latent_std)
            print(f"   V3 Latentæ ‡å‡†åŒ–: std {latent_std:.3f} -> 3.5")
        
        # å¦‚æœå‡å€¼åç§»è¿‡å¤§ï¼Œè¿›è¡Œä¸­å¿ƒåŒ–
        if abs(latent_mean) > 2.0:
            latent = latent - latent_mean * 0.3
            print(f"   V3 Latentä¸­å¿ƒåŒ–: mean {latent_mean:.3f} -> {torch.mean(latent):.3f}")
        
        # è§£ç 
        latent_for_decode = latent / pipeline.vae.config.scaling_factor
        reconstructed_mel = pipeline.vae.decode(latent_for_decode).sample
        
        print(f"   V3è§£ç è¾“å‡º: {reconstructed_mel.shape}")
        print(f"   è§£ç èŒƒå›´: [{reconstructed_mel.min():.3f}, {reconstructed_mel.max():.3f}]")
    
    # V3 HiFiGANå¤„ç† - ä¼˜åŒ–çš„vocoderç­–ç•¥
    print("\nğŸ¤ V3: HiFiGAN vocoder (ä¼˜åŒ–ç­–ç•¥)...")
    
    # V3ä¼˜å…ˆç­–ç•¥: æ”¹è¿›çš„pipelineæ–¹æ³•
    try:
        print("   ğŸš€ V3ç­–ç•¥: ä¼˜åŒ–çš„pipeline.mel_spectrogram_to_waveform...")
        
        # V3æ”¹è¿›5: mel-spectrogramåå¤„ç†
        mel_processed = reconstructed_mel.clone()
        
        # è½»å¾®çš„mel-spectrogramå¹³æ»‘
        if mel_processed.dim() == 4:
            # å¯¹æ—¶é—´ç»´åº¦è¿›è¡Œè½»å¾®å¹³æ»‘
            kernel = torch.ones(1, 1, 1, 3, device=mel_processed.device, dtype=mel_processed.dtype) / 3
            mel_processed = torch.nn.functional.conv2d(
                mel_processed, 
                kernel, 
                padding=(0, 1)
            )
        
        waveform = pipeline.mel_spectrogram_to_waveform(mel_processed)
        reconstructed_audio = waveform.squeeze().cpu().detach().numpy()
        vocoder_method = "V3_AudioLDM2_Pipeline_Balanced"
        print(f"   âœ… V3æˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
        
    except Exception as e:
        print(f"   âŒ V3ç­–ç•¥å¤±è´¥: {e}")
        
        # å›é€€åˆ°æ ‡å‡†æ–¹æ³•
        try:
            print("   ğŸ”„ V3å›é€€: æ ‡å‡†pipelineæ–¹æ³•...")
            waveform = pipeline.mel_spectrogram_to_waveform(reconstructed_mel)
            reconstructed_audio = waveform.squeeze().cpu().detach().numpy()
            vocoder_method = "V3_AudioLDM2_Pipeline_Standard"
            print(f"   âœ… V3å›é€€æˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
            
        except Exception as e2:
            print(f"   âŒ V3å›é€€å¤±è´¥: {e2}")
            return None
    
    # V3åå¤„ç† - å¹³è¡¡çš„éŸ³è´¨ä¼˜åŒ–
    print("\nğŸ”§ V3: å¹³è¡¡åå¤„ç†...")
    
    # é•¿åº¦åŒ¹é…
    reference_audio = audio_16k
    if len(reconstructed_audio) > len(reference_audio):
        reconstructed_audio = reconstructed_audio[:len(reference_audio)]
    elif len(reconstructed_audio) < len(reference_audio):
        reconstructed_audio = np.pad(reconstructed_audio, (0, len(reference_audio) - len(reconstructed_audio)))
    
    # V3æ”¹è¿›6: æ™ºèƒ½éŸ³é‡åŒ¹é…
    ref_rms = np.sqrt(np.mean(reference_audio ** 2))
    rec_rms = np.sqrt(np.mean(reconstructed_audio ** 2))
    
    if rec_rms > 0:
        # V3ç‰¹è‰²: æ›´ä¿å®ˆçš„éŸ³é‡åŒ¹é…
        volume_ratio = ref_rms / rec_rms
        
        # æ ¹æ®ä¿¡å™ªæ¯”è°ƒæ•´éŸ³é‡åŒ¹é…å¼ºåº¦
        initial_snr = 10 * np.log10(np.mean(reference_audio**2) / (np.mean((reference_audio - reconstructed_audio)**2) + 1e-10))
        
        if initial_snr > 0:  # å¦‚æœä¿¡å™ªæ¯”è¾ƒå¥½ï¼Œè½»å¾®è°ƒæ•´
            volume_ratio = 1.0 + (volume_ratio - 1.0) * 0.8
        else:  # å¦‚æœä¿¡å™ªæ¯”è¾ƒå·®ï¼Œæ›´ç§¯æè°ƒæ•´
            volume_ratio = 1.0 + (volume_ratio - 1.0) * 1.2
        
        # é™åˆ¶éŸ³é‡è°ƒæ•´èŒƒå›´
        volume_ratio = np.clip(volume_ratio, 0.2, 5.0)
        reconstructed_audio = reconstructed_audio * volume_ratio
        print(f"   V3éŸ³é‡åŒ¹é…: {rec_rms:.4f} -> {ref_rms:.4f} (æ¯”ä¾‹: {volume_ratio:.2f})")
    
    # V3æ”¹è¿›7: è½»å¾®çš„åå¤„ç†æ»¤æ³¢
    try:
        # åªå¯¹é‡å»ºéŸ³é¢‘è¿›è¡Œè½»å¾®çš„å»å™ª
        from scipy import signal
        # ä½¿ç”¨å¾ˆè½»çš„é«˜é€šæ»¤æ³¢å™¨ï¼Œå»é™¤ä½é¢‘å™ªå£°
        sos = signal.butter(1, 50/(16000/2), 'high', output='sos')
        reconstructed_audio = signal.sosfilt(sos, reconstructed_audio)
        print(f"   V3åå¤„ç†æ»¤æ³¢: å®Œæˆ")
    except:
        print(f"   V3åå¤„ç†æ»¤æ³¢: è·³è¿‡")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("vae_hifigan_ultimate_fix")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
    reconstructed_path = output_dir / f"{input_name}_V3_{vocoder_method}_{timestamp}.wav"
    
    print("\nğŸ’¾ V3ä¿å­˜ç»“æœ...")
    save_audio_compatible(reference_audio, original_path)
    save_audio_compatible(reconstructed_audio, reconstructed_path)
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    mse = np.mean((reference_audio - reconstructed_audio) ** 2)
    snr = 10 * np.log10(np.mean(reference_audio ** 2) / (mse + 1e-10))
    correlation = np.corrcoef(reference_audio, reconstructed_audio)[0, 1] if len(reference_audio) > 1 else 0
    mae = np.mean(np.abs(reference_audio - reconstructed_audio))
    
    # V3ç»¼åˆè´¨é‡åˆ†æ•°
    quality_score = snr + correlation * 8 + (1 / (mae + 0.01)) * 2
    
    # è¾“å‡ºç»“æœ
    print(f"\n{'='*60}")
    print(f"ğŸ¯ V3ç»“æœ (AudioLDM2 å¹³è¡¡ä¼˜åŒ–ç‰ˆæœ¬)")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ğŸ“Š SNR: {snr:.2f} dB")
    print(f"ğŸ“Š MSE: {mse:.6f}")
    print(f"ğŸ“Š MAE: {mae:.6f}")
    print(f"ğŸ“Š ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"ğŸ† V3ç»¼åˆè´¨é‡åˆ†æ•°: {quality_score:.2f}")
    print(f"ğŸ¤ é‡å»ºæ–¹æ³•: {vocoder_method}")
    
    # V3è¯Šæ–­åˆ†æ
    print(f"\nğŸ”¬ V3ç‰¹è‰²æ”¹è¿›:")
    print(f"   âœ… åŠ¨æ€éŸ³é‡é¢„å¤„ç†")
    print(f"   âœ… æ··åˆé‡‡æ ·ç­–ç•¥ (70%mode + 30%sample)")
    print(f"   âœ… è‡ªé€‚åº”latentè°ƒæ•´")
    print(f"   âœ… mel-spectrogramæ—¶é—´å¹³æ»‘")
    print(f"   âœ… æ™ºèƒ½éŸ³é‡åŒ¹é…")
    print(f"   âœ… è½»å¾®åå¤„ç†æ»¤æ³¢")
    
    # è´¨é‡è¯„ä¼°
    if quality_score > 8:
        print(f"ğŸ‰ V3é‡å»ºè´¨é‡ä¼˜ç§€ï¼")
    elif quality_score > 5:
        print(f"âœ… V3é‡å»ºè´¨é‡è‰¯å¥½")
    else:
        print(f"âš ï¸ V3é‡å»ºè´¨é‡éœ€è¦æ”¹è¿›")
    
    return {
        'snr': snr,
        'mse': mse,
        'mae': mae,
        'correlation': correlation,
        'quality_score': quality_score,
        'output_file': str(reconstructed_path),
        'vocoder_method': vocoder_method
    }


def test_audioldm2_v4_highfreq_fix(audio_path, max_length=10.0):
    """
    V4ç‰ˆæœ¬: é«˜é¢‘ä¿¡å·ä¿®å¤ç‰ˆæœ¬
    ä¸“é—¨è§£å†³melé¢‘è°±å›¾é«˜é¢‘ä¿¡å·ä¸¢å¤±é—®é¢˜
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ğŸ¯ V4: AudioLDM2 é«˜é¢‘ä¿¡å·ä¿®å¤ç‰ˆæœ¬")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # åŠ è½½AudioLDM2 pipeline
    print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   VAE scaling_factor: {pipeline.vae.config.scaling_factor}")
    print(f"   Vocoderç±»å‹: {type(pipeline.vocoder).__name__}")
    
    # è·å–feature extractorå‚æ•°
    fe_sr = pipeline.feature_extractor.sampling_rate
    print(f"   FeatureExtractoré‡‡æ ·ç‡: {fe_sr} Hz")
    
    # V4æ”¹è¿›1: æ£€æŸ¥ClapFeatureExtractorçš„é¢‘ç‡èŒƒå›´
    print(f"   ClapFeatureExtractorå‚æ•°æ£€æŸ¥:")
    if hasattr(pipeline.feature_extractor, 'feature_extractor'):
        inner_extractor = pipeline.feature_extractor.feature_extractor
        print(f"   - fmin: {getattr(inner_extractor, 'fmin', 'N/A')}")
        print(f"   - fmax: {getattr(inner_extractor, 'fmax', 'N/A')}")
        print(f"   - n_mels: {getattr(inner_extractor, 'n_mels', 'N/A')}")
        print(f"   - hop_length: {getattr(inner_extractor, 'hop_length', 'N/A')}")
    
    # åŠ è½½éŸ³é¢‘
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘: {Path(audio_path).name}")
    
    # ä½¿ç”¨feature extractorçš„é‡‡æ ·ç‡
    audio_fe_sr, sr_fe = librosa.load(audio_path, sr=fe_sr, duration=max_length)
    audio_16k, sr_16k = librosa.load(audio_path, sr=16000, duration=max_length)
    
    print(f"   {fe_sr}HzéŸ³é¢‘: {len(audio_fe_sr)/sr_fe:.2f}ç§’, èŒƒå›´[{audio_fe_sr.min():.3f}, {audio_fe_sr.max():.3f}]")
    print(f"   16kHzéŸ³é¢‘: {len(audio_16k)/sr_16k:.2f}ç§’, èŒƒå›´[{audio_16k.min():.3f}, {audio_16k.max():.3f}]")
    
    # V4ç‰¹è‰²: é«˜é¢‘ä¿æŠ¤çš„ç‰¹å¾æå–
    print("ğŸµ V4: é«˜é¢‘ä¿æŠ¤çš„ClapFeatureExtractorå¤„ç†...")
    
    # å…ˆè·å–åŸå§‹mel-spectrogramä½œä¸ºå‚è€ƒ
    print("   ğŸ” åˆ†æåŸå§‹éŸ³é¢‘çš„é¢‘è°±ç‰¹å¾...")
    original_mel = librosa.feature.melspectrogram(
        y=audio_fe_sr, 
        sr=fe_sr, 
        n_mels=64,
        hop_length=480,  # ä½¿ç”¨AudioLDM2çš„hop_length
        n_fft=1024,
        fmin=0,  # ä»0Hzå¼€å§‹
        fmax=fe_sr // 2,  # åˆ°Nyquisté¢‘ç‡
        window='hann',
        power=2.0
    )
    original_mel_db = librosa.power_to_db(original_mel, ref=np.max)
    
    print(f"   åŸå§‹melé¢‘è°±: {original_mel.shape}")
    print(f"   åŸå§‹melèŒƒå›´: [{original_mel_db.min():.1f}, {original_mel_db.max():.1f}] dB")
    
    # åˆ†æé«˜é¢‘èƒ½é‡åˆ†å¸ƒ
    high_freq_bins = original_mel_db[48:, :]  # é«˜é¢‘éƒ¨åˆ†ï¼ˆå¤§çº¦75%ä»¥ä¸Šï¼‰
    mid_freq_bins = original_mel_db[16:48, :]  # ä¸­é¢‘éƒ¨åˆ†
    low_freq_bins = original_mel_db[:16, :]    # ä½é¢‘éƒ¨åˆ†
    
    print(f"   é«˜é¢‘èƒ½é‡ (75-100%): å¹³å‡ {high_freq_bins.mean():.1f} dB, æœ€å¤§ {high_freq_bins.max():.1f} dB")
    print(f"   ä¸­é¢‘èƒ½é‡ (25-75%): å¹³å‡ {mid_freq_bins.mean():.1f} dB, æœ€å¤§ {mid_freq_bins.max():.1f} dB")
    print(f"   ä½é¢‘èƒ½é‡ (0-25%): å¹³å‡ {low_freq_bins.mean():.1f} dB, æœ€å¤§ {low_freq_bins.max():.1f} dB")
    
    try:
        # V4æ”¹è¿›2: ä¿æŠ¤é«˜é¢‘çš„éŸ³é¢‘é¢„å¤„ç†
        audio_input = audio_fe_sr.copy()
        
        # æ¸©å’Œçš„å½’ä¸€åŒ–ï¼Œé¿å…æŸå¤±é«˜é¢‘ä¿¡æ¯
        peak_value = np.max(np.abs(audio_input))
        if peak_value > 0:
            # æ›´ä¿å®ˆçš„å½’ä¸€åŒ–ï¼Œä¿æŒåŠ¨æ€èŒƒå›´
            audio_input = audio_input / peak_value * 0.98
        
        features = pipeline.feature_extractor(
            audio_input, 
            return_tensors="pt", 
            sampling_rate=fe_sr
        )
        
        mel_input = features.input_features.to(device)
        if device == "cuda":
            mel_input = mel_input.half()
        
        print(f"   âœ… V4 ClapFeatureExtractoræˆåŠŸ")
        print(f"   è¾“å…¥: {mel_input.shape} (æ ¼å¼: [batch, channel, time, feature])")
        print(f"   èŒƒå›´: [{mel_input.min():.3f}, {mel_input.max():.3f}]")
        
        # V4æ”¹è¿›3: åˆ†æClapFeatureExtractorçš„è¾“å‡º
        mel_clap = mel_input.squeeze().cpu().numpy()
        if mel_clap.ndim == 3:
            mel_clap = mel_clap[0]  # å–ç¬¬ä¸€ä¸ªé€šé“
        
        print(f"   CLAPè¾“å‡ºåˆ†æ:")
        print(f"   - å½¢çŠ¶: {mel_clap.shape}")
        print(f"   - é«˜é¢‘éƒ¨åˆ† (75-100%): å¹³å‡ {mel_clap[48:, :].mean():.1f}, æœ€å¤§ {mel_clap[48:, :].max():.1f}")
        print(f"   - ä¸­é¢‘éƒ¨åˆ† (25-75%): å¹³å‡ {mel_clap[16:48, :].mean():.1f}, æœ€å¤§ {mel_clap[16:48, :].max():.1f}")
        print(f"   - ä½é¢‘éƒ¨åˆ† (0-25%): å¹³å‡ {mel_clap[:16, :].mean():.1f}, æœ€å¤§ {mel_clap[:16, :].max():.1f}")
        
        use_clap_features = True
        
    except Exception as e:
        print(f"   âŒ V4 ClapFeatureExtractorå¤±è´¥: {e}")
        use_clap_features = False
    
    # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„ä¼ ç»Ÿæ–¹æ³•
    if not use_clap_features:
        print("   ğŸ”„ V4: ä½¿ç”¨é«˜é¢‘ä¿æŠ¤çš„ä¼ ç»Ÿæ–¹æ³•...")
        
        # V4æ”¹è¿›4: é«˜é¢‘ä¿æŠ¤çš„mel-spectrogram
        mel_spec = librosa.feature.melspectrogram(
            y=audio_fe_sr, 
            sr=fe_sr, 
            n_mels=64,
            hop_length=480,  # åŒ¹é…AudioLDM2
            n_fft=1024,
            fmin=0,  # ä»0Hzå¼€å§‹ï¼Œä¸ä¸¢å¤±ä½é¢‘
            fmax=fe_sr // 2,  # åˆ°Nyquisté¢‘ç‡ï¼Œä¿æŒé«˜é¢‘
            window='hann',
            center=True,
            pad_mode='reflect',
            power=2.0
        )
        
        # æ›´ä¿å®ˆçš„dBè½¬æ¢ï¼Œä¿æŒåŠ¨æ€èŒƒå›´
        mel_db = librosa.power_to_db(mel_spec, ref=np.max, top_db=120)  # å¢åŠ åŠ¨æ€èŒƒå›´
        
        # æ”¹è¿›çš„å½’ä¸€åŒ–ï¼Œä¿æŒé«˜é¢‘ä¿¡æ¯
        mel_db_normalized = (mel_db + 120) / 120 * 2 - 1
        
        mel_tensor = torch.from_numpy(mel_db_normalized).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.half()
        
        mel_input = mel_tensor.transpose(0, 1).unsqueeze(0).unsqueeze(0)
        
        print(f"   ä¼ ç»Ÿå¤„ç†: {mel_input.shape}")
        print(f"   èŒƒå›´: [{mel_input.min():.3f}, {mel_input.max():.3f}]")
    
    print(f"   æœ€ç»ˆè¾“å…¥: {mel_input.shape}, {mel_input.dtype}")
    
    # V4 VAEå¤„ç† - é«˜é¢‘ä¿æŠ¤
    print("\nğŸ§  V4: VAEç¼–ç è§£ç  (é«˜é¢‘ä¿æŠ¤)...")
    with torch.no_grad():
        # ç¼–ç 
        latent_dist = pipeline.vae.encode(mel_input)
        
        # V4æ”¹è¿›5: ä½¿ç”¨æ›´ç¨³å®šçš„é‡‡æ ·ç­–ç•¥
        if hasattr(latent_dist, 'latent_dist'):
            latent = latent_dist.latent_dist.mode()  # ä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·
        else:
            latent = latent_dist.mode() if hasattr(latent_dist, 'mode') else latent_dist.sample()
        
        # åº”ç”¨scaling_factor
        latent = latent * pipeline.vae.config.scaling_factor
        print(f"   V4ç¼–ç latent: {latent.shape}")
        print(f"   LatentèŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
        
        # V4æ”¹è¿›6: æ¸©å’Œçš„latentè°ƒæ•´ï¼Œé¿å…æŸå¤±é«˜é¢‘ä¿¡æ¯
        latent_std = torch.std(latent)
        if latent_std > 5.0:  # æ›´å®½æ¾çš„é˜ˆå€¼
            latent = latent * (4.5 / latent_std)
            print(f"   V4 Latentè½»å¾®è°ƒæ•´: std {latent_std:.3f} -> 4.5")
        
        # è§£ç 
        latent_for_decode = latent / pipeline.vae.config.scaling_factor
        reconstructed_mel = pipeline.vae.decode(latent_for_decode).sample
        
        print(f"   V4è§£ç è¾“å‡º: {reconstructed_mel.shape}")
        print(f"   è§£ç èŒƒå›´: [{reconstructed_mel.min():.3f}, {reconstructed_mel.max():.3f}]")
        
        # V4æ”¹è¿›7: åˆ†æé‡å»ºmelçš„é¢‘è°±ç‰¹å¾
        recon_mel = reconstructed_mel.squeeze().cpu().numpy()
        if recon_mel.ndim == 3:
            recon_mel = recon_mel[0]  # å–ç¬¬ä¸€ä¸ªé€šé“
        
        print(f"   é‡å»ºmelåˆ†æ:")
        print(f"   - å½¢çŠ¶: {recon_mel.shape}")
        print(f"   - é«˜é¢‘éƒ¨åˆ† (75-100%): å¹³å‡ {recon_mel[48:, :].mean():.3f}, æœ€å¤§ {recon_mel[48:, :].max():.3f}")
        print(f"   - ä¸­é¢‘éƒ¨åˆ† (25-75%): å¹³å‡ {recon_mel[16:48, :].mean():.3f}, æœ€å¤§ {recon_mel[16:48, :].max():.3f}")
        print(f"   - ä½é¢‘éƒ¨åˆ† (0-25%): å¹³å‡ {recon_mel[:16, :].mean():.3f}, æœ€å¤§ {recon_mel[:16, :].max():.3f}")
        
        # æ£€æŸ¥é«˜é¢‘ä¿¡æ¯ä¸¢å¤±
        high_freq_loss = np.mean(recon_mel[48:, :] == recon_mel[48:, :].min())
        print(f"   - é«˜é¢‘ä¿¡æ¯ä¸¢å¤±ç‡: {high_freq_loss*100:.1f}%")
        
        if high_freq_loss > 0.5:
            print(f"   âš ï¸ æ£€æµ‹åˆ°ä¸¥é‡é«˜é¢‘ä¿¡æ¯ä¸¢å¤±ï¼")
        elif high_freq_loss > 0.1:
            print(f"   âš ï¸ æ£€æµ‹åˆ°è½»å¾®é«˜é¢‘ä¿¡æ¯ä¸¢å¤±")
        else:
            print(f"   âœ… é«˜é¢‘ä¿¡æ¯ä¿æŒè‰¯å¥½")
    
    # V4 HiFiGANå¤„ç† - é«˜é¢‘ä¼˜åŒ–
    print("\nğŸ¤ V4: HiFiGAN vocoder (é«˜é¢‘ä¼˜åŒ–)...")
    
    # V4æ”¹è¿›8: é«˜é¢‘å¢å¼ºçš„vocoderå¤„ç†
    try:
        print("   ğŸš€ V4ç­–ç•¥: é«˜é¢‘å¢å¼ºçš„vocoderå¤„ç†...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é«˜é¢‘å¢å¼º
        if high_freq_loss > 0.1:
            print("   ğŸ”§ åº”ç”¨é«˜é¢‘å¢å¼º...")
            
            # æ–¹æ³•1: é«˜é¢‘éƒ¨åˆ†é€‚åº¦å¢å¼º
            mel_enhanced = reconstructed_mel.clone()
            
            # å¯¹é«˜é¢‘éƒ¨åˆ†è¿›è¡Œè½»å¾®å¢å¼º
            high_freq_mask = torch.zeros_like(mel_enhanced)
            high_freq_mask[:, :, :, 48:] = 1.0  # é«˜é¢‘éƒ¨åˆ†
            
            # è®¡ç®—é«˜é¢‘å¢å¼ºå› å­
            high_freq_mean = torch.mean(mel_enhanced * high_freq_mask)
            mid_freq_mean = torch.mean(mel_enhanced * (1 - high_freq_mask))
            
            if high_freq_mean < mid_freq_mean - 2.0:  # å¦‚æœé«˜é¢‘æ˜æ˜¾ä½äºä¸­é¢‘
                enhancement_factor = 1.0 + min(0.5, (mid_freq_mean - high_freq_mean) / 10.0)
                mel_enhanced = mel_enhanced * (1 + high_freq_mask * (enhancement_factor - 1))
                print(f"   é«˜é¢‘å¢å¼ºå› å­: {enhancement_factor:.3f}")
            
            waveform = pipeline.mel_spectrogram_to_waveform(mel_enhanced)
        else:
            waveform = pipeline.mel_spectrogram_to_waveform(reconstructed_mel)
        
        reconstructed_audio = waveform.squeeze().cpu().detach().numpy()
        vocoder_method = "V4_AudioLDM2_HighFreq_Enhanced"
        print(f"   âœ… V4æˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
        
    except Exception as e:
        print(f"   âŒ V4ç­–ç•¥å¤±è´¥: {e}")
        
        # å›é€€åˆ°æ ‡å‡†æ–¹æ³•
        try:
            print("   ğŸ”„ V4å›é€€: æ ‡å‡†pipelineæ–¹æ³•...")
            waveform = pipeline.mel_spectrogram_to_waveform(reconstructed_mel)
            reconstructed_audio = waveform.squeeze().cpu().detach().numpy()
            vocoder_method = "V4_AudioLDM2_Pipeline_Standard"
            print(f"   âœ… V4å›é€€æˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
            
        except Exception as e2:
            print(f"   âŒ V4å›é€€å¤±è´¥: {e2}")
            return None
    
    # V4åå¤„ç† - é«˜é¢‘ä¿æŠ¤
    print("\nğŸ”§ V4: é«˜é¢‘ä¿æŠ¤åå¤„ç†...")
    
    # é•¿åº¦åŒ¹é…
    reference_audio = audio_16k
    if len(reconstructed_audio) > len(reference_audio):
        reconstructed_audio = reconstructed_audio[:len(reference_audio)]
    elif len(reconstructed_audio) < len(reference_audio):
        reconstructed_audio = np.pad(reconstructed_audio, (0, len(reference_audio) - len(reconstructed_audio)))
    
    # V4æ”¹è¿›9: ä¿æŠ¤é«˜é¢‘çš„éŸ³é‡åŒ¹é…
    ref_rms = np.sqrt(np.mean(reference_audio ** 2))
    rec_rms = np.sqrt(np.mean(reconstructed_audio ** 2))
    
    if rec_rms > 0:
        volume_ratio = ref_rms / rec_rms
        # é™åˆ¶éŸ³é‡è°ƒæ•´èŒƒå›´ï¼Œé¿å…è¿‡åº¦æ”¾å¤§æŸå¤±é«˜é¢‘
        volume_ratio = np.clip(volume_ratio, 0.3, 3.0)
        reconstructed_audio = reconstructed_audio * volume_ratio
        print(f"   V4éŸ³é‡åŒ¹é…: {rec_rms:.4f} -> {ref_rms:.4f} (æ¯”ä¾‹: {volume_ratio:.2f})")
    
    # V4æ”¹è¿›10: é«˜é¢‘åˆ†æ
    print("   ğŸ” V4é«˜é¢‘åˆ†æ...")
    
    # åˆ†æé‡å»ºéŸ³é¢‘çš„é¢‘è°±
    recon_spec = np.abs(np.fft.fft(reconstructed_audio[:8192]))[:4096]  # å–å‰8192æ ·æœ¬åšFFT
    ref_spec = np.abs(np.fft.fft(reference_audio[:8192]))[:4096]
    
    # è®¡ç®—é«˜é¢‘èƒ½é‡æ¯”
    high_freq_energy_ref = np.sum(ref_spec[2048:])  # é«˜é¢‘éƒ¨åˆ†
    high_freq_energy_recon = np.sum(recon_spec[2048:])
    
    total_energy_ref = np.sum(ref_spec)
    total_energy_recon = np.sum(recon_spec)
    
    if total_energy_ref > 0 and total_energy_recon > 0:
        high_freq_ratio_ref = high_freq_energy_ref / total_energy_ref
        high_freq_ratio_recon = high_freq_energy_recon / total_energy_recon
        
        print(f"   åŸå§‹éŸ³é¢‘é«˜é¢‘èƒ½é‡æ¯”: {high_freq_ratio_ref:.3f}")
        print(f"   é‡å»ºéŸ³é¢‘é«˜é¢‘èƒ½é‡æ¯”: {high_freq_ratio_recon:.3f}")
        print(f"   é«˜é¢‘ä¿æŒç‡: {high_freq_ratio_recon/high_freq_ratio_ref*100:.1f}%")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("vae_hifigan_ultimate_fix")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
    reconstructed_path = output_dir / f"{input_name}_V4_{vocoder_method}_{timestamp}.wav"
    
    print("\nğŸ’¾ V4ä¿å­˜ç»“æœ...")
    save_audio_compatible(reference_audio, original_path)
    save_audio_compatible(reconstructed_audio, reconstructed_path)
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    mse = np.mean((reference_audio - reconstructed_audio) ** 2)
    snr = 10 * np.log10(np.mean(reference_audio ** 2) / (mse + 1e-10))
    correlation = np.corrcoef(reference_audio, reconstructed_audio)[0, 1] if len(reference_audio) > 1 else 0
    mae = np.mean(np.abs(reference_audio - reconstructed_audio))
    
    # V4ç»¼åˆè´¨é‡åˆ†æ•°ï¼ˆè€ƒè™‘é«˜é¢‘ä¿æŒï¼‰
    high_freq_score = high_freq_ratio_recon / high_freq_ratio_ref if high_freq_ratio_ref > 0 else 0
    quality_score = snr + correlation * 8 + high_freq_score * 5
    
    # è¾“å‡ºç»“æœ
    print(f"\n{'='*60}")
    print(f"ğŸ¯ V4ç»“æœ (AudioLDM2 é«˜é¢‘ä¿®å¤ç‰ˆæœ¬)")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ğŸ“Š SNR: {snr:.2f} dB")
    print(f"ğŸ“Š MSE: {mse:.6f}")
    print(f"ğŸ“Š MAE: {mae:.6f}")
    print(f"ğŸ“Š ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"ğŸ“Š é«˜é¢‘ä¿æŒåˆ†æ•°: {high_freq_score:.3f}")
    print(f"ğŸ† V4ç»¼åˆè´¨é‡åˆ†æ•°: {quality_score:.2f}")
    print(f"ğŸ¤ é‡å»ºæ–¹æ³•: {vocoder_method}")
    
    # V4è¯Šæ–­åˆ†æ
    print(f"\nğŸ”¬ V4é«˜é¢‘ä¿®å¤ç‰¹è‰²:")
    print(f"   âœ… é«˜é¢‘ä¿æŠ¤çš„éŸ³é¢‘é¢„å¤„ç†")
    print(f"   âœ… é¢‘è°±åˆ†æå’Œç›‘æ§")
    print(f"   âœ… é«˜é¢‘ä¿¡æ¯ä¸¢å¤±æ£€æµ‹")
    print(f"   âœ… è‡ªé€‚åº”é«˜é¢‘å¢å¼º")
    print(f"   âœ… é«˜é¢‘ä¿æŠ¤çš„éŸ³é‡åŒ¹é…")
    print(f"   âœ… è¯¦ç»†çš„é¢‘è°±åˆ†æ")
    
    # è´¨é‡è¯„ä¼°
    if high_freq_score > 0.8:
        print(f"ğŸ‰ V4é«˜é¢‘ä¿æŒä¼˜ç§€ï¼")
    elif high_freq_score > 0.6:
        print(f"âœ… V4é«˜é¢‘ä¿æŒè‰¯å¥½")
    elif high_freq_score > 0.3:
        print(f"âš ï¸ V4é«˜é¢‘æœ‰ä¸€å®šæŸå¤±")
    else:
        print(f"âŒ V4é«˜é¢‘æŸå¤±ä¸¥é‡")
    
    return {
        'snr': snr,
        'mse': mse,
        'mae': mae,
        'correlation': correlation,
        'quality_score': quality_score,
        'high_freq_score': high_freq_score,
        'output_file': str(reconstructed_path),
        'vocoder_method': vocoder_method
    }


def main():
    """ä¸»å‡½æ•°ï¼šè‡ªåŠ¨å¤„ç†AudioLDM2_Music_output.wavï¼Œå¹¶æä¾›å¤šä¸ªç‰ˆæœ¬å¯¹æ¯”"""
    input_file = "AudioLDM2_Music_output.wav"
    
    print("ğŸµ AudioLDM2 VAEé‡å»ºå¤šç‰ˆæœ¬å¯¹æ¯”")
    print("=" * 50)
    print("ğŸ“ ç‰ˆæœ¬è¯´æ˜:")
    print("   V1 (æ¨è): AudioLDM2 Pipeline Standard improved - æœ€ä½³å¬æ„Ÿå’Œä¿¡å·ä¿çœŸåº¦")
    print("   V3 (æ–°): å¹³è¡¡ä¼˜åŒ–ç‰ˆæœ¬ - ç²¾ç»†åŒ–å‚æ•°è°ƒæ•´å’Œå¤„ç†ç­–ç•¥")
    print("   V4 (é«˜é¢‘ä¿®å¤): ä¸“é—¨é’ˆå¯¹é«˜é¢‘ä¿¡å·ä¸¢å¤±é—®é¢˜")
    print("=" * 50)
    
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
        print("è¯·ç¡®ä¿AudioLDM2_Music_output.wavåœ¨å½“å‰ç›®å½•ä¸­")
        return
    
    print(f"ğŸµ è‡ªåŠ¨å¤„ç†æ–‡ä»¶: {input_file}")
    
    # è¿è¡ŒV1ç‰ˆæœ¬ï¼ˆæ¨èï¼‰
    print("\n" + "="*60)
    print("ğŸ”§ è¿è¡ŒV1ç‰ˆæœ¬ (æ¨è)")
    print("="*60)
    try:
        v1_result = test_audioldm2_ultimate_fix(input_file)
        if v1_result:
            print(f"âœ… V1å¤„ç†æˆåŠŸ")
        else:
            print(f"âŒ V1å¤„ç†å¤±è´¥")
    except Exception as e:
        print(f"âŒ V1å¤„ç†å¤±è´¥: {e}")
        v1_result = None
    
    # è¿è¡ŒV3ç‰ˆæœ¬ï¼ˆæ–°çš„å¹³è¡¡ä¼˜åŒ–ï¼‰
    print("\n" + "="*60)
    print("ğŸ”§ è¿è¡ŒV3ç‰ˆæœ¬ (å¹³è¡¡ä¼˜åŒ–)")
    print("="*60)
    try:
        v3_result = test_audioldm2_v3_balanced(input_file)
        if v3_result:
            print(f"âœ… V3å¤„ç†æˆåŠŸ")
        else:
            print(f"âŒ V3å¤„ç†å¤±è´¥")
    except Exception as e:
        print(f"âŒ V3å¤„ç†å¤±è´¥: {e}")
        v3_result = None
    
    # è¿è¡ŒV4ç‰ˆæœ¬ï¼ˆé«˜é¢‘ä¿®å¤ï¼‰
    print("\n" + "="*60)
    print("ğŸ”§ è¿è¡ŒV4ç‰ˆæœ¬ (é«˜é¢‘ä¿®å¤)")
    print("="*60)
    try:
        v4_result = test_audioldm2_v4_highfreq_fix(input_file)
        if v4_result:
            print(f"âœ… V4å¤„ç†æˆåŠŸ")
        else:
            print(f"âŒ V4å¤„ç†å¤±è´¥")
    except Exception as e:
        print(f"âŒ V4å¤„ç†å¤±è´¥: {e}")
        v4_result = None
    
    # ç»“æœå¯¹æ¯”
    print("\n" + "="*60)
    print("ï¿½ ç‰ˆæœ¬å¯¹æ¯”ç»“æœ")
    print("="*60)
    
    results = []
    if v1_result:
        results.append(("V1 (æ¨è)", v1_result))
    if v3_result:
        results.append(("V3 (å¹³è¡¡ä¼˜åŒ–)", v3_result))
    if v4_result:
        results.append(("V4 (é«˜é¢‘ä¿®å¤)", v4_result))
    
    if results:
        print(f"{'ç‰ˆæœ¬':<15} {'SNR(dB)':<10} {'ç›¸å…³æ€§':<10} {'è´¨é‡åˆ†æ•°':<10} {'è¾“å‡ºæ–‡ä»¶'}")
        print("-" * 80)
        for name, result in results:
            print(f"{name:<15} {result['snr']:<10.2f} {result['correlation']:<10.4f} {result['quality_score']:<10.2f} {Path(result['output_file']).name}")
        
        # æ¨èæœ€ä½³ç‰ˆæœ¬
        best_result = max(results, key=lambda x: x[1]['quality_score'])
        print(f"\nğŸ† æ¨èä½¿ç”¨: {best_result[0]}")
        print(f"   è´¨é‡åˆ†æ•°: {best_result[1]['quality_score']:.2f}")
        print(f"   è¾“å‡ºæ–‡ä»¶: {best_result[1]['output_file']}")
        
        # è¯¦ç»†å»ºè®®
        if best_result[0] == "V1 (æ¨è)":
            print(f"\nğŸ’¡ V1ç»§ç»­ä¿æŒæœ€ä½³æ•ˆæœï¼Œå»ºè®®ä½¿ç”¨V1ç»“æœ")
        elif best_result[0] == "V3 (å¹³è¡¡ä¼˜åŒ–)":
            print(f"\nğŸ‰ V3å¹³è¡¡ä¼˜åŒ–ç‰ˆæœ¬è¡¨ç°æ›´å¥½ï¼")
            print(f"   V3çš„æ”¹è¿›åŒ…æ‹¬ï¼šåŠ¨æ€éŸ³é‡è°ƒæ•´ã€æ··åˆé‡‡æ ·ã€æ™ºèƒ½åå¤„ç†ç­‰")
        elif best_result[0] == "V4 (é«˜é¢‘ä¿®å¤)":
            print(f"\nğŸ‰ V4é«˜é¢‘ä¿®å¤ç‰ˆæœ¬è¡¨ç°å‡ºè‰²ï¼")
            print(f"   V4ä¸“æ³¨äºæ¢å¤é«˜é¢‘ç»†èŠ‚ï¼Œé€‚åˆå¯¹é«˜é¢‘è¦æ±‚è¾ƒé«˜çš„åœºæ™¯")
        
    else:
        print("âŒ æ‰€æœ‰ç‰ˆæœ¬éƒ½å¤„ç†å¤±è´¥")
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼è¯·æ£€æŸ¥è¾“å‡ºæ–‡ä»¶å¹¶è¿›è¡Œä¸»è§‚å¬æ„Ÿæµ‹è¯•")
    print(f"ğŸ’¡ å»ºè®®ï¼šå®¢è§‚æŒ‡æ ‡ä»…ä¾›å‚è€ƒï¼Œæœ€ç»ˆæ•ˆæœè¯·ä»¥ä¸»è§‚å¬æ„Ÿä¸ºå‡†")


if __name__ == "__main__":
    main()
