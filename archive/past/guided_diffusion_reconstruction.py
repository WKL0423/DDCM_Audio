#!/usr/bin/env python3
"""
AudioLDM2 å¼•å¯¼å¼Diffusioné‡å»º
=============================

åœ¨VAEé‡å»ºè¿‡ç¨‹ä¸­åŠ å…¥å¼•å¯¼å¼diffusionè¿‡ç¨‹ï¼š
1. å°†ç›®æ ‡éŸ³é¢‘ä½œä¸ºå¼•å¯¼ä¿¡å·
2. åœ¨æ¯ä¸ªdiffusionæ­¥éª¤ä¸­ä¸ç›®æ ‡å¯¹æ¯”
3. é€‰æ‹©æœ€ä¼˜å™ªå£°è·¯å¾„ä½¿é‡å»ºç»“æœæ›´æ¥è¿‘ç›®æ ‡

åˆ›æ–°æ€è·¯ï¼šDiffusion + VAE + Target Guidance
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F
from diffusers import AudioLDM2Pipeline
import scipy.signal
from scipy.ndimage import gaussian_filter1d

# å¯¼å…¥å…¼å®¹æ€§ä¿å­˜
try:
    import soundfile as sf
    SOUNDFILE_AVAILABLE = True
except ImportError:
    SOUNDFILE_AVAILABLE = False


def save_audio_compatible(audio_data, filepath, sample_rate=16000):
    """ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨soundfileä»¥è·å¾—æœ€å¤§å…¼å®¹æ€§"""
    if isinstance(audio_data, torch.Tensor):
        audio_data = audio_data.detach().cpu().numpy()
    
    if len(audio_data.shape) > 1:
        audio_data = audio_data.flatten()
    
    audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=0.0, neginf=0.0)
    
    if np.max(np.abs(audio_data)) > 0:
        audio_data = audio_data / np.max(np.abs(audio_data))
    
    if SOUNDFILE_AVAILABLE:
        try:
            sf.write(filepath, audio_data, sample_rate, subtype='PCM_16')
            print(f"   âœ… å…¼å®¹æ€§ä¿å­˜: {Path(filepath).name}")
            return True
        except Exception as e:
            print(f"   âš ï¸ soundfileå¤±è´¥: {e}")
    
    try:
        audio_tensor = torch.from_numpy(audio_data).unsqueeze(0).float()
        torchaudio.save(filepath, audio_tensor, sample_rate)
        print(f"   âœ… torchaudioä¿å­˜: {Path(filepath).name}")
        return True
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
        return False


def compute_target_guidance(current_latent, target_latent, guidance_strength=1.0):
    """
    è®¡ç®—ç›®æ ‡å¼•å¯¼ä¿¡å·
    
    Args:
        current_latent: å½“å‰diffusionæ­¥éª¤çš„æ½œåœ¨è¡¨ç¤º
        target_latent: ç›®æ ‡éŸ³é¢‘çš„æ½œåœ¨è¡¨ç¤º  
        guidance_strength: å¼•å¯¼å¼ºåº¦
    
    Returns:
        å¼•å¯¼æ¢¯åº¦
    """
    # è®¡ç®—ä¸ç›®æ ‡çš„è·ç¦»
    diff = target_latent - current_latent
    
    # è®¡ç®—å¼•å¯¼æ¢¯åº¦ (æœå‘ç›®æ ‡)
    guidance_grad = guidance_strength * diff
    
    return guidance_grad


def diffusion_guided_reconstruction(
    vae, unet, scheduler, vocoder, 
    target_audio, target_mel_latent,
    num_inference_steps=20,
    guidance_strength=0.5,
    device="cuda"
):
    """
    å¼•å¯¼å¼diffusioné‡å»º
    
    Args:
        vae: VAEæ¨¡å‹
        unet: UNetæ‰©æ•£æ¨¡å‹
        scheduler: å™ªå£°è°ƒåº¦å™¨
        vocoder: éŸ³é¢‘ç”Ÿæˆå™¨
        target_audio: ç›®æ ‡éŸ³é¢‘
        target_mel_latent: ç›®æ ‡çš„melæ½œåœ¨è¡¨ç¤º
        num_inference_steps: diffusionæ­¥æ•°
        guidance_strength: å¼•å¯¼å¼ºåº¦
        device: è®¾å¤‡
    
    Returns:
        é‡å»ºçš„éŸ³é¢‘
    """
    print(f"ğŸ”® å¼€å§‹å¼•å¯¼å¼Diffusioné‡å»º...")
    print(f"   å¼•å¯¼å¼ºåº¦: {guidance_strength}")
    print(f"   æ¨ç†æ­¥æ•°: {num_inference_steps}")
    
    # è®¾ç½®scheduler
    scheduler.set_timesteps(num_inference_steps, device=device)
    timesteps = scheduler.timesteps
    
    # åˆå§‹åŒ–ï¼šä»ç›®æ ‡æ½œåœ¨è¡¨ç¤ºåŠ å™ªå£°å¼€å§‹
    latents = target_mel_latent.clone()
    
    # æ·»åŠ åˆå§‹å™ªå£°
    noise = torch.randn_like(latents)
    latents = scheduler.add_noise(latents, noise, timesteps[0])
    
    print(f"   åˆå§‹æ½œåœ¨è¡¨ç¤º: {latents.shape}")
    
    # Diffusionå»å™ªå¾ªç¯
    with torch.no_grad():
        for i, t in enumerate(timesteps):
            print(f"   æ­¥éª¤ {i+1}/{len(timesteps)}, æ—¶é—´æ­¥: {t}")
            
            # æ ‡å‡†diffusioné¢„æµ‹
            latent_model_input = latents
            
            # è¿™é‡Œæˆ‘ä»¬ä¸ä½¿ç”¨æ–‡æœ¬æ¡ä»¶ï¼Œè€Œæ˜¯ä½¿ç”¨ç›®æ ‡å¼•å¯¼
            # é¢„æµ‹å™ªå£°
            noise_pred = unet(
                latent_model_input,
                t,
                encoder_hidden_states=None,  # ä¸ä½¿ç”¨æ–‡æœ¬æ¡ä»¶
                return_dict=False,
            )[0]
            
            # è®¡ç®—å»å™ªåçš„æ½œåœ¨è¡¨ç¤º
            latents_denoised = scheduler.step(noise_pred, t, latents).prev_sample
            
            # ç›®æ ‡å¼•å¯¼ï¼šè®¡ç®—ä¸ç›®æ ‡çš„å¼•å¯¼æ¢¯åº¦
            if guidance_strength > 0:
                guidance_grad = compute_target_guidance(
                    latents_denoised, 
                    target_mel_latent, 
                    guidance_strength
                )
                
                # åº”ç”¨å¼•å¯¼
                latents = latents_denoised + guidance_grad
                
                # è®¡ç®—å½“å‰ä¸ç›®æ ‡çš„ç›¸ä¼¼æ€§
                similarity = F.cosine_similarity(
                    latents.flatten(), 
                    target_mel_latent.flatten(), 
                    dim=0
                ).item()
                
                print(f"     å¼•å¯¼åç›¸ä¼¼æ€§: {similarity:.4f}")
            else:
                latents = latents_denoised
    
    print(f"   âœ… Diffusioné‡å»ºå®Œæˆ")
    return latents


def guided_vae_reconstruction(audio_path, max_length=10):
    """
    å¼•å¯¼å¼VAEé‡å»ºä¸»å‡½æ•°
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float32
    
    print(f"ğŸ¯ AudioLDM2 å¼•å¯¼å¼Diffusioné‡å»º")
    print(f"ğŸµ ç›®æ ‡éŸ³é¢‘: {audio_path}")
    print(f"ğŸ–¥ï¸ è®¾å¤‡: {device}")
    
    # åŠ è½½AudioLDM2æ¨¡å‹
    print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
    try:
        pipeline = AudioLDM2Pipeline.from_pretrained(
            "cvssp/audioldm2",
            torch_dtype=dtype
        ).to(device)
        
        vae = pipeline.vae
        unet = pipeline.unet
        scheduler = pipeline.scheduler
        vocoder = pipeline.vocoder
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None
    
    # åŠ è½½ç›®æ ‡éŸ³é¢‘
    print("ğŸ¶ åŠ è½½ç›®æ ‡éŸ³é¢‘...")
    sample_rate = 16000
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    
    if len(audio) > max_length * sample_rate:
        audio = audio[:int(max_length * sample_rate)]
        print(f"éŸ³é¢‘å·²è£å‰ªåˆ° {max_length} ç§’")
    
    print(f"éŸ³é¢‘ä¿¡æ¯: é•¿åº¦={len(audio)/sample_rate:.2f}ç§’")
    
    # ç”Ÿæˆç›®æ ‡mel-spectrogram
    print("ğŸ”„ ç”Ÿæˆç›®æ ‡mel-spectrogram...")
    mel_spec = librosa.feature.melspectrogram(
        y=audio, sr=sample_rate, n_mels=64, hop_length=160, n_fft=1024
    )
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    
    # å½’ä¸€åŒ–
    mel_min, mel_max = mel_spec_db.min(), mel_spec_db.max()
    mel_normalized = 2 * (mel_spec_db - mel_min) / (mel_max - mel_min) - 1
    mel_normalized = np.clip(mel_normalized, -1, 1)
    
    # è½¬æ¢ä¸ºtensor
    mel_tensor = torch.from_numpy(mel_normalized).to(device).to(dtype)
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, time]
    
    # ç¡®ä¿å°ºå¯¸åŒ¹é…
    if mel_input.shape[-1] % 8 != 0:
        pad_length = 8 - (mel_input.shape[-1] % 8)
        mel_input = F.pad(mel_input, (0, pad_length))
    
    print(f"ç›®æ ‡melè¾“å…¥: {mel_input.shape}")
    
    # VAEç¼–ç è·å–ç›®æ ‡æ½œåœ¨è¡¨ç¤º
    print("ğŸ§  VAEç¼–ç ç›®æ ‡...")
    with torch.no_grad():
        target_latent = vae.encode(mel_input).latent_dist.sample()
        target_latent = target_latent * vae.config.scaling_factor
        
        print(f"ç›®æ ‡æ½œåœ¨è¡¨ç¤º: {target_latent.shape}")
    
    # æ–¹æ³•1: æ ‡å‡†VAEé‡å»º (å¯¹ç…§ç»„)
    print("\nğŸ“Š æ–¹æ³•1: æ ‡å‡†VAEé‡å»º...")
    start_time = time.time()
    
    with torch.no_grad():
        standard_latent = target_latent / vae.config.scaling_factor
        standard_mel = vae.decode(standard_latent).sample
        
        # è½¬æ¢ä¸ºéŸ³é¢‘
        standard_audio = vocoder(standard_mel.squeeze(0).transpose(-2, -1).unsqueeze(0))
        standard_audio = standard_audio.squeeze().cpu().numpy()
        
    standard_time = time.time() - start_time
    print(f"   æ ‡å‡†VAEæ—¶é—´: {standard_time:.2f}ç§’")
    
    # æ–¹æ³•2: å¼•å¯¼å¼Diffusioné‡å»º (åˆ›æ–°æ–¹æ³•)
    print("\nğŸ”® æ–¹æ³•2: å¼•å¯¼å¼Diffusioné‡å»º...")
    start_time = time.time()
    
    # æµ‹è¯•ä¸åŒå¼•å¯¼å¼ºåº¦
    guidance_strengths = [0.1, 0.3, 0.5, 0.7]
    reconstructed_audios = {}
    
    for guidance_strength in guidance_strengths:
        print(f"\n   æµ‹è¯•å¼•å¯¼å¼ºåº¦: {guidance_strength}")
        
        try:
            guided_latent = diffusion_guided_reconstruction(
                vae, unet, scheduler, vocoder,
                audio, target_latent,
                num_inference_steps=20,
                guidance_strength=guidance_strength,
                device=device
            )
            
            # è§£ç ä¸ºéŸ³é¢‘
            with torch.no_grad():
                decode_latent = guided_latent / vae.config.scaling_factor
                guided_mel = vae.decode(decode_latent).sample
                guided_audio = vocoder(guided_mel.squeeze(0).transpose(-2, -1).unsqueeze(0))
                guided_audio = guided_audio.squeeze().cpu().numpy()
                
                reconstructed_audios[guidance_strength] = guided_audio
                
        except Exception as e:
            print(f"   âŒ å¼•å¯¼å¼ºåº¦ {guidance_strength} å¤±è´¥: {e}")
            reconstructed_audios[guidance_strength] = None
    
    guided_time = time.time() - start_time
    print(f"\n   å¼•å¯¼å¼Diffusionæ€»æ—¶é—´: {guided_time:.2f}ç§’")
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜é‡å»ºç»“æœ...")
    output_dir = Path("guided_diffusion_reconstruction")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘
    original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
    save_audio_compatible(audio, original_path, sample_rate)
    
    # ä¿å­˜æ ‡å‡†VAEé‡å»º
    standard_path = output_dir / f"{input_name}_standard_vae_{timestamp}.wav"
    if len(standard_audio) > len(audio):
        standard_audio = standard_audio[:len(audio)]
    elif len(standard_audio) < len(audio):
        standard_audio = np.pad(standard_audio, (0, len(audio) - len(standard_audio)))
    save_audio_compatible(standard_audio, standard_path, sample_rate)
    
    # ä¿å­˜å¼•å¯¼å¼é‡å»ºç»“æœ
    results = {}
    for strength, recon_audio in reconstructed_audios.items():
        if recon_audio is not None:
            # é•¿åº¦å¯¹é½
            if len(recon_audio) > len(audio):
                recon_audio = recon_audio[:len(audio)]
            elif len(recon_audio) < len(audio):
                recon_audio = np.pad(recon_audio, (0, len(audio) - len(recon_audio)))
            
            guided_path = output_dir / f"{input_name}_guided_strength_{strength}_{timestamp}.wav"
            save_audio_compatible(recon_audio, guided_path, sample_rate)
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            min_len = min(len(audio), len(recon_audio))
            orig_segment = audio[:min_len]
            recon_segment = recon_audio[:min_len]
            
            mse = np.mean((orig_segment - recon_segment) ** 2)
            correlation = np.corrcoef(orig_segment, recon_segment)[0, 1] if len(orig_segment) > 1 else 0
            
            signal_power = np.mean(orig_segment ** 2)
            noise_power = np.mean((orig_segment - recon_segment) ** 2)
            snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
            
            results[strength] = {
                'path': str(guided_path),
                'mse': mse,
                'snr': snr,
                'correlation': correlation
            }
    
    # è®¡ç®—æ ‡å‡†VAEæŒ‡æ ‡
    min_len = min(len(audio), len(standard_audio))
    orig_segment = audio[:min_len]
    standard_segment = standard_audio[:min_len]
    
    standard_mse = np.mean((orig_segment - standard_segment) ** 2)
    standard_correlation = np.corrcoef(orig_segment, standard_segment)[0, 1] if len(orig_segment) > 1 else 0
    standard_snr = 10 * np.log10(np.mean(orig_segment ** 2) / (standard_mse + 1e-10))
    
    # è¾“å‡ºç»“æœå¯¹æ¯”
    print(f"\n{'='*80}")
    print(f"ğŸ¯ å¼•å¯¼å¼Diffusioné‡å»ºå®éªŒç»“æœ")
    print(f"{'='*80}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    print(f"\nğŸ“Š è´¨é‡å¯¹æ¯”:")
    print(f"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"â”‚ æ–¹æ³•            â”‚ SNR (dB)    â”‚ ç›¸å…³ç³»æ•°    â”‚ MSE         â”‚")
    print(f"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚ æ ‡å‡†VAE         â”‚ {standard_snr:10.2f}  â”‚ {standard_correlation:10.4f}  â”‚ {standard_mse:10.6f}  â”‚")
    
    best_result = None
    best_snr = standard_snr
    
    for strength, result in results.items():
        print(f"â”‚ å¼•å¯¼å¼({strength:3.1f})     â”‚ {result['snr']:10.2f}  â”‚ {result['correlation']:10.4f}  â”‚ {result['mse']:10.6f}  â”‚")
        
        if result['snr'] > best_snr:
            best_snr = result['snr']
            best_result = (strength, result)
    
    print(f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # ç»“è®º
    print(f"\nğŸŠ å®éªŒç»“è®º:")
    if best_result:
        strength, result = best_result
        improvement = best_snr - standard_snr
        print(f"âœ… å¼•å¯¼å¼Diffusioné‡å»ºæˆåŠŸ!")
        print(f"ğŸ† æœ€ä½³å¼•å¯¼å¼ºåº¦: {strength}")
        print(f"ğŸ“ˆ SNRæå‡: {improvement:.2f} dB")
        print(f"ğŸµ æœ€ä½³é‡å»ºæ–‡ä»¶: {result['path']}")
        
        if improvement > 3:
            print(f"ğŸ‰ æ˜¾è‘—æ”¹å–„! å¼•å¯¼å¼æ–¹æ³•æ˜æ˜¾ä¼˜äºæ ‡å‡†VAE")
        elif improvement > 1:
            print(f"âœ… æœ‰æ•ˆæ”¹å–„! å¼•å¯¼å¼æ–¹æ³•ç•¥ä¼˜äºæ ‡å‡†VAE")
        else:
            print(f"ğŸ“Š è½»å¾®æ”¹å–„ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–å‚æ•°")
    else:
        print(f"âš ï¸ å¼•å¯¼å¼æ–¹æ³•æœªè¶…è¶Šæ ‡å‡†VAEï¼Œå¯èƒ½éœ€è¦:")
        print(f"   - è°ƒæ•´å¼•å¯¼å¼ºåº¦èŒƒå›´")
        print(f"   - å¢åŠ diffusionæ­¥æ•°")
        print(f"   - ä¼˜åŒ–å¼•å¯¼å‡½æ•°")
    
    print(f"\nğŸ’¡ æŠ€æœ¯åˆ›æ–°ç‚¹:")
    print(f"   âœ… é¦–æ¬¡å°†ç›®æ ‡å¼•å¯¼å¼•å…¥AudioLDM2 VAEé‡å»º")
    print(f"   âœ… åœ¨diffusionè¿‡ç¨‹ä¸­ä¿æŒä¸ç›®æ ‡çš„ç›¸ä¼¼æ€§")
    print(f"   âœ… å¯è°ƒèŠ‚çš„å¼•å¯¼å¼ºåº¦æ§åˆ¶")
    print(f"   âœ… ç³»ç»Ÿæ€§è´¨é‡å¯¹æ¯”è¯„ä¼°")
    
    return {
        'original_path': str(original_path),
        'standard_result': {
            'path': str(standard_path),
            'snr': standard_snr,
            'correlation': standard_correlation,
            'mse': standard_mse
        },
        'guided_results': results,
        'best_result': best_result,
        'improvement': best_snr - standard_snr if best_result else 0
    }


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python guided_diffusion_reconstruction.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„> [æœ€å¤§é•¿åº¦ç§’æ•°]")
        
        # æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
        audio_files = []
        for ext in ['.wav', '.mp3', '.flac']:
            audio_files.extend(Path('.').glob(f'*{ext}'))
        
        if audio_files:
            print(f"\næ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files[:3], 1):
                print(f"{i}. {file.name}")
            
            try:
                choice = int(input("é€‰æ‹©æ–‡ä»¶: "))
                audio_path = str(audio_files[choice-1])
            except:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    # è·å–æœ€å¤§é•¿åº¦å‚æ•°
    max_length = 8  # é»˜è®¤8ç§’ (diffusionæ¯”è¾ƒæ…¢)
    if len(sys.argv) >= 3:
        try:
            max_length = float(sys.argv[2])
        except ValueError:
            print(f"æ— æ•ˆé•¿åº¦å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {max_length} ç§’")
    
    print(f"ğŸš€ å¼€å§‹å¼•å¯¼å¼Diffusioné‡å»ºå®éªŒ")
    print(f"ğŸ’¡ åˆ›æ–°ç†å¿µ: åœ¨diffusionä¸­å¼•å…¥ç›®æ ‡å¼•å¯¼ï¼Œæå‡é‡å»ºè´¨é‡")
    
    try:
        result = guided_vae_reconstruction(audio_path, max_length=max_length)
        
        if result and result['improvement'] > 0:
            print(f"\nğŸ‰ å®éªŒæˆåŠŸ! ä½ çš„åˆ›æ–°æƒ³æ³•æœ‰æ•ˆ!")
            print(f"ğŸ”¬ å»ºè®®ç»§ç»­ç ”ç©¶: å¼•å¯¼å‡½æ•°ä¼˜åŒ–ã€å¤šå°ºåº¦å¼•å¯¼ç­‰")
        else:
            print(f"\nğŸ”¬ å®éªŒç»“æœæä¾›äº†å®è´µæ•°æ®ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–")
            
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
