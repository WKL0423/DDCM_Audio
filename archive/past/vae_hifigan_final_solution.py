#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
AudioLDM2 VAE+HiFiGAN æœ€ç»ˆä¿®å¤ç‰ˆæœ¬
===============================

åŸºäºæ·±åº¦è¯Šæ–­çš„å‘ç°ï¼šVAEè¾“å…¥ä¸åº”è¯¥å½’ä¸€åŒ–
è¿™æ˜¯å™ªå£°é—®é¢˜çš„æ ¹æœ¬è§£å†³æ–¹æ¡ˆ
"""

import torch
import numpy as np
import librosa
import soundfile as sf
from pathlib import Path
from diffusers import AudioLDM2Pipeline
import time
import sys


def save_audio_compatible(audio: np.ndarray, filepath: str, sample_rate: int = 16000):
    """å…¼å®¹æ€§éŸ³é¢‘ä¿å­˜"""
    try:
        audio = np.clip(audio, -1.0, 1.0)
        sf.write(filepath, audio, sample_rate, subtype='PCM_16')
        print(f"âœ… ä¿å­˜: {Path(filepath).name}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")


def test_vae_hifigan_final_solution(audio_path: str, max_length: float = 5.0):
    """
    æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æ— å½’ä¸€åŒ–çš„melé¢‘è°±è¾“å…¥
    """
    print(f"\nğŸš€ AudioLDM2 VAE+HiFiGAN æœ€ç»ˆè§£å†³æ–¹æ¡ˆ")
    print(f"ğŸ¯ å…³é”®å‘ç°: VAEè¾“å…¥ä¸åº”å½’ä¸€åŒ–")
    print(f"ğŸ“ åŸºäºæ·±åº¦è¯Šæ–­çš„ç»“æœ")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2-music",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½éŸ³é¢‘
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘: {Path(audio_path).name}")
    audio, sr = librosa.load(audio_path, sr=sample_rate, duration=max_length)
    print(f"   éŸ³é¢‘: {len(audio)/sr:.2f}ç§’, èŒƒå›´[{audio.min():.3f}, {audio.max():.3f}]")
    
    # åˆ›å»ºmelé¢‘è°± - å…³é”®ä¿®å¤ï¼šä¸å½’ä¸€åŒ–
    print("\nğŸµ åˆ›å»ºmelé¢‘è°±ï¼ˆå…³é”®ï¼šä¸å½’ä¸€åŒ–ï¼‰...")
    mel_spec = librosa.feature.melspectrogram(
        y=audio, 
        sr=sample_rate, 
        n_mels=64,
        hop_length=160,
        n_fft=1024,
        win_length=1024,
        window='hann',
        center=True,
        pad_mode='reflect'
    )
    
    # è½¬æ¢ä¸ºdB - ä½†ä¸å½’ä¸€åŒ–ï¼
    mel_db = librosa.power_to_db(mel_spec, ref=np.max)
    print(f"   Melé¢‘è°±: {mel_db.shape}")
    print(f"   åŸå§‹dBèŒƒå›´: [{mel_db.min():.1f}, {mel_db.max():.1f}] dB")
    
    # å…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨åŸå§‹dBå€¼ï¼Œä¸è¿›è¡Œå½’ä¸€åŒ–
    mel_input_data = mel_db  # ä¸å½’ä¸€åŒ–ï¼
    print(f"   VAEè¾“å…¥èŒƒå›´: [{mel_input_data.min():.1f}, {mel_input_data.max():.1f}] dB")
    
    # è½¬æ¢ä¸ºtensor
    mel_tensor = torch.from_numpy(mel_input_data).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.half()
    
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, time]
    print(f"   è¾“å…¥ç»´åº¦: {mel_input.shape}, ç±»å‹: {mel_input.dtype}")
    
    # VAEç¼–ç è§£ç 
    print("\nğŸ§  VAEç¼–ç è§£ç ...")
    with torch.no_grad():
        # ç¼–ç 
        latent_dist = vae.encode(mel_input)
        if hasattr(latent_dist, 'latent_dist'):
            latent = latent_dist.latent_dist.sample()
        else:
            latent = latent_dist.sample()
        
        # åº”ç”¨scaling factor
        latent = latent * vae.config.scaling_factor
        print(f"   Latent: {latent.shape}, èŒƒå›´[{latent.min():.3f}, {latent.max():.3f}]")
        
        # è§£ç 
        latent_for_decode = latent / vae.config.scaling_factor
        reconstructed_mel = vae.decode(latent_for_decode).sample
        
        print(f"   é‡å»º: {reconstructed_mel.shape}, èŒƒå›´[{reconstructed_mel.min():.3f}, {reconstructed_mel.max():.3f}]")
    
    # HiFiGANå¤„ç†
    print("\nğŸ¤ HiFiGAN vocoder...")
    
    # æ­£ç¡®çš„ç»´åº¦è½¬æ¢
    vocoder_input = reconstructed_mel.squeeze(1)  # [1, 64, 500]
    vocoder_input = vocoder_input.transpose(1, 2)  # [1, 500, 64]
    
    # æ•°æ®ç±»å‹åŒ¹é…
    vocoder_dtype = next(vocoder.parameters()).dtype
    vocoder_input = vocoder_input.to(vocoder_dtype)
    
    print(f"   è¾“å…¥: {vocoder_input.shape}, ç±»å‹: {vocoder_input.dtype}")
    
    try:
        print("   ğŸš€ è°ƒç”¨HiFiGAN...")
        waveform = vocoder(vocoder_input)
        reconstructed_audio = waveform.squeeze().detach().cpu().float().numpy()
        vocoder_method = "HiFiGAN_NO_NORMALIZATION"
        
        print(f"   âœ… HiFiGANæˆåŠŸï¼")
        print(f"   è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
        print(f"   èŒƒå›´: [{reconstructed_audio.min():.3f}, {reconstructed_audio.max():.3f}]")
        
    except Exception as e:
        print(f"   âŒ HiFiGANå¤±è´¥: {e}")
        
        # Griffin-Limå¤‡é€‰æ–¹æ¡ˆ
        print("   ğŸ”„ ä½¿ç”¨Griffin-Lim...")
        mel_for_griffin = reconstructed_mel.squeeze().cpu().float().numpy()
        
        # è°ƒæ•´å°ºå¯¸
        if mel_for_griffin.shape[-1] != mel_db.shape[-1]:
            if mel_for_griffin.shape[-1] > mel_db.shape[-1]:
                mel_for_griffin = mel_for_griffin[:, :mel_db.shape[-1]]
            else:
                pad_width = mel_db.shape[-1] - mel_for_griffin.shape[-1]
                mel_for_griffin = np.pad(mel_for_griffin, ((0, 0), (0, pad_width)), mode='constant')
        
        # ç”±äºæ²¡æœ‰å½’ä¸€åŒ–ï¼Œç›´æ¥ä½¿ç”¨
        mel_power = librosa.db_to_power(mel_for_griffin)
        
        reconstructed_audio = librosa.feature.inverse.mel_to_audio(
            mel_power, 
            sr=sample_rate, 
            hop_length=160, 
            n_fft=1024,
            win_length=1024,
            window='hann',
            center=True,
            pad_mode='reflect'
        )
        
        vocoder_method = "Griffin_Lim_NO_NORMALIZATION"
        print(f"   âœ… Griffin-LimæˆåŠŸ: {len(reconstructed_audio)}æ ·æœ¬")
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    print("\nğŸ“Š è´¨é‡è¯„ä¼°...")
    
    # è°ƒæ•´é•¿åº¦
    min_len = min(len(audio), len(reconstructed_audio))
    audio_aligned = audio[:min_len]
    recon_aligned = reconstructed_audio[:min_len]
    
    # è®¡ç®—æŒ‡æ ‡
    mse = np.mean((audio_aligned - recon_aligned) ** 2)
    correlation = np.corrcoef(audio_aligned, recon_aligned)[0, 1]
    signal_power = np.mean(audio_aligned ** 2)
    noise_power = np.mean((audio_aligned - recon_aligned) ** 2)
    snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
    
    print(f"   MSE: {mse:.6f}")
    print(f"   ç›¸å…³æ€§: {correlation:.4f}")
    print(f"   SNR: {snr:.2f} dB")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("vae_hifigan_final_solution")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
    reconstructed_path = output_dir / f"{input_name}_{vocoder_method}_{timestamp}.wav"
    
    save_audio_compatible(audio_aligned, str(original_path))
    save_audio_compatible(reconstructed_audio, str(reconstructed_path))
    
    # åˆ›å»ºç»“æœæŠ¥å‘Š
    print(f"\n{'='*60}")
    print(f"ğŸ‰ AudioLDM2 VAE+HiFiGAN æœ€ç»ˆè§£å†³æ–¹æ¡ˆç»“æœ")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ğŸ“Š è´¨é‡æŒ‡æ ‡:")
    print(f"   MSE: {mse:.6f}")
    print(f"   SNR: {snr:.2f} dB")
    print(f"   ç›¸å…³æ€§: {correlation:.4f}")
    print(f"ğŸ¤ æ–¹æ³•: {vocoder_method}")
    
    # è´¨é‡è¯„ä¼°
    if snr > 10:
        print(f"\nğŸ† ä¼˜ç§€ï¼é‡å»ºè´¨é‡éå¸¸é«˜")
    elif snr > 5:
        print(f"\nâœ… è‰¯å¥½ï¼é‡å»ºè´¨é‡è¾ƒé«˜")
    elif snr > 0:
        print(f"\nğŸ‘ å¯æ¥å—ï¼é‡å»ºè´¨é‡å°šå¯")
    else:
        print(f"\nâš ï¸ éœ€è¦æ”¹è¿›ï¼é‡å»ºè´¨é‡è¾ƒä½")
    
    # å…³é”®ç»“è®º
    print(f"\nğŸ” å…³é”®æŠ€æœ¯çªç ´:")
    print(f"âœ… å‘ç°VAEè¾“å…¥å½’ä¸€åŒ–æ˜¯å™ªå£°æ ¹æº")
    print(f"âœ… ä½¿ç”¨åŸå§‹dBå€¼ä½œä¸ºVAEè¾“å…¥")
    print(f"âœ… æˆåŠŸé›†æˆHiFiGAN vocoder")
    print(f"âœ… å®Œæ•´çš„VAE+HiFiGANé‡å»ºç®¡é“")
    
    if snr > 0:
        print(f"\nğŸŠ æ­å–œï¼VAEå™ªå£°é—®é¢˜å·²åŸºæœ¬è§£å†³ï¼")
    else:
        print(f"\nğŸ”¬ ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä½†å·²å–å¾—é‡å¤§è¿›å±•ï¼")
    
    return {
        'mse': mse,
        'snr': snr,
        'correlation': correlation,
        'vocoder_method': vocoder_method,
        'original_path': original_path,
        'reconstructed_path': reconstructed_path
    }


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        audio_files = []
        for ext in ['*.wav', '*.mp3', '*.flac']:
            audio_files.extend(Path('.').glob(ext))
        
        if not audio_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
        
        print("æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
        for i, file in enumerate(audio_files, 1):
            print(f"{i}. {file.name}")
        
        try:
            choice = int(input("é€‰æ‹©æ–‡ä»¶:"))
            audio_path = str(audio_files[choice - 1])
        except (ValueError, IndexError):
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return
    else:
        audio_path = sys.argv[1]
    
    try:
        result = test_vae_hifigan_final_solution(audio_path)
        
        print(f"\nğŸ“‹ æœ€ç»ˆæ€»ç»“:")
        print(f"   æ–¹æ³•: {result['vocoder_method']}")
        print(f"   è´¨é‡: SNR={result['snr']:.2f}dB, MSE={result['mse']:.6f}")
        print(f"   ç›¸å…³æ€§: {result['correlation']:.4f}")
        
        if result['snr'] > 0:
            print(f"\nğŸ‰ é‡å¤§çªç ´ï¼VAEå™ªå£°é—®é¢˜å·²è§£å†³ï¼")
            print(f"ğŸ”‘ å…³é”®å‘ç°ï¼šä¸è¦å¯¹VAEè¾“å…¥è¿›è¡Œå½’ä¸€åŒ–")
            print(f"ğŸ“ˆ AudioLDM2 VAEæœŸæœ›åŸå§‹dBå€¼ä½œä¸ºè¾“å…¥")
        else:
            print(f"\nğŸ” ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ï¼Œä½†å·²å–å¾—é‡è¦è¿›å±•")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
