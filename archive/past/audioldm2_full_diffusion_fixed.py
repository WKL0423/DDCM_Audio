"""
AudioLDM2 å®Œæ•´ Diffusion Pipeline é‡å»º
åŒ…å«çœŸæ­£çš„ diffusion å»å™ªè¿‡ç¨‹ï¼Œè€Œä¸ä»…ä»…æ˜¯ VAE encode/decode
"""

import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Optional, Union, Tuple, List
from diffusers import AudioLDM2Pipeline, DDIMScheduler, PNDMScheduler
import warnings
warnings.filterwarnings("ignore")

class AudioLDM2FullDiffusion:
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """
        åˆå§‹åŒ– AudioLDM2 å®Œæ•´ diffusion pipeline
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸµ åˆå§‹åŒ– AudioLDM2 å®Œæ•´ diffusion pipeline...")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        
        # åŠ è½½å®Œæ•´çš„ AudioLDM2 pipeline
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        # è®¾ç½®ä¸åŒçš„è°ƒåº¦å™¨é€‰é¡¹
        self.schedulers = {
            "ddim": DDIMScheduler.from_config(self.pipeline.scheduler.config),
            "pndm": PNDMScheduler.from_config(self.pipeline.scheduler.config),
            "default": self.pipeline.scheduler
        }
        
        print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        print(f"   ğŸ“Š VAE channels: {self.pipeline.vae.config.latent_channels}")
        print(f"   ğŸ”„ è°ƒåº¦å™¨é€‰é¡¹: {list(self.schedulers.keys())}")
        
    def audio_to_mel_features(self, audio: torch.Tensor) -> torch.Tensor:
        """
        å°†éŸ³é¢‘è½¬æ¢ä¸º mel-spectrogram features
        
        Args:
            audio: éŸ³é¢‘å¼ é‡ [samples] æˆ– [1, samples]
            
        Returns:
            mel_features: mel-spectrogram features
        """
        with torch.no_grad():
            # ç¡®ä¿éŸ³é¢‘åœ¨ CPU ä¸Šä¸”ä¸º numpy æ ¼å¼
            if audio.is_cuda:
                audio = audio.cpu()
            
            if audio.dim() == 2:
                audio = audio.squeeze(0)
            
            audio_numpy = audio.numpy()
            
            # ä½¿ç”¨ ClapFeatureExtractor å¤„ç†éŸ³é¢‘
            inputs = self.pipeline.feature_extractor(
                audio_numpy,
                sampling_rate=48000,
                return_tensors="pt"
            )
            
            mel_features = inputs["input_features"].to(self.device)
            
            # ç¡®ä¿æ­£ç¡®çš„ç»´åº¦ [batch, channels, height, width]
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)  # [batch, 1, freq, time]
            
            return mel_features
    
    def encode_audio_to_latent(self, audio: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç éŸ³é¢‘ä¸ºæ½œåœ¨è¡¨ç¤º
        
        Args:
            audio: éŸ³é¢‘å¼ é‡
            
        Returns:
            latent: æ½œåœ¨è¡¨ç¤º
        """
        with torch.no_grad():
            # è½¬æ¢ä¸º mel features
            mel_features = self.audio_to_mel_features(audio)
            
            # ä½¿ç”¨ VAE ç¼–ç 
            latent_dist = self.pipeline.vae.encode(mel_features)
            
            # è·å–æ½œåœ¨è¡¨ç¤º
            if hasattr(latent_dist, 'latent_dist'):
                latent = latent_dist.latent_dist.mode() if hasattr(latent_dist.latent_dist, 'mode') else latent_dist.latent_dist.sample()
            else:
                latent = latent_dist.mode() if hasattr(latent_dist, 'mode') else latent_dist.sample()
            
            return latent
    
    def decode_latent_to_audio(self, latent: torch.Tensor) -> torch.Tensor:
        """
        è§£ç æ½œåœ¨è¡¨ç¤ºä¸ºéŸ³é¢‘
        
        Args:
            latent: æ½œåœ¨è¡¨ç¤º
            
        Returns:
            audio: éŸ³é¢‘å¼ é‡
        """
        with torch.no_grad():
            # ä½¿ç”¨ VAE è§£ç 
            mel = self.pipeline.vae.decode(latent).sample
            
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ vocoder æˆ–å…¶ä»–æ–¹æ³•å°† mel è½¬æ¢ä¸ºéŸ³é¢‘
            # ä½†æ˜¯ AudioLDM2Pipeline å¯èƒ½æ²¡æœ‰ç›´æ¥çš„ vocoder
            # æˆ‘ä»¬æš‚æ—¶è¿”å› mel spectrogramï¼Œè®© pipeline çš„å®Œæ•´æµç¨‹å¤„ç†
            return mel
    
    def reconstruct_with_vae_only(self, audio: torch.Tensor) -> torch.Tensor:
        """
        ä»…ä½¿ç”¨ VAE é‡å»ºï¼ˆæ—  diffusionï¼‰
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘
            
        Returns:
            reconstructed_mel: é‡å»ºçš„ mel-spectrogram
        """
        print("ğŸ”„ VAE-only é‡å»º...")
        
        # ç¼–ç 
        latent = self.encode_audio_to_latent(audio)
        print(f"   æ½œåœ¨è¡¨ç¤ºå½¢çŠ¶: {latent.shape}")
        
        # è§£ç 
        reconstructed_mel = self.decode_latent_to_audio(latent)
        print(f"   é‡å»º mel å½¢çŠ¶: {reconstructed_mel.shape}")
        
        return reconstructed_mel
    
    def reconstruct_with_diffusion(self, 
                                  audio: torch.Tensor,
                                  prompt: str = "high quality music",
                                  num_inference_steps: int = 20,
                                  guidance_scale: float = 7.5) -> torch.Tensor:
        """
        ä½¿ç”¨å®Œæ•´ diffusion è¿‡ç¨‹é‡å»ºéŸ³é¢‘
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘
            prompt: æ–‡æœ¬æç¤º
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            
        Returns:
            reconstructed_audio: é‡å»ºçš„éŸ³é¢‘
        """
        print(f"ğŸµ Diffusion é‡å»º...")
        print(f"   ğŸ“ æç¤º: {prompt}")
        print(f"   ğŸ”„ æ¨ç†æ­¥æ•°: {num_inference_steps}")
        print(f"   ğŸšï¸ å¼•å¯¼å¼ºåº¦: {guidance_scale}")
        
        # ä½¿ç”¨ pipeline ç›´æ¥ç”Ÿæˆï¼Œè€Œä¸æ˜¯ç¼–ç -è§£ç 
        # è¿™æ‰æ˜¯çœŸæ­£çš„ diffusion è¿‡ç¨‹
        with torch.no_grad():
            # è®¡ç®—éŸ³é¢‘é•¿åº¦ï¼ˆç§’ï¼‰
            audio_length = audio.shape[-1] / 48000.0
            audio_length = min(max(audio_length, 2.0), 10.0)  # é™åˆ¶åœ¨ 2-10 ç§’
            
            result = self.pipeline(
                prompt=prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                audio_length_in_s=audio_length
            )
            
            generated_audio = result.audios[0]
            
        print(f"   âœ… ç”ŸæˆéŸ³é¢‘å½¢çŠ¶: {generated_audio.shape}")
        return generated_audio
    
    def compare_methods(self, 
                       input_audio: torch.Tensor,
                       output_dir: str = "diffusion_comparison") -> dict:
        """
        å¯¹æ¯”ä¸åŒé‡å»ºæ–¹æ³•
        
        Args:
            input_audio: è¾“å…¥éŸ³é¢‘
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            results: å¯¹æ¯”ç»“æœ
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        results = {}
        
        # 1. VAE-only é‡å»ºï¼ˆæ—  diffusionï¼‰
        print("\nğŸ”„ æ–¹æ³• 1: VAE-only é‡å»ºï¼ˆæ—  diffusionï¼‰")
        try:
            vae_only_mel = self.reconstruct_with_vae_only(input_audio)
            results["vae_only_mel"] = vae_only_mel
            print("   âœ… VAE-only å®Œæˆï¼ˆè¿”å› mel-spectrogramï¼‰")
        except Exception as e:
            print(f"   âŒ VAE-only å¤±è´¥: {e}")
        
        # 2-4. ä¸åŒå‚æ•°çš„ diffusion é‡å»º
        diffusion_configs = [
            {"name": "quick_diffusion", "steps": 10, "guidance": 5.0, "prompt": "music"},
            {"name": "balanced_diffusion", "steps": 20, "guidance": 7.5, "prompt": "high quality music"},
            {"name": "detailed_diffusion", "steps": 30, "guidance": 10.0, "prompt": "high quality classical music with rich harmonics"}
        ]
        
        for config in diffusion_configs:
            print(f"\nğŸ”„ æ–¹æ³•: {config['name']}")
            try:
                diffusion_audio = self.reconstruct_with_diffusion(
                    input_audio,
                    prompt=config["prompt"],
                    num_inference_steps=config["steps"],
                    guidance_scale=config["guidance"]
                )
                results[config["name"]] = diffusion_audio
                print(f"   âœ… {config['name']} å®Œæˆ")
            except Exception as e:
                print(f"   âŒ {config['name']} å¤±è´¥: {e}")
        
        # ä¿å­˜éŸ³é¢‘ç»“æœ
        sample_rate = 16000
        for method_name, result in results.items():
            if "mel" in method_name:
                # è·³è¿‡ mel-spectrogram ç»“æœï¼Œå› ä¸ºå®ƒä»¬ä¸æ˜¯éŸ³é¢‘
                continue
                
            output_file = output_path / f"{method_name}_output.wav"
            
            # ç¡®ä¿éŸ³é¢‘æ ¼å¼æ­£ç¡®
            if isinstance(result, torch.Tensor):
                if result.dim() == 1:
                    audio_to_save = result.unsqueeze(0)
                else:
                    audio_to_save = result
            else:
                audio_to_save = torch.tensor(result)
                if audio_to_save.dim() == 1:
                    audio_to_save = audio_to_save.unsqueeze(0)
            
            # ä¿å­˜éŸ³é¢‘
            try:
                torchaudio.save(str(output_file), audio_to_save.cpu(), sample_rate)
                print(f"   ğŸ’¾ ä¿å­˜: {output_file}")
            except Exception as e:
                print(f"   âŒ ä¿å­˜å¤±è´¥ {output_file}: {e}")
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘
        original_file = output_path / "original_input.wav"
        if input_audio.dim() == 1:
            input_audio_save = input_audio.unsqueeze(0)
        else:
            input_audio_save = input_audio
        
        try:
            # é‡é‡‡æ ·åŸå§‹éŸ³é¢‘åˆ° 16kHz ç”¨äºå¯¹æ¯”
            if input_audio_save.shape[-1] > 16000 * 10:  # å‡è®¾åŸå§‹æ˜¯ 48kHz
                resampler = torchaudio.transforms.Resample(48000, 16000)
                input_audio_save = resampler(input_audio_save)
            
            torchaudio.save(str(original_file), input_audio_save.cpu(), sample_rate)
            print(f"   ğŸ’¾ ä¿å­˜åŸå§‹éŸ³é¢‘: {original_file}")
        except Exception as e:
            print(f"   âŒ ä¿å­˜åŸå§‹éŸ³é¢‘å¤±è´¥: {e}")
        
        return results

def save_audio_compatible(audio: torch.Tensor, 
                         filepath: str, 
                         sample_rate: int = 16000,
                         normalize: bool = True) -> None:
    """
    å…¼å®¹æ€§éŸ³é¢‘ä¿å­˜å‡½æ•°
    """
    # ç¡®ä¿éŸ³é¢‘åœ¨ CPU ä¸Š
    if audio.is_cuda:
        audio = audio.cpu()
    
    # å¤„ç†éŸ³é¢‘ç»´åº¦
    if audio.dim() == 3:
        audio = audio.squeeze(0)
    if audio.dim() == 1:
        audio = audio.unsqueeze(0)
    
    # å½’ä¸€åŒ–
    if normalize:
        audio = audio / (audio.abs().max() + 1e-8)
        audio = audio * 0.8  # é˜²æ­¢å‰Šæ³¢
    
    # ä¿å­˜
    torchaudio.save(filepath, audio, sample_rate)

def main():
    """ä¸»å‡½æ•°ï¼šå¤„ç† AudioLDM2_Music_output.wav"""
    
    # è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        return
    
    print(f"ğŸµ å¤„ç†æ–‡ä»¶: {input_file}")
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = torchaudio.load(input_file)
    print(f"   ğŸ“Š åŸå§‹éŸ³é¢‘: {audio.shape}, é‡‡æ ·ç‡: {sr}")
    
    # é‡é‡‡æ ·åˆ° 48kHzï¼ˆAudioLDM2 çš„ ClapFeatureExtractor è¦æ±‚ï¼‰
    if sr != 48000:
        resampler = torchaudio.transforms.Resample(sr, 48000)
        audio = resampler(audio)
        print(f"   ğŸ”„ é‡é‡‡æ ·åˆ° 48kHz: {audio.shape}")
    
    # è½¬ä¸ºå•å£°é“
    if audio.shape[0] > 1:
        audio = audio.mean(dim=0, keepdim=True)
        print(f"   ğŸ”Š è½¬ä¸ºå•å£°é“: {audio.shape}")
    
    # é™åˆ¶éŸ³é¢‘é•¿åº¦ï¼ˆé¿å…æ˜¾å­˜ä¸è¶³ï¼‰
    max_length = 48000 * 10  # 10 ç§’
    if audio.shape[-1] > max_length:
        audio = audio[..., :max_length]
        print(f"   âœ‚ï¸ è£å‰ªåˆ° 10 ç§’: {audio.shape}")
    
    # åˆå§‹åŒ– AudioLDM2 å®Œæ•´ diffusion pipeline
    try:
        reconstructor = AudioLDM2FullDiffusion("cvssp/audioldm2-music")
    except Exception as e:
        print(f"âŒ éŸ³ä¹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("   å°è¯•ä½¿ç”¨åŸºç¡€æ¨¡å‹...")
        try:
            reconstructor = AudioLDM2FullDiffusion("cvssp/audioldm2")
        except Exception as e2:
            print(f"âŒ åŸºç¡€æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
            return
    
    # æ‰§è¡Œå¯¹æ¯”æµ‹è¯•
    print("\n" + "="*50)
    print("ğŸ¯ å¼€å§‹å¯¹æ¯” VAE-only vs å®Œæ•´ Diffusion æ–¹æ³•")
    print("="*50)
    
    results = reconstructor.compare_methods(audio.squeeze(0))
    
    print("\nâœ… å®Œæ•´ diffusion é‡å»ºæµ‹è¯•å®Œæˆï¼")
    print("ğŸ“ æŸ¥çœ‹ diffusion_comparison/ ç›®å½•è·å–ç»“æœæ–‡ä»¶")
    
    print("\nğŸ“‹ ç»“æœæ€»ç»“:")
    print("   ğŸ” VAE-only: åªåš encode â†’ decodeï¼Œæ— å™ªå£°å»é™¤")
    print("   ğŸµ Diffusion: å®Œæ•´çš„æ–‡æœ¬å¼•å¯¼ç”Ÿæˆï¼ŒåŒ…å«å»å™ªè¿‡ç¨‹")
    print("   ğŸ¯ å¯¹æ¯”: Diffusion èƒ½ç”Ÿæˆå…¨æ–°éŸ³é¢‘ï¼Œè€Œéé‡å»ºåŸéŸ³é¢‘")
    
    print("\nğŸ‰ AudioLDM2 å®Œæ•´ diffusion æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()
