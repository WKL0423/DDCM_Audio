#!/usr/bin/env python3
"""
AudioLDM2 é«˜é¢‘ä¿®å¤ä¸“ç”¨è„šæœ¬
V5ç‰ˆæœ¬ï¼šå°è¯•é«˜é‡‡æ ·ç‡è¾“å‡ºå’Œé«˜é¢‘å¢å¼º
"""

import torch
import numpy as np
import librosa
import soundfile as sf
from diffusers import AudioLDM2Pipeline
from pathlib import Path
import time
import os

def save_audio_compatible(audio, path, sr=16000):
    """å…¼å®¹çš„éŸ³é¢‘ä¿å­˜å‡½æ•°"""
    try:
        # ç¡®ä¿éŸ³é¢‘æ˜¯numpyæ•°ç»„
        if isinstance(audio, torch.Tensor):
            audio = audio.cpu().numpy()
        
        # ç¡®ä¿éŸ³é¢‘æ˜¯ä¸€ç»´çš„
        if len(audio.shape) > 1:
            audio = audio.squeeze()
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        if audio.max() > 1.0 or audio.min() < -1.0:
            audio = audio / np.max(np.abs(audio))
        
        # ä¿å­˜ä¸ºWAVæ–‡ä»¶
        sf.write(path, audio, sr)
        print(f"   ğŸ’¾ ä¿å­˜æˆåŠŸ: {path}")
        return True
        
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥ {path}: {e}")
        return False

def high_freq_enhancement(audio, sr=16000, enhancement_factor=1.5):
    """é«˜é¢‘å¢å¼ºå¤„ç†"""
    try:
        # è®¡ç®—é¢‘è°±
        fft = np.fft.fft(audio)
        freqs = np.fft.fftfreq(len(audio), 1/sr)
        
        # å®šä¹‰é«˜é¢‘èŒƒå›´ï¼ˆ4kHzä»¥ä¸Šï¼‰
        high_freq_mask = np.abs(freqs) > 4000
        
        # å¯¹é«˜é¢‘éƒ¨åˆ†è¿›è¡Œå¢å¼º
        fft[high_freq_mask] *= enhancement_factor
        
        # è½¬æ¢å›æ—¶åŸŸ
        enhanced_audio = np.real(np.fft.ifft(fft))
        
        return enhanced_audio
    except:
        return audio

def test_v5_high_sample_rate(audio_path, max_length=10.0):
    """
    V5ç‰ˆæœ¬ï¼šé«˜é‡‡æ ·ç‡è¾“å‡º + é«˜é¢‘å¢å¼º
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ğŸ¯ V5: é«˜é‡‡æ ·ç‡è¾“å‡º + é«˜é¢‘å¢å¼º")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # åŠ è½½AudioLDM2 pipeline
    print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2-music",  # ä½¿ç”¨éŸ³ä¹ä¸“ç”¨æ¨¡å‹
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   VAE scaling_factor: {pipeline.vae.config.scaling_factor}")
    print(f"   Vocoderç±»å‹: {type(pipeline.vocoder).__name__}")
    
    # è·å–feature extractorå‚æ•°
    fe_sr = pipeline.feature_extractor.sampling_rate
    print(f"   FeatureExtractoré‡‡æ ·ç‡: {fe_sr} Hz")
    
    # åŠ è½½éŸ³é¢‘ - ä½¿ç”¨é«˜é‡‡æ ·ç‡
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘: {Path(audio_path).name}")
    
    # ä½¿ç”¨48kHzä¿æŒæ›´å¤šé«˜é¢‘ä¿¡æ¯
    audio_48k, _ = librosa.load(audio_path, sr=48000, duration=max_length)
    audio_fe_sr, _ = librosa.load(audio_path, sr=fe_sr, duration=max_length)
    
    print(f"   48kHzéŸ³é¢‘: {len(audio_48k)/48000:.2f}ç§’, èŒƒå›´[{audio_48k.min():.3f}, {audio_48k.max():.3f}]")
    print(f"   {fe_sr}HzéŸ³é¢‘: {len(audio_fe_sr)/fe_sr:.2f}ç§’, èŒƒå›´[{audio_fe_sr.min():.3f}, {audio_fe_sr.max():.3f}]")
    
    # åˆ†æåŸå§‹éŸ³é¢‘çš„é¢‘è°±ç‰¹å¾
    print("ğŸ” åˆ†æåŸå§‹éŸ³é¢‘é¢‘è°±...")
    fft_orig = np.abs(np.fft.fft(audio_48k[:48000]))[:24000]  # å–1ç§’åˆ†æ
    freqs = np.fft.fftfreq(48000, 1/48000)[:24000]
    
    # è®¡ç®—ä¸åŒé¢‘æ®µçš„èƒ½é‡
    low_freq_mask = freqs < 4000
    mid_freq_mask = (freqs >= 4000) & (freqs < 8000)
    high_freq_mask = freqs >= 8000
    
    low_energy = np.sum(fft_orig[low_freq_mask])
    mid_energy = np.sum(fft_orig[mid_freq_mask])
    high_energy = np.sum(fft_orig[high_freq_mask])
    total_energy = low_energy + mid_energy + high_energy
    
    print(f"   ä½é¢‘èƒ½é‡ (0-4kHz): {low_energy/total_energy*100:.1f}%")
    print(f"   ä¸­é¢‘èƒ½é‡ (4-8kHz): {mid_energy/total_energy*100:.1f}%")
    print(f"   é«˜é¢‘èƒ½é‡ (8-24kHz): {high_energy/total_energy*100:.1f}%")
    
    # V5ç‰¹è‰²ï¼šä½¿ç”¨ClapFeatureExtractorå¤„ç†
    print("ğŸµ V5: ä½¿ç”¨ClapFeatureExtractor...")
    try:
        # è½»å¾®å½’ä¸€åŒ–
        audio_input = audio_fe_sr.copy()
        peak_value = np.max(np.abs(audio_input))
        if peak_value > 0:
            audio_input = audio_input / peak_value * 0.98
        
        features = pipeline.feature_extractor(
            audio_input, 
            return_tensors="pt", 
            sampling_rate=fe_sr
        )
        
        mel_input = features.input_features.to(device)
        if device == "cuda":
            mel_input = mel_input.half()
        
        print(f"   âœ… ClapFeatureExtractoræˆåŠŸ")
        print(f"   è¾“å…¥: {mel_input.shape}")
        print(f"   èŒƒå›´: [{mel_input.min():.3f}, {mel_input.max():.3f}]")
        
    except Exception as e:
        print(f"   âŒ ClapFeatureExtractorå¤±è´¥: {e}")
        return None
    
    # VAEç¼–ç è§£ç 
    print("ğŸ§  V5: VAEç¼–ç è§£ç ...")
    with torch.no_grad():
        # ç¼–ç 
        latent_dist = pipeline.vae.encode(mel_input)
        latent = latent_dist.latent_dist.mode()
        latent = latent * pipeline.vae.config.scaling_factor
        
        print(f"   ç¼–ç latent: {latent.shape}")
        print(f"   LatentèŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
        
        # è§£ç 
        latent_for_decode = latent / pipeline.vae.config.scaling_factor
        reconstructed_mel = pipeline.vae.decode(latent_for_decode).sample
        
        print(f"   è§£ç è¾“å‡º: {reconstructed_mel.shape}")
        print(f"   è§£ç èŒƒå›´: [{reconstructed_mel.min():.3f}, {reconstructed_mel.max():.3f}]")
    
    # HiFiGAN vocoderå¤„ç†
    print("ğŸ¤ V5: HiFiGAN vocoder...")
    try:
        waveform = pipeline.mel_spectrogram_to_waveform(reconstructed_mel)
        reconstructed_audio = waveform.squeeze().cpu().detach().numpy()
        print(f"   âœ… VocoderæˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
        
        # æ£€æŸ¥vocoderè¾“å‡ºé‡‡æ ·ç‡
        vocoder_sr = len(reconstructed_audio) / max_length
        print(f"   Vocoderè¾“å‡ºé‡‡æ ·ç‡: {vocoder_sr:.0f} Hz")
        
    except Exception as e:
        print(f"   âŒ Vocoderå¤±è´¥: {e}")
        return None
    
    # V5ç‰¹è‰²ï¼šé«˜é¢‘å¢å¼ºåå¤„ç†
    print("ğŸ”§ V5: é«˜é¢‘å¢å¼ºåå¤„ç†...")
    
    # ä¸Šé‡‡æ ·åˆ°48kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
    if len(reconstructed_audio) < len(audio_48k):
        # ä½¿ç”¨librosaä¸Šé‡‡æ ·
        current_sr = len(reconstructed_audio) / max_length
        reconstructed_48k = librosa.resample(
            reconstructed_audio, 
            orig_sr=current_sr, 
            target_sr=48000
        )
        print(f"   ä¸Šé‡‡æ ·åˆ°48kHz: {len(reconstructed_48k)}æ ·æœ¬")
    else:
        reconstructed_48k = reconstructed_audio[:len(audio_48k)]
    
    # é«˜é¢‘å¢å¼º
    print("ğŸµ åº”ç”¨é«˜é¢‘å¢å¼º...")
    enhanced_audio = high_freq_enhancement(reconstructed_48k, sr=48000, enhancement_factor=2.0)
    
    # åˆ†æå¢å¼ºåçš„é¢‘è°±
    fft_enhanced = np.abs(np.fft.fft(enhanced_audio[:48000]))[:24000]
    
    enhanced_low_energy = np.sum(fft_enhanced[low_freq_mask])
    enhanced_mid_energy = np.sum(fft_enhanced[mid_freq_mask])
    enhanced_high_energy = np.sum(fft_enhanced[high_freq_mask])
    enhanced_total_energy = enhanced_low_energy + enhanced_mid_energy + enhanced_high_energy
    
    print(f"   å¢å¼ºåä½é¢‘èƒ½é‡: {enhanced_low_energy/enhanced_total_energy*100:.1f}%")
    print(f"   å¢å¼ºåä¸­é¢‘èƒ½é‡: {enhanced_mid_energy/enhanced_total_energy*100:.1f}%")
    print(f"   å¢å¼ºåé«˜é¢‘èƒ½é‡: {enhanced_high_energy/enhanced_total_energy*100:.1f}%")
    
    # éŸ³é‡åŒ¹é…
    ref_rms = np.sqrt(np.mean(audio_48k ** 2))
    enhanced_rms = np.sqrt(np.mean(enhanced_audio ** 2))
    
    if enhanced_rms > 0:
        volume_ratio = ref_rms / enhanced_rms
        volume_ratio = np.clip(volume_ratio, 0.3, 3.0)
        enhanced_audio = enhanced_audio * volume_ratio
        print(f"   éŸ³é‡åŒ¹é…: {enhanced_rms:.4f} -> {ref_rms:.4f} (æ¯”ä¾‹: {volume_ratio:.2f})")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("vae_hifigan_ultimate_fix")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜48kHzé«˜è´¨é‡ç‰ˆæœ¬
    original_path = output_dir / f"{input_name}_original_48k_{timestamp}.wav"
    enhanced_path = output_dir / f"{input_name}_V5_Enhanced_48k_{timestamp}.wav"
    
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    save_audio_compatible(audio_48k, original_path, sr=48000)
    save_audio_compatible(enhanced_audio, enhanced_path, sr=48000)
    
    # åŒæ—¶ä¿å­˜16kHzç‰ˆæœ¬ä¾›å¯¹æ¯”
    audio_16k = librosa.resample(audio_48k, orig_sr=48000, target_sr=16000)
    enhanced_16k = librosa.resample(enhanced_audio, orig_sr=48000, target_sr=16000)
    
    original_16k_path = output_dir / f"{input_name}_original_16k_{timestamp}.wav"
    enhanced_16k_path = output_dir / f"{input_name}_V5_Enhanced_16k_{timestamp}.wav"
    
    save_audio_compatible(audio_16k, original_16k_path, sr=16000)
    save_audio_compatible(enhanced_16k, enhanced_16k_path, sr=16000)
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    min_len = min(len(audio_48k), len(enhanced_audio))
    reference_audio = audio_48k[:min_len]
    enhanced_audio = enhanced_audio[:min_len]
    
    mse = np.mean((reference_audio - enhanced_audio) ** 2)
    snr = 10 * np.log10(np.mean(reference_audio ** 2) / (mse + 1e-10))
    correlation = np.corrcoef(reference_audio, enhanced_audio)[0, 1] if len(reference_audio) > 1 else 0
    
    # é«˜é¢‘ä¿æŒç‡
    high_freq_preserve_rate = enhanced_high_energy / high_energy if high_energy > 0 else 0
    
    print(f"\n{'='*60}")
    print(f"ğŸ¯ V5ç»“æœ (é«˜é‡‡æ ·ç‡ + é«˜é¢‘å¢å¼º)")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘ (48kHz): {original_path}")
    print(f"ğŸ“ å¢å¼ºéŸ³é¢‘ (48kHz): {enhanced_path}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘ (16kHz): {original_16k_path}")
    print(f"ğŸ“ å¢å¼ºéŸ³é¢‘ (16kHz): {enhanced_16k_path}")
    print(f"ğŸ“Š SNR: {snr:.2f} dB")
    print(f"ğŸ“Š MSE: {mse:.6f}")
    print(f"ğŸ“Š ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"ğŸ“Š é«˜é¢‘ä¿æŒç‡: {high_freq_preserve_rate:.3f}")
    print(f"\nğŸ”¬ V5ç‰¹è‰²:")
    print(f"   âœ… 48kHzé«˜é‡‡æ ·ç‡è¾“å‡º")
    print(f"   âœ… é«˜é¢‘å¢å¼ºåå¤„ç†")
    print(f"   âœ… è¯¦ç»†é¢‘è°±åˆ†æ")
    print(f"   âœ… åŒé‡‡æ ·ç‡è¾“å‡º")
    
    if high_freq_preserve_rate > 0.5:
        print(f"ğŸ‰ V5é«˜é¢‘ä¿æŒè‰¯å¥½ï¼")
    elif high_freq_preserve_rate > 0.2:
        print(f"âœ… V5é«˜é¢‘æœ‰ä¸€å®šæ”¹å–„")
    else:
        print(f"âš ï¸ V5é«˜é¢‘æ”¹å–„æœ‰é™")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    print(f"   1. å¯¹æ¯”48kHzå’Œ16kHzç‰ˆæœ¬çš„å¬æ„Ÿå·®å¼‚")
    print(f"   2. ä½¿ç”¨é¢‘è°±åˆ†æå·¥å…·æŸ¥çœ‹é«˜é¢‘æ¢å¤æ•ˆæœ")
    print(f"   3. å¦‚æœæ•ˆæœä»ä¸ç†æƒ³ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘å…¶ä»–æ–¹æ³•")

def main():
    """ä¸»å‡½æ•°"""
    input_file = "AudioLDM2_Music_output.wav"
    
    print("ğŸµ V5: é«˜é‡‡æ ·ç‡è¾“å‡º + é«˜é¢‘å¢å¼ºæµ‹è¯•")
    print("=" * 50)
    
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
        return
    
    test_v5_high_sample_rate(input_file)

if __name__ == "__main__":
    main()
