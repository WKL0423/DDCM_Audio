#!/usr/bin/env python3
"""
AudioLDM2 HiFiGAN ä¿®å¤ç‰ˆæœ¬ - é’ˆå¯¹å™ªå£°é—®é¢˜ä¼˜åŒ–
==============================================

åŸºäºAudioLDM2å®˜æ–¹é¢„å¤„ç†å‚æ•°å’Œæ­£ç¡®çš„melé¢‘è°±å½’ä¸€åŒ–
è§£å†³å™ªå£°é—®é¢˜çš„å…³é”®ä¿®å¤
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F
from diffusers import AudioLDM2Pipeline

# å°è¯•å¯¼å…¥ soundfile ä»¥è·å¾—æ›´å¥½çš„å…¼å®¹æ€§
try:
    import soundfile as sf
    SOUNDFILE_AVAILABLE = True
except ImportError:
    SOUNDFILE_AVAILABLE = False
    print("âš ï¸ soundfile ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ torchaudio ä¿å­˜ï¼ˆå¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼‰")


def save_audio_compatible(audio_data, filepath, sample_rate=16000):
    """ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ soundfile ä»¥è·å¾—æœ€å¤§å…¼å®¹æ€§"""
    if isinstance(audio_data, torch.Tensor):
        audio_data = audio_data.detach().cpu().numpy()
    
    if len(audio_data.shape) > 1:
        audio_data = audio_data.flatten()
    
    # æ¸…ç†æ•°æ®
    audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=0.0, neginf=0.0)
    
    # å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
    if np.max(np.abs(audio_data)) > 0:
        audio_data = audio_data / np.max(np.abs(audio_data))
    
    success = False
    
    if SOUNDFILE_AVAILABLE:
        try:
            sf.write(filepath, audio_data, sample_rate, subtype='PCM_16')
            print(f"   âœ… ä½¿ç”¨ soundfile (PCM_16) ä¿å­˜: {Path(filepath).name}")
            success = True
        except Exception as e:
            print(f"   âš ï¸ soundfile ä¿å­˜å¤±è´¥: {e}")
    
    if not success:
        try:
            audio_tensor = torch.from_numpy(audio_data).unsqueeze(0).float()
            torchaudio.save(filepath, audio_tensor, sample_rate)
            print(f"   âœ… ä½¿ç”¨ torchaudio ä¿å­˜: {Path(filepath).name}")
            success = True
        except Exception as e:
            print(f"   âŒ torchaudio ä¿å­˜ä¹Ÿå¤±è´¥: {e}")
    
    return success


def create_mel_spectrogram_audioldm2(audio, sr=16000):
    """
    ä½¿ç”¨AudioLDM2å®˜æ–¹å‚æ•°åˆ›å»ºmelé¢‘è°±å›¾
    
    åŸºäºAudioLDM2è®ºæ–‡å’Œå®˜æ–¹å®ç°çš„æ ‡å‡†å‚æ•°ï¼š
    - n_mels: 64 (AudioLDM2æ ‡å‡†)
    - n_fft: 1024 
    - hop_length: 160
    - win_length: 1024
    - window: hann
    - å½’ä¸€åŒ–: ä½¿ç”¨AudioLDM2æ ‡å‡†å½’ä¸€åŒ–
    """
    print("ğŸµ ä½¿ç”¨AudioLDM2å®˜æ–¹å‚æ•°åˆ›å»ºmelé¢‘è°±...")
    
    # AudioLDM2å®˜æ–¹å‚æ•°
    n_mels = 64
    n_fft = 1024
    hop_length = 160
    win_length = 1024
    fmin = 0
    fmax = sr // 2
    
    # åˆ›å»ºmelé¢‘è°±å›¾
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sr,
        n_mels=n_mels,
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=win_length,
        window='hann',
        fmin=fmin,
        fmax=fmax,
        power=2.0
    )
    
    print(f"   Melé¢‘è°±åŸå§‹å½¢çŠ¶: {mel_spec.shape}")
    print(f"   Melé¢‘è°±åŸå§‹èŒƒå›´: [{mel_spec.min():.6f}, {mel_spec.max():.6f}]")
    
    # è½¬æ¢ä¸ºå¯¹æ•°å°ºåº¦ (AudioLDM2ä½¿ç”¨çš„æ ‡å‡†æ–¹æ³•)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max, top_db=None)
    
    print(f"   Melé¢‘è°±dBå½¢çŠ¶: {mel_spec_db.shape}")
    print(f"   Melé¢‘è°±dBèŒƒå›´: [{mel_spec_db.min():.6f}, {mel_spec_db.max():.6f}]")
    
    # AudioLDM2æ ‡å‡†å½’ä¸€åŒ–: ç¼©æ”¾åˆ°[-1, 1]
    # ä½¿ç”¨å›ºå®šçš„åŠ¨æ€èŒƒå›´ï¼Œè€Œä¸æ˜¯åŸºäºå½“å‰æ ·æœ¬çš„min/max
    # è¿™æ ·å¯ä»¥ä¿è¯è®­ç»ƒå’Œæ¨ç†æ—¶çš„ä¸€è‡´æ€§
    
    # æ–¹æ³•1: ä½¿ç”¨å›ºå®šçš„dBèŒƒå›´ (æ›´ç¬¦åˆAudioLDM2è®­ç»ƒ)
    min_db = -80.0  # AudioLDM2é€šå¸¸ä½¿ç”¨çš„æœ€å°dBå€¼
    max_db = mel_spec_db.max()
    
    # è£å‰ªåˆ°åˆç†èŒƒå›´
    mel_spec_db = np.clip(mel_spec_db, min_db, max_db)
    
    # å½’ä¸€åŒ–åˆ°[-1, 1]
    mel_spec_normalized = 2.0 * (mel_spec_db - min_db) / (max_db - min_db) - 1.0
    
    print(f"   å½’ä¸€åŒ–å‚æ•°: min_db={min_db:.2f}, max_db={max_db:.2f}")
    print(f"   å½’ä¸€åŒ–åèŒƒå›´: [{mel_spec_normalized.min():.6f}, {mel_spec_normalized.max():.6f}]")
    
    return mel_spec_normalized, min_db, max_db


def test_audioldm2_hifigan_fixed(audio_path, max_length=5):
    """
    ä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨æ­£ç¡®çš„AudioLDM2é¢„å¤„ç†å‚æ•°
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ¯ AudioLDM2 HiFiGAN å™ªå£°ä¿®å¤æµ‹è¯•")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # åŠ è½½AudioLDM2
    print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2",  # ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼Œä¸æ˜¯éŸ³ä¹ä¸“ç”¨ç‰ˆæœ¬
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   VAEç±»å‹: {type(vae).__name__}")
    print(f"   Vocoderç±»å‹: {type(vocoder).__name__}")
    
    # åŠ è½½éŸ³é¢‘
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘: {Path(audio_path).name}")
    audio, sr = librosa.load(audio_path, sr=sample_rate, duration=max_length)
    print(f"   éŸ³é¢‘æ—¶é•¿: {len(audio)/sr:.2f}ç§’")
    print(f"   éŸ³é¢‘èŒƒå›´: [{audio.min():.6f}, {audio.max():.6f}]")
    
    # ä½¿ç”¨æ”¹è¿›çš„melé¢‘è°±åˆ›å»º
    mel_spec_normalized, min_db, max_db = create_mel_spectrogram_audioldm2(audio, sr)
    
    # è½¬æ¢ä¸ºtensor
    mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.half()
    
    # è°ƒæ•´ä¸ºVAEæœŸæœ›çš„å½¢çŠ¶: [batch, channels, height, width]
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, time]
    print(f"ğŸµ VAEè¾“å…¥: {mel_input.shape}, æ•°æ®ç±»å‹: {mel_input.dtype}")
    print(f"   è¾“å…¥èŒƒå›´: [{mel_input.min():.6f}, {mel_input.max():.6f}]")
    
    # VAEå¤„ç†
    print("ğŸ§  å¼€å§‹VAEç¼–ç è§£ç ...")
    with torch.no_grad():
        # ç¡®ä¿å°ºå¯¸åŒ¹é…VAEè¦æ±‚ (AudioLDM2 VAEé€šå¸¸è¦æ±‚èƒ½è¢«æŸä¸ªæ•°æ•´é™¤)
        orig_width = mel_input.shape[-1]
        
        # VAEç¼–ç 
        latent_dist = vae.encode(mel_input)
        if hasattr(latent_dist, 'latent_dist'):
            latent = latent_dist.latent_dist.sample()
        else:
            latent = latent_dist.sample()
        
        print(f"   ç¼–ç latent: {latent.shape}")
        print(f"   LatentèŒƒå›´: [{latent.min():.6f}, {latent.max():.6f}]")
        
        # VAEè§£ç 
        reconstructed_mel = vae.decode(latent).sample
        
        print(f"   è§£ç è¾“å‡º: {reconstructed_mel.shape}")
        print(f"   è§£ç èŒƒå›´: [{reconstructed_mel.min():.6f}, {reconstructed_mel.max():.6f}]")
        
        # è£å‰ªåˆ°åŸå§‹å®½åº¦
        if reconstructed_mel.shape[-1] > orig_width:
            reconstructed_mel = reconstructed_mel[:, :, :, :orig_width]
            print(f"   è£å‰ªå: {reconstructed_mel.shape}")
        
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨AudioLDM2æ ‡å‡†çš„HiFiGANè¾“å…¥æ ¼å¼
        print("ğŸ¤ å‡†å¤‡HiFiGANè¾“å…¥...")
        
        # æ£€æŸ¥vocoderæœŸæœ›çš„è¾“å…¥æ ¼å¼
        print(f"   åŸå§‹melå½¢çŠ¶: {reconstructed_mel.shape}")
        
        # AudioLDM2çš„mel_spectrogram_to_waveformæ–¹æ³•
        if reconstructed_mel.dim() == 4:
            vocoder_input = reconstructed_mel.squeeze(1)  # [batch, 1, height, width] -> [batch, height, width]
            print(f"   squeeze(1): {vocoder_input.shape}")
        else:
            vocoder_input = reconstructed_mel
        
        # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
        vocoder_dtype = next(vocoder.parameters()).dtype
        if vocoder_input.dtype != vocoder_dtype:
            vocoder_input = vocoder_input.to(vocoder_dtype)
            print(f"   è½¬æ¢æ•°æ®ç±»å‹åˆ°: {vocoder_dtype}")
        
        print(f"   æœ€ç»ˆvocoderè¾“å…¥: {vocoder_input.shape}, {vocoder_input.dtype}")
        print(f"   Vocoderè¾“å…¥èŒƒå›´: [{vocoder_input.min():.6f}, {vocoder_input.max():.6f}]")
        
        # ä½¿ç”¨AudioLDM2å†…ç½®HiFiGAN
        try:
            print("ğŸš€ è°ƒç”¨AudioLDM2 HiFiGAN...")
            
            # ç›´æ¥ä½¿ç”¨pipelineçš„æ ‡å‡†æ–¹æ³•
            waveform = pipeline.mel_spectrogram_to_waveform(reconstructed_mel)
            reconstructed_audio = waveform.squeeze().cpu().numpy()
            vocoder_method = "AudioLDM2_HiFiGAN_FIXED"
            
            print(f"âœ… HiFiGANæˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
            print(f"   é‡å»ºéŸ³é¢‘èŒƒå›´: [{reconstructed_audio.min():.6f}, {reconstructed_audio.max():.6f}]")
            
        except Exception as e:
            print(f"âŒ HiFiGANä»ç„¶å¤±è´¥: {e}")
            print("ğŸ”„ ä½¿ç”¨Griffin-Limå¤‡é€‰...")
            
            # Griffin-Limé™çº§ (ä½¿ç”¨æ­£ç¡®çš„åå½’ä¸€åŒ–)
            mel_np = reconstructed_mel.squeeze().cpu().float().numpy()
            
            # åå½’ä¸€åŒ–ï¼šä»[-1,1]æ¢å¤åˆ°dBå°ºåº¦
            mel_denorm_db = (mel_np + 1.0) / 2.0 * (max_db - min_db) + min_db
            
            # ä»dBè½¬æ¢åˆ°åŠŸç‡è°±
            mel_power = librosa.db_to_power(mel_denorm_db)
            
            # Griffin-Limé‡å»º
            reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                mel_power, 
                sr=sample_rate, 
                hop_length=160, 
                n_fft=1024,
                win_length=1024,
                window='hann',
                n_iter=32  # å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥æé«˜è´¨é‡
            )
            vocoder_method = "Griffin_Lim_Fixed"
            print(f"âœ… Griffin-LimæˆåŠŸ: {len(reconstructed_audio)}æ ·æœ¬")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("vae_hifigan_noise_fixed")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜è·¯å¾„
    original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
    reconstructed_path = output_dir / f"{input_name}_{vocoder_method}_{timestamp}.wav"
    
    # é•¿åº¦åŒ¹é…
    if len(reconstructed_audio) > len(audio):
        reconstructed_audio = reconstructed_audio[:len(audio)]
    elif len(reconstructed_audio) < len(audio):
        reconstructed_audio = np.pad(reconstructed_audio, (0, len(audio) - len(reconstructed_audio)))
    
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    save_audio_compatible(audio, original_path, sample_rate)
    save_audio_compatible(reconstructed_audio, reconstructed_path, sample_rate)
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    mse = np.mean((audio - reconstructed_audio) ** 2)
    snr = 10 * np.log10(np.mean(audio ** 2) / (mse + 1e-10))
    correlation = np.corrcoef(audio, reconstructed_audio)[0, 1] if len(audio) > 1 else 0
    
    # è¾“å‡ºç»“æœ
    print(f"\n{'='*60}")
    print(f"ğŸ¯ AudioLDM2 å™ªå£°ä¿®å¤æµ‹è¯•ç»“æœ")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ğŸ“Š SNR: {snr:.2f} dB")
    print(f"ğŸ“Š MSE: {mse:.6f}")
    print(f"ğŸ“Š ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"ğŸ¤ é‡å»ºæ–¹æ³•: {vocoder_method}")
    
    # åˆ†æç»“æœ
    if vocoder_method == "AudioLDM2_HiFiGAN_FIXED":
        print(f"\nğŸ‰ HiFiGANä¿®å¤æˆåŠŸï¼")
        print(f"âœ… ä½¿ç”¨äº†AudioLDM2å®˜æ–¹é¢„å¤„ç†å‚æ•°")
        print(f"âœ… ä½¿ç”¨äº†æ­£ç¡®çš„melé¢‘è°±å½’ä¸€åŒ–")
        print(f"âœ… ä½¿ç”¨äº†æ ‡å‡†çš„vocoderè°ƒç”¨æ–¹æ³•")
        
        if snr > 0:
            print(f"ğŸ† é‡å»ºè´¨é‡è‰¯å¥½ï¼å™ªå£°é—®é¢˜å·²æ˜¾è‘—æ”¹å–„")
        elif snr > -5:
            print(f"âœ… é‡å»ºè´¨é‡å¯æ¥å—ï¼Œç»§ç»­ä¼˜åŒ–ä¸­")
        else:
            print(f"âš ï¸ ä»éœ€è¿›ä¸€æ­¥è°ƒä¼˜ï¼Œä½†æ–¹å‘æ­£ç¡®")
    else:
        print(f"\nğŸ”§ ä½¿ç”¨ä¿®å¤ç‰ˆGriffin-Lim")
        print(f"âœ… ä½¿ç”¨äº†æ­£ç¡®çš„dBèŒƒå›´åå½’ä¸€åŒ–")
        print(f"âœ… å¢åŠ äº†Griffin-Limè¿­ä»£æ¬¡æ•°")
    
    print(f"\nğŸ”¬ æŠ€æœ¯æ”¹è¿›:")
    print(f"   âœ… ä½¿ç”¨AudioLDM2å®˜æ–¹melå‚æ•°")
    print(f"   âœ… ä½¿ç”¨å›ºå®šdBèŒƒå›´å½’ä¸€åŒ–")
    print(f"   âœ… ä½¿ç”¨æ ‡å‡†vocoderè°ƒç”¨")
    print(f"   âœ… æ”¹è¿›åå½’ä¸€åŒ–è¿‡ç¨‹")
    
    return {
        'snr': snr,
        'mse': mse,
        'correlation': correlation,
        'vocoder_method': vocoder_method,
        'original_path': str(original_path),
        'reconstructed_path': str(reconstructed_path)
    }


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        audio_files = list(Path('.').glob('*.wav'))
        if audio_files:
            print("æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files[:3], 1):
                print(f"{i}. {file.name}")
            
            try:
                choice = int(input("é€‰æ‹©æ–‡ä»¶: "))
                audio_path = str(audio_files[choice-1])
            except:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    print(f"ğŸš€ å¼€å§‹AudioLDM2å™ªå£°ä¿®å¤æµ‹è¯•")
    
    try:
        result = test_audioldm2_hifigan_fixed(audio_path)
        
        print(f"\nğŸ“‹ ä¿®å¤æµ‹è¯•æ€»ç»“:")
        print(f"   æ–¹æ³•: {result['vocoder_method']}")
        print(f"   SNR: {result['snr']:.2f}dB")
        print(f"   MSE: {result['mse']:.6f}")
        print(f"   ç›¸å…³æ€§: {result['correlation']:.4f}")
        
        if "FIXED" in result['vocoder_method']:
            print(f"\nğŸŠ å™ªå£°ä¿®å¤æµ‹è¯•å®Œæˆï¼")
            print(f"ğŸ”¬ ä½¿ç”¨äº†AudioLDM2å®˜æ–¹å‚æ•°å’Œæ ‡å‡†åŒ–æ–¹æ³•")
        else:
            print(f"\nğŸ” ç»§ç»­ä¼˜åŒ–å™ªå£°é—®é¢˜...")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
