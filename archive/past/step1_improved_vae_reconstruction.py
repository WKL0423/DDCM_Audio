#!/usr/bin/env python3
"""
Step 1: æ”¹è¿›çš„VAEé‡å»ºæµ‹è¯•
åŸºäºAudioLDMè®­ç»ƒä»£ç çš„æ­£ç¡®æ•°æ®å¤„ç†æµç¨‹
ä¸“æ³¨äºï¼š
1. æ­£ç¡®çš„Melé¢‘è°±æå–ï¼ˆç¬¦åˆè®­ç»ƒæ—¶çš„æ ¼å¼ï¼‰
2. å­é¢‘å¸¦å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
3. å‡†ç¡®çš„VAEç¼–ç è§£ç 
4. è¯¦ç»†çš„è´¨é‡åˆ†æ
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Tuple
# ä½¿ç”¨è‡ªå®šä¹‰çš„AudioLDM2Pipeline
from New_pipeline_audioldm2 import AudioLDM2Pipeline
import torchaudio
from pathlib import Path
import time
import soundfile as sf
import librosa
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

class ImprovedVAEReconstructor:
    """
    æ”¹è¿›çš„VAEé‡å»ºå™¨
    åŸºäºAudioLDMè®­ç»ƒä»£ç çš„æ­£ç¡®å¤„ç†æµç¨‹
    """
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """åˆå§‹åŒ–VAEé‡å»ºå™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸµ åˆå§‹åŒ–æ”¹è¿›VAEé‡å»ºå™¨")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        
        # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float32,  # ä½¿ç”¨float32é¿å…ç±»å‹é—®é¢˜
        ).to(self.device)
        
        print(f"âœ… VAEé‡å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š VAE scaling_factor: {self.pipeline.vae.config.scaling_factor}")
        print(f"   ğŸ¤ ç‰¹å¾æå–å™¨é‡‡æ ·ç‡: {self.pipeline.feature_extractor.sampling_rate}Hz")
        
        # AudioLDMè®­ç»ƒæ—¶çš„æ ‡å‡†å‚æ•°
        self.target_sr = 16000  # è®­ç»ƒæ—¶ä½¿ç”¨16kHz
        self.mel_bins = 64      # è®­ç»ƒæ—¶ä½¿ç”¨64ä¸ªMel bins
        self.mel_fmax = 8000    # æœ€å¤§é¢‘ç‡8kHz
        self.hop_length = 160   # è®­ç»ƒé…ç½®ä¸­çš„hop_length
        self.win_length = 1024  # è®­ç»ƒé…ç½®ä¸­çš„win_length
        self.n_fft = 1024       # è®­ç»ƒé…ç½®ä¸­çš„filter_length
        
        print(f"   ğŸ”§ ä½¿ç”¨è®­ç»ƒæ—¶æ ‡å‡†å‚æ•°:")
        print(f"      é‡‡æ ·ç‡: {self.target_sr}Hz")
        print(f"      Melé¢‘é“: {self.mel_bins}")
        print(f"      æœ€å¤§é¢‘ç‡: {self.mel_fmax}Hz")
    
    def reconstruct_audio(self, audio_path: str) -> Dict:
        """
        å®Œæ•´çš„VAEé‡å»ºæµç¨‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        """
        print(f"\nğŸµ æ”¹è¿›VAEé‡å»ºå¤„ç†: {Path(audio_path).name}")
        
        # 1. åŠ è½½å’Œé¢„å¤„ç†è¾“å…¥éŸ³é¢‘ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„æ ‡å‡†æµç¨‹ï¼‰
        original_audio, mel_spectrogram = self._load_and_extract_mel(audio_path)
        
        # 2. VAEç¼–ç ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„å¤„ç†ï¼‰
        latent = self._encode_to_latent(mel_spectrogram)
        
        # 3. VAEè§£ç é‡å»º
        reconstructed_mel = self._decode_latent_to_mel(latent)
        
        # 4. Melè½¬éŸ³é¢‘
        reconstructed_audio = self._mel_to_audio(reconstructed_mel)
        
        # 5. ä¿å­˜ç»“æœå’Œåˆ†æ
        result = self._save_and_analyze(original_audio, reconstructed_audio, audio_path, 
                                      mel_spectrogram, reconstructed_mel)
        
        # 6. æ˜¾ç¤ºç»“æœ
        self._display_results(result)
        
        return result
    
    def _load_and_extract_mel(self, audio_path: str) -> Tuple[np.ndarray, torch.Tensor]:
        """åŠ è½½éŸ³é¢‘å¹¶æå–Melé¢‘è°±ï¼ˆæŒ‰è®­ç»ƒæ—¶çš„æ ‡å‡†æµç¨‹ï¼‰"""
        print(f"   ğŸ“ åŠ è½½éŸ³é¢‘...")
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = torchaudio.load(audio_path)
        
        # é‡é‡‡æ ·åˆ°è®­ç»ƒæ—¶çš„é‡‡æ ·ç‡ (16kHz)
        if sr != self.target_sr:
            print(f"   ğŸ”„ é‡é‡‡æ ·: {sr}Hz -> {self.target_sr}Hz")
            resampler = torchaudio.transforms.Resample(sr, self.target_sr)
            audio = resampler(audio)
        
        # è½¬ä¸ºå•å£°é“
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # é™åˆ¶é•¿åº¦åˆ°10.24ç§’ï¼ˆè®­ç»ƒæ—¶çš„æ ‡å‡†é•¿åº¦ï¼‰
        max_length = int(self.target_sr * 10.24)
        if audio.shape[-1] > max_length:
            audio = audio[..., :max_length]
            print(f"   âœ‚ï¸ æˆªå–åˆ°10.24ç§’")
        
        print(f"   âœ… éŸ³é¢‘é¢„å¤„ç†å®Œæˆ: {audio.shape}, {audio.shape[-1]/self.target_sr:.2f}ç§’")
        
        # æå–Melé¢‘è°±ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„å‚æ•°ï¼‰
        print(f"   ğŸ”„ æå–Melé¢‘è°±...")
        with torch.no_grad():
            audio_np = audio.squeeze(0).numpy()
            
            # ä½¿ç”¨librosaæå–Melé¢‘è°±ï¼ˆæ›´æ¥è¿‘è®­ç»ƒæ—¶çš„å¤„ç†ï¼‰
            mel_spec = librosa.feature.melspectrogram(
                y=audio_np,
                sr=self.target_sr,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                win_length=self.win_length,
                n_mels=self.mel_bins,
                fmax=self.mel_fmax
            )
            
            # è½¬æ¢ä¸ºå¯¹æ•°åˆ»åº¦
            log_mel = librosa.power_to_db(mel_spec, ref=np.max)
            
            # è½¬æ¢ä¸ºtensorå¹¶è°ƒæ•´ç»´åº¦
            log_mel_tensor = torch.from_numpy(log_mel).float()
            
            # è°ƒæ•´ç»´åº¦ä¸º [batch, channels, time, frequency]
            mel_spectrogram = log_mel_tensor.unsqueeze(0).unsqueeze(0).to(self.device)
            
            print(f"   ğŸ“Š Melé¢‘è°±: {mel_spectrogram.shape}")
            print(f"   ğŸ“Š MelèŒƒå›´: [{mel_spectrogram.min():.3f}, {mel_spectrogram.max():.3f}] dB")
            
        return audio_np, mel_spectrogram
    
    def _encode_to_latent(self, mel_spectrogram: torch.Tensor) -> torch.Tensor:
        """ç¼–ç Melé¢‘è°±ä¸ºlatentï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„å¤„ç†ï¼‰"""
        print(f"   ğŸ”„ VAEç¼–ç ...")
        
        with torch.no_grad():
            # æ£€æŸ¥VAEæ˜¯å¦æœ‰å­é¢‘å¸¦å¤„ç†
            vae = self.pipeline.vae
            
            # æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„encodeè¿‡ç¨‹
            # 1. å­é¢‘å¸¦åˆ†è§£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(vae, 'freq_split_subband'):
                print(f"   ğŸ”§ åº”ç”¨å­é¢‘å¸¦åˆ†è§£...")
                mel_input = vae.freq_split_subband(mel_spectrogram)
            else:
                mel_input = mel_spectrogram
            
            print(f"   ğŸ“Š è¾“å…¥åˆ°encoder: {mel_input.shape}")
            
            # 2. Encoder
            if hasattr(vae, 'encoder'):
                h = vae.encoder(mel_input)
            else:
                # å¦‚æœç›´æ¥ä½¿ç”¨pipelineçš„VAE
                latent_dist = vae.encode(mel_input)
                latent = latent_dist.latent_dist.mode()
                latent = latent * vae.config.scaling_factor
                print(f"   âœ… VAEç¼–ç å®Œæˆ: {latent.shape}")
                print(f"   ğŸ“Š LatentèŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
                print(f"   ğŸ“Š Latent std: {latent.std():.3f}")
                return latent
            
            # 3. é‡åŒ–å·ç§¯
            if hasattr(vae, 'quant_conv'):
                moments = vae.quant_conv(h)
                # ä½¿ç”¨DiagonalGaussianDistribution
                from audioldm_train.modules.diffusionmodules.distributions import DiagonalGaussianDistribution
                posterior = DiagonalGaussianDistribution(moments)
                latent = posterior.mode()  # ä½¿ç”¨mode()æ›´ç¨³å®š
            else:
                # å›é€€åˆ°æ ‡å‡†å¤„ç†
                latent_dist = vae.encode(mel_input)
                latent = latent_dist.latent_dist.mode()
            
            # 4. åº”ç”¨scaling factor
            if hasattr(vae.config, 'scaling_factor'):
                latent = latent * vae.config.scaling_factor
            
            print(f"   âœ… VAEç¼–ç å®Œæˆ: {latent.shape}")
            print(f"   ğŸ“Š LatentèŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
            print(f"   ğŸ“Š Latent std: {latent.std():.3f}")
            
        return latent
    
    def _decode_latent_to_mel(self, latent: torch.Tensor) -> torch.Tensor:
        """è§£ç latentä¸ºMelé¢‘è°±ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„å¤„ç†ï¼‰"""
        print(f"   ğŸ”„ VAEè§£ç ...")
        
        with torch.no_grad():
            vae = self.pipeline.vae
            
            # 1. åå‘scaling
            if hasattr(vae.config, 'scaling_factor'):
                latent_for_decode = latent / vae.config.scaling_factor
            else:
                latent_for_decode = latent
            
            print(f"   ğŸ“Š è§£ç latent: {latent_for_decode.shape}")
            print(f"   ğŸ“Š è§£ç latentèŒƒå›´: [{latent_for_decode.min():.3f}, {latent_for_decode.max():.3f}]")
            
            # 2. è§£ç è¿‡ç¨‹
            if hasattr(vae, 'post_quant_conv') and hasattr(vae, 'decoder'):
                # è®­ç»ƒä»£ç é£æ ¼çš„è§£ç 
                z = vae.post_quant_conv(latent_for_decode)
                dec = vae.decoder(z)
                
                # 3. å­é¢‘å¸¦åˆå¹¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if hasattr(vae, 'freq_merge_subband'):
                    print(f"   ğŸ”§ åº”ç”¨å­é¢‘å¸¦åˆå¹¶...")
                    reconstructed_mel = vae.freq_merge_subband(dec)
                else:
                    reconstructed_mel = dec
            else:
                # æ ‡å‡†pipelineè§£ç 
                reconstructed_mel = vae.decode(latent_for_decode).sample
            
            print(f"   âœ… VAEè§£ç å®Œæˆ: {reconstructed_mel.shape}")
            print(f"   ğŸ“Š é‡å»ºMelèŒƒå›´: [{reconstructed_mel.min():.3f}, {reconstructed_mel.max():.3f}] dB")
            
        return reconstructed_mel
    
    def _mel_to_audio(self, mel_spectrogram: torch.Tensor) -> np.ndarray:
        """å°†Melé¢‘è°±è½¬æ¢ä¸ºéŸ³é¢‘"""
        print(f"   ğŸ”„ Melè½¬éŸ³é¢‘...")
        
        with torch.no_grad():
            # ä½¿ç”¨AudioLDM2çš„vocoder
            audio_tensor = self.pipeline.mel_spectrogram_to_waveform(mel_spectrogram)
            audio = audio_tensor.squeeze().detach().cpu().numpy()
            
            print(f"   âœ… Melè½¬éŸ³é¢‘å®Œæˆ: {len(audio)}æ ·æœ¬ ({len(audio)/self.target_sr:.2f}ç§’)")
            
        return audio
    
    def _save_and_analyze(self, original_audio: np.ndarray, reconstructed_audio: np.ndarray, 
                         audio_path: str, original_mel: torch.Tensor, 
                         reconstructed_mel: torch.Tensor) -> Dict:
        """ä¿å­˜ç»“æœå¹¶è¿›è¡Œè´¨é‡åˆ†æ"""
        print(f"   ğŸ’¾ ä¿å­˜ç»“æœå’Œè´¨é‡åˆ†æ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("step_1_improved_vae_reconstruction")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = int(time.time())
        input_name = Path(audio_path).stem
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(original_audio), len(reconstructed_audio))
        original_audio = original_audio[:min_len]
        reconstructed_audio = reconstructed_audio[:min_len]
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
        reconstructed_path = output_dir / f"{input_name}_improved_vae_reconstructed_{timestamp}.wav"
        
        sf.write(str(original_path), original_audio, self.target_sr)
        sf.write(str(reconstructed_path), reconstructed_audio, self.target_sr)
        
        # éŸ³é¢‘è´¨é‡åˆ†æ
        audio_analysis = self._analyze_audio_quality(original_audio, reconstructed_audio)
        
        # é¢‘è°±åˆ†æ
        spectral_analysis = self._analyze_frequency_content(original_audio, reconstructed_audio)
        
        # Melé¢‘è°±åˆ†æ
        mel_analysis = self._analyze_mel_spectrogram(original_mel, reconstructed_mel)
        
        # ç”Ÿæˆå¯¹æ¯”å›¾
        self._plot_comprehensive_analysis(original_audio, reconstructed_audio, 
                                        original_mel, reconstructed_mel, 
                                        output_dir, timestamp)
        
        result = {
            "input_file": audio_path,
            "original_path": str(original_path),
            "reconstructed_path": str(reconstructed_path),
            "audio_length": min_len / self.target_sr,
            "audio_quality_metrics": audio_analysis,
            "frequency_analysis": spectral_analysis,
            "mel_analysis": mel_analysis,
            "timestamp": timestamp
        }
        
        return result
    
    def _analyze_audio_quality(self, original: np.ndarray, reconstructed: np.ndarray) -> Dict:
        """åˆ†æéŸ³é¢‘è´¨é‡æŒ‡æ ‡"""
        # åŸºç¡€è´¨é‡æŒ‡æ ‡
        mse = np.mean((original - reconstructed) ** 2)
        snr = 10 * np.log10(np.mean(original ** 2) / (mse + 1e-10))
        correlation = np.corrcoef(original, reconstructed)[0, 1] if len(original) > 1 else 0
        mae = np.mean(np.abs(original - reconstructed))
        
        # RMSæ¯”è¾ƒ
        rms_original = np.sqrt(np.mean(original ** 2))
        rms_reconstructed = np.sqrt(np.mean(reconstructed ** 2))
        rms_ratio = rms_reconstructed / (rms_original + 1e-10)
        
        return {
            "snr_db": snr,
            "correlation": correlation,
            "mse": mse,
            "mae": mae,
            "rms_original": rms_original,
            "rms_reconstructed": rms_reconstructed,
            "rms_ratio": rms_ratio
        }
    
    def _analyze_frequency_content(self, original: np.ndarray, reconstructed: np.ndarray) -> Dict:
        """åˆ†æé¢‘è°±å†…å®¹"""
        if len(original) < 8192:
            return {"error": "éŸ³é¢‘å¤ªçŸ­ï¼Œæ— æ³•è¿›è¡Œé¢‘è°±åˆ†æ"}
        
        # FFTåˆ†æ
        fft_len = 8192
        orig_fft = np.abs(np.fft.fft(original[:fft_len]))[:fft_len//2]
        recon_fft = np.abs(np.fft.fft(reconstructed[:fft_len]))[:fft_len//2]
        
        # é¢‘æ®µåˆ†æ
        freqs = np.fft.fftfreq(fft_len, 1/self.target_sr)[:fft_len//2]
        
        # å®šä¹‰é¢‘æ®µï¼ˆæ›´ç»†è‡´çš„åˆ’åˆ†ï¼‰
        low_freq_mask = freqs < 500     # æä½é¢‘ 0-500Hz
        mid_low_mask = (freqs >= 500) & (freqs < 1000)   # ä½é¢‘ 500Hz-1kHz
        mid_freq_mask = (freqs >= 1000) & (freqs < 2000)  # ä¸­ä½é¢‘ 1-2kHz
        mid_high_mask = (freqs >= 2000) & (freqs < 4000)  # ä¸­é«˜é¢‘ 2-4kHz
        high_freq_mask = (freqs >= 4000) & (freqs < 6000) # é«˜é¢‘ 4-6kHz
        ultra_high_mask = freqs >= 6000  # è¶…é«˜é¢‘ 6-8kHz
        
        # è®¡ç®—å„é¢‘æ®µèƒ½é‡
        def calc_retention(orig_mask, recon_mask):
            orig_energy = np.sum(orig_fft[orig_mask])
            recon_energy = np.sum(recon_fft[recon_mask])
            return recon_energy / (orig_energy + 1e-10)
        
        return {
            "ultra_low_retention": calc_retention(low_freq_mask, low_freq_mask),
            "low_freq_retention": calc_retention(mid_low_mask, mid_low_mask),
            "mid_low_retention": calc_retention(mid_freq_mask, mid_freq_mask),
            "mid_high_retention": calc_retention(mid_high_mask, mid_high_mask),
            "high_freq_retention": calc_retention(high_freq_mask, high_freq_mask),
            "ultra_high_retention": calc_retention(ultra_high_mask, ultra_high_mask),
            "total_energy_ratio": np.sum(recon_fft) / (np.sum(orig_fft) + 1e-10)
        }
    
    def _analyze_mel_spectrogram(self, original_mel: torch.Tensor, 
                               reconstructed_mel: torch.Tensor) -> Dict:
        """åˆ†æMelé¢‘è°±çš„é‡å»ºè´¨é‡"""
        with torch.no_grad():
            orig_mel_np = original_mel.squeeze().detach().cpu().numpy()
            recon_mel_np = reconstructed_mel.squeeze().detach().cpu().numpy()
            
            # Melé¢‘è°±MSE
            mel_mse = np.mean((orig_mel_np - recon_mel_np) ** 2)
            
            # Melé¢‘è°±ç›¸å…³æ€§
            flat_orig = orig_mel_np.flatten()
            flat_recon = recon_mel_np.flatten()
            mel_correlation = np.corrcoef(flat_orig, flat_recon)[0, 1]
            
            # ä¸åŒé¢‘ç‡binçš„ä¿æŒæƒ…å†µ
            freq_bin_retention = []
            for i in range(min(orig_mel_np.shape[-1], recon_mel_np.shape[-1])):
                orig_bin = orig_mel_np[..., i]
                recon_bin = recon_mel_np[..., i]
                bin_corr = np.corrcoef(orig_bin.flatten(), recon_bin.flatten())[0, 1]
                freq_bin_retention.append(bin_corr)
            
            return {
                "mel_mse": mel_mse,
                "mel_correlation": mel_correlation,
                "avg_freq_bin_retention": np.mean(freq_bin_retention),
                "low_freq_bins_retention": np.mean(freq_bin_retention[:16]),
                "mid_freq_bins_retention": np.mean(freq_bin_retention[16:48]),
                "high_freq_bins_retention": np.mean(freq_bin_retention[48:])
            }
    
    def _plot_comprehensive_analysis(self, original_audio: np.ndarray, reconstructed_audio: np.ndarray,
                                   original_mel: torch.Tensor, reconstructed_mel: torch.Tensor,
                                   output_dir: Path, timestamp: int):
        """ç”Ÿæˆç»¼åˆåˆ†æå›¾è¡¨"""
        try:
            fig, axes = plt.subplots(3, 2, figsize=(16, 12))
            
            # åŸå§‹å’Œé‡å»ºMelé¢‘è°±å¯¹æ¯”
            orig_mel_np = original_mel.squeeze().detach().cpu().numpy()
            recon_mel_np = reconstructed_mel.squeeze().detach().cpu().numpy()
            
            im1 = axes[0,0].imshow(orig_mel_np, aspect='auto', origin='lower', cmap='viridis')
            axes[0,0].set_title('åŸå§‹Melé¢‘è°±')
            axes[0,0].set_ylabel('Melé¢‘ç‡bin')
            plt.colorbar(im1, ax=axes[0,0])
            
            im2 = axes[0,1].imshow(recon_mel_np, aspect='auto', origin='lower', cmap='viridis')
            axes[0,1].set_title('é‡å»ºMelé¢‘è°±')
            axes[0,1].set_ylabel('Melé¢‘ç‡bin')
            plt.colorbar(im2, ax=axes[0,1])
            
            # Melé¢‘è°±å·®å¼‚
            mel_diff = orig_mel_np - recon_mel_np
            im3 = axes[1,0].imshow(mel_diff, aspect='auto', origin='lower', cmap='RdBu_r')
            axes[1,0].set_title('Melé¢‘è°±å·®å¼‚ (åŸå§‹-é‡å»º)')
            axes[1,0].set_ylabel('Melé¢‘ç‡bin')
            plt.colorbar(im3, ax=axes[1,0])
            
            # é¢‘ç‡binå¹³å‡ä¿æŒæƒ…å†µ
            freq_bin_means_orig = np.mean(orig_mel_np, axis=0)
            freq_bin_means_recon = np.mean(recon_mel_np, axis=0)
            
            axes[1,1].plot(freq_bin_means_orig, label='åŸå§‹', alpha=0.7)
            axes[1,1].plot(freq_bin_means_recon, label='é‡å»º', alpha=0.7)
            axes[1,1].set_title('å„é¢‘ç‡binå¹³å‡èƒ½é‡')
            axes[1,1].set_xlabel('Melé¢‘ç‡bin')
            axes[1,1].set_ylabel('å¹³å‡èƒ½é‡ (dB)')
            axes[1,1].legend()
            axes[1,1].grid(True)
            
            # æ—¶åŸŸæ³¢å½¢å¯¹æ¯”
            time_axis = np.linspace(0, len(original_audio)/self.target_sr, len(original_audio))
            
            axes[2,0].plot(time_axis, original_audio, label='åŸå§‹', alpha=0.7)
            min_len = min(len(original_audio), len(reconstructed_audio))
            axes[2,0].plot(time_axis[:min_len], reconstructed_audio[:min_len], label='é‡å»º', alpha=0.7)
            axes[2,0].set_title('æ—¶åŸŸæ³¢å½¢å¯¹æ¯”')
            axes[2,0].set_xlabel('æ—¶é—´ (s)')
            axes[2,0].set_ylabel('å¹…åº¦')
            axes[2,0].legend()
            axes[2,0].grid(True)
            
            # é¢‘åŸŸå¯¹æ¯”
            fft_len = min(8192, len(original_audio), len(reconstructed_audio))
            orig_fft = np.abs(np.fft.fft(original_audio[:fft_len]))[:fft_len//2]
            recon_fft = np.abs(np.fft.fft(reconstructed_audio[:fft_len]))[:fft_len//2]
            freqs = np.fft.fftfreq(fft_len, 1/self.target_sr)[:fft_len//2]
            
            axes[2,1].semilogy(freqs, orig_fft, label='åŸå§‹', alpha=0.7)
            axes[2,1].semilogy(freqs, recon_fft, label='é‡å»º', alpha=0.7)
            axes[2,1].set_xlabel('é¢‘ç‡ (Hz)')
            axes[2,1].set_ylabel('å¹…åº¦')
            axes[2,1].set_title('é¢‘åŸŸå“åº”å¯¹æ¯”')
            axes[2,1].legend()
            axes[2,1].grid(True)
            axes[2,1].set_xlim([0, self.mel_fmax])
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            plot_path = output_dir / f"comprehensive_analysis_{timestamp}.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"   ğŸ“Š ç»¼åˆåˆ†æå›¾å·²ä¿å­˜: {plot_path}")
            
        except Exception as e:
            print(f"   âš ï¸ åˆ†æå›¾ç”Ÿæˆå¤±è´¥: {e}")
    
    def _display_results(self, result: Dict):
        """æ˜¾ç¤ºè¯¦ç»†ç»“æœ"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ æ”¹è¿›VAEé‡å»ºç»“æœ")
        print(f"{'='*80}")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {result['input_file']}")
        print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {result['original_path']}")
        print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {result['reconstructed_path']}")
        print(f"â±ï¸ éŸ³é¢‘é•¿åº¦: {result['audio_length']:.2f}ç§’")
        
        # éŸ³é¢‘è´¨é‡æŒ‡æ ‡
        audio_metrics = result['audio_quality_metrics']
        print(f"\nğŸ“Š éŸ³é¢‘è´¨é‡æŒ‡æ ‡:")
        print(f"   SNR: {audio_metrics['snr_db']:.2f} dB")
        print(f"   ç›¸å…³ç³»æ•°: {audio_metrics['correlation']:.4f}")
        print(f"   MSE: {audio_metrics['mse']:.6f}")
        print(f"   MAE: {audio_metrics['mae']:.6f}")
        print(f"   RMSæ¯”ç‡: {audio_metrics['rms_ratio']:.4f}")
        
        # é¢‘ç‡åˆ†æ
        freq_analysis = result['frequency_analysis']
        if 'error' not in freq_analysis:
            print(f"\nğŸµ è¯¦ç»†é¢‘ç‡åˆ†æ:")
            print(f"   æä½é¢‘ä¿æŒç‡ (0-500Hz): {freq_analysis['ultra_low_retention']:.3f}")
            print(f"   ä½é¢‘ä¿æŒç‡ (500Hz-1kHz): {freq_analysis['low_freq_retention']:.3f}")
            print(f"   ä¸­ä½é¢‘ä¿æŒç‡ (1-2kHz): {freq_analysis['mid_low_retention']:.3f}")
            print(f"   ä¸­é«˜é¢‘ä¿æŒç‡ (2-4kHz): {freq_analysis['mid_high_retention']:.3f}")
            print(f"   é«˜é¢‘ä¿æŒç‡ (4-6kHz): {freq_analysis['high_freq_retention']:.3f}")
            print(f"   è¶…é«˜é¢‘ä¿æŒç‡ (6-8kHz): {freq_analysis['ultra_high_retention']:.3f}")
            print(f"   æ€»èƒ½é‡æ¯”ç‡: {freq_analysis['total_energy_ratio']:.3f}")
        
        # Melé¢‘è°±åˆ†æ
        mel_analysis = result['mel_analysis']
        print(f"\nğŸ” Melé¢‘è°±åˆ†æ:")
        print(f"   Mel MSE: {mel_analysis['mel_mse']:.6f}")
        print(f"   Melç›¸å…³æ€§: {mel_analysis['mel_correlation']:.4f}")
        print(f"   å¹³å‡é¢‘ç‡binä¿æŒç‡: {mel_analysis['avg_freq_bin_retention']:.4f}")
        print(f"   ä½é¢‘binä¿æŒç‡: {mel_analysis['low_freq_bins_retention']:.4f}")
        print(f"   ä¸­é¢‘binä¿æŒç‡: {mel_analysis['mid_freq_bins_retention']:.4f}")
        print(f"   é«˜é¢‘binä¿æŒç‡: {mel_analysis['high_freq_bins_retention']:.4f}")
        
        # è¯Šæ–­å’Œå»ºè®®
        self._provide_diagnosis_and_suggestions(result)
    
    def _provide_diagnosis_and_suggestions(self, result: Dict):
        """æä¾›è¯Šæ–­å’Œæ”¹è¿›å»ºè®®"""
        print(f"\nğŸ’¡ è¯Šæ–­å’Œå»ºè®®:")
        
        audio_metrics = result['audio_quality_metrics']
        freq_analysis = result['frequency_analysis']
        mel_analysis = result['mel_analysis']
        
        # SNRè¯Šæ–­
        if audio_metrics['snr_db'] > 10:
            print(f"   âœ… SNRè‰¯å¥½ ({audio_metrics['snr_db']:.1f}dB)ï¼ŒåŸºç¡€é‡å»ºè´¨é‡ä¸é”™")
        elif audio_metrics['snr_db'] > 0:
            print(f"   âš ï¸ SNRä¸€èˆ¬ ({audio_metrics['snr_db']:.1f}dB)ï¼Œæœ‰æ”¹å–„ç©ºé—´")
        else:
            print(f"   âŒ SNRè¾ƒä½ ({audio_metrics['snr_db']:.1f}dB)ï¼Œé‡å»ºè´¨é‡éœ€è¦æ”¹è¿›")
        
        # é«˜é¢‘è¯Šæ–­
        high_freq_retention = freq_analysis.get('high_freq_retention', 0)
        ultra_high_retention = freq_analysis.get('ultra_high_retention', 0)
        
        if high_freq_retention < 0.3:
            print(f"   âŒ ä¸¥é‡é«˜é¢‘æŸå¤±ï¼4-6kHzä¿æŒç‡ä»… {high_freq_retention*100:.1f}%")
        elif high_freq_retention < 0.6:
            print(f"   âš ï¸ æ˜æ˜¾é«˜é¢‘æŸå¤±ï¼Œ4-6kHzä¿æŒç‡ {high_freq_retention*100:.1f}%")
        else:
            print(f"   âœ… é«˜é¢‘ä¿æŒè¾ƒå¥½ï¼Œ4-6kHzä¿æŒç‡ {high_freq_retention*100:.1f}%")
        
        if ultra_high_retention < 0.1:
            print(f"   âŒ è¶…é«˜é¢‘å‡ ä¹å®Œå…¨ä¸¢å¤±ï¼6-8kHzä¿æŒç‡ä»… {ultra_high_retention*100:.1f}%")
        
        # Melé¢‘è°±è¯Šæ–­
        if mel_analysis['mel_correlation'] > 0.8:
            print(f"   âœ… Melé¢‘è°±é‡å»ºè‰¯å¥½ (ç›¸å…³æ€§: {mel_analysis['mel_correlation']:.3f})")
        else:
            print(f"   âš ï¸ Melé¢‘è°±é‡å»ºæœ‰å¾…æ”¹è¿› (ç›¸å…³æ€§: {mel_analysis['mel_correlation']:.3f})")
        
        # æ”¹è¿›å»ºè®®
        print(f"\nğŸ”§ æ”¹è¿›å»ºè®®:")
        if high_freq_retention < 0.5:
            print(f"   ğŸ¯ éœ€è¦åœ¨latentç©ºé—´è¿›è¡Œé«˜é¢‘å¢å¼º")
            print(f"   ğŸ¯ è€ƒè™‘é¢‘ç‡æ„ŸçŸ¥çš„æŸå¤±å‡½æ•°")
        
        if mel_analysis['high_freq_bins_retention'] < 0.7:
            print(f"   ğŸ¯ é«˜é¢‘Mel biné‡å»ºä¸ä½³ï¼Œéœ€è¦é’ˆå¯¹æ€§ä¼˜åŒ–")
        
        print(f"   ğŸ“Š è¯¦ç»†åˆ†æå›¾è¡¨å·²ç”Ÿæˆï¼Œå¯è¿›ä¸€æ­¥åˆ†æ")

def test_improved_vae():
    """æµ‹è¯•æ”¹è¿›çš„VAEé‡å»º"""
    print("ğŸµ æ”¹è¿›VAEé‡å»ºæµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        print("è¯·ç¡®ä¿AudioLDM2_Music_output.wavåœ¨å½“å‰ç›®å½•ä¸­")
        return
    
    # åˆ›å»ºæ”¹è¿›çš„VAEé‡å»ºå™¨
    vae_reconstructor = ImprovedVAEReconstructor()
    
    # æ‰§è¡Œé‡å»º
    result = vae_reconstructor.reconstruct_audio(input_file)
    
    print(f"\nâœ… æ”¹è¿›VAEé‡å»ºæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ æŸ¥çœ‹è¾“å‡ºç›®å½•: step_1_improved_vae_reconstruction/")
    print(f"ğŸµ å¯¹æ¯”åŸå§‹ã€é‡å»ºéŸ³é¢‘å’Œè¯¦ç»†åˆ†æå›¾è¡¨")

if __name__ == "__main__":
    test_improved_vae()
