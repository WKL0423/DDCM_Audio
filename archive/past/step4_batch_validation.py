#!/usr/bin/env python3
"""
Step 4: æ‰¹é‡éªŒè¯å’Œå®é™…åº”ç”¨é›†æˆ
åŸºäºStep 3çš„æœ€ä½³å‚æ•°ï¼ˆå¢å¼ºç³»æ•°1.4xï¼‰ï¼Œåœ¨å¤šä¸ªéŸ³é¢‘æ–‡ä»¶ä¸ŠéªŒè¯æ€§èƒ½
å¹¶æä¾›å¯é›†æˆåˆ°ç”Ÿäº§ç¯å¢ƒçš„å¢å¼ºç®¡é“
"""

import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import librosa
from pathlib import Path
import time
from diffusers import AudioLDM2Pipeline
from typing import Dict, Tuple, List, Optional
import warnings
import pandas as pd
from scipy import signal
warnings.filterwarnings("ignore")

class BatchValidationPipeline:
    """
    æ‰¹é‡éªŒè¯ç®¡é“
    éªŒè¯æœ€ä½³å¢å¼ºå‚æ•°åœ¨å¤šä¸ªéŸ³é¢‘æ–‡ä»¶ä¸Šçš„æ€§èƒ½
    """
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """åˆå§‹åŒ–æ‰¹é‡éªŒè¯ç®¡é“"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸ¯ åˆå§‹åŒ–æ‰¹é‡éªŒè¯ç®¡é“")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
          # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        # ç»Ÿä¸€ä½¿ç”¨float32é¿å…ç±»å‹ä¸åŒ¹é…é—®é¢˜
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
        ).to(self.device)
        
        # æœ€ä½³å‚æ•°ï¼ˆæ¥è‡ªStep 3çš„ä¼˜åŒ–ç»“æœï¼‰
        self.best_boost_factor = 1.4
        
        print(f"âœ… æ‰¹é‡éªŒè¯ç®¡é“åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ¯ æœ€ä½³å¢å¼ºç³»æ•°: {self.best_boost_factor}x")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("batch_validation_results")
        self.output_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def enhanced_vae_reconstruction(self, audio_tensor: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨æœ€ä½³å‚æ•°è¿›è¡Œå¢å¼ºVAEé‡å»º
        """
        try:
            # 1. VAEç¼–ç 
            latents = self.pipeline.vae.encode(audio_tensor).latent_dist.sample()
            latents = latents * self.pipeline.vae.config.scaling_factor
            
            # 2. åº”ç”¨æœ€ä½³å¢å¼ºï¼ˆåŸºäºStep 3çš„ç»“æœï¼‰
            enhanced_latents = self.apply_frequency_boost(latents, self.best_boost_factor)
            
            # 3. VAEè§£ç 
            enhanced_latents = enhanced_latents / self.pipeline.vae.config.scaling_factor
            reconstructed = self.pipeline.vae.decode(enhanced_latents).sample
            
            return reconstructed
            
        except Exception as e:
            print(f"âŒ å¢å¼ºé‡å»ºè¿‡ç¨‹å‡ºé”™: {e}")
            return None
    
    def apply_frequency_boost(self, latents: torch.Tensor, boost_factor: float) -> torch.Tensor:
        """
        åº”ç”¨é¢‘ç‡å¢å¼ºï¼ˆåŸºäºStep 3ä¼˜åŒ–çš„æ–¹æ³•ï¼‰
        ä½¿ç”¨æ‰‹åŠ¨numpyå®ç°é¿å…PyTorch strideé—®é¢˜
        """
        try:
            # è½¬æ¢ä¸ºnumpyè¿›è¡Œå¤„ç†
            latents_np = latents.detach().cpu().numpy()
            enhanced_np = np.zeros_like(latents_np)
            
            # å®šä¹‰Laplacianå¢å¼ºæ ¸ï¼ˆæ‰‹åŠ¨å®ç°ï¼‰
            for b in range(latents_np.shape[0]):  # batch
                for c in range(latents_np.shape[1]):  # channel
                    channel_data = latents_np[b, c]
                    
                    # è®¡ç®—Laplacianï¼ˆè¾¹ç¼˜æ£€æµ‹ï¼Œå¢å¼ºé«˜é¢‘ç»†èŠ‚ï¼‰
                    laplacian = np.zeros_like(channel_data)
                    
                    # æ‰‹åŠ¨è®¡ç®—2D Laplacianç®—å­
                    for i in range(1, channel_data.shape[0] - 1):
                        for j in range(1, channel_data.shape[1] - 1):
                            laplacian[i, j] = (
                                4 * channel_data[i, j] - 
                                channel_data[i-1, j] - channel_data[i+1, j] - 
                                channel_data[i, j-1] - channel_data[i, j+1]
                            )
                    
                    # åº”ç”¨å¢å¼º
                    enhanced_np[b, c] = channel_data + boost_factor * 0.1 * laplacian
            
            # è½¬æ¢å›tensor
            enhanced_latents = torch.from_numpy(enhanced_np).to(latents.device, latents.dtype)
            return enhanced_latents
            
        except Exception as e:
            print(f"âŒ é¢‘ç‡å¢å¼ºè¿‡ç¨‹å‡ºé”™: {e}")
            return latents
    
    def calculate_metrics(self, original: np.ndarray, reconstructed: np.ndarray, 
                         sample_rate: int) -> Dict:
        """è®¡ç®—è¯¦ç»†çš„éŸ³é¢‘è´¨é‡æŒ‡æ ‡"""
        try:
            # åŸºæœ¬æŒ‡æ ‡
            mse = np.mean((original - reconstructed) ** 2)
            snr = 10 * np.log10(np.var(original) / (mse + 1e-10))
            correlation = np.corrcoef(original, reconstructed)[0, 1]
            
            # é¢‘åŸŸåˆ†æ
            freqs_orig, psd_orig = signal.welch(original, sample_rate, nperseg=1024)
            freqs_recon, psd_recon = signal.welch(reconstructed, sample_rate, nperseg=1024)
            
            # é¢‘æ®µä¿æŒç‡
            low_freq_mask = freqs_orig < 500
            mid_freq_mask = (freqs_orig >= 500) & (freqs_orig < 4000)
            high_freq_mask = freqs_orig >= 4000
            
            low_retention = np.sum(psd_recon[low_freq_mask]) / (np.sum(psd_orig[low_freq_mask]) + 1e-10)
            mid_retention = np.sum(psd_recon[mid_freq_mask]) / (np.sum(psd_orig[mid_freq_mask]) + 1e-10)
            high_retention = np.sum(psd_recon[high_freq_mask]) / (np.sum(psd_orig[high_freq_mask]) + 1e-10)
            
            return {
                'snr': snr,
                'correlation': correlation,
                'mse': mse,
                'low_freq_retention': low_retention,
                'mid_freq_retention': mid_retention,
                'high_freq_retention': high_retention
            }
            
        except Exception as e:
            print(f"âŒ æŒ‡æ ‡è®¡ç®—å‡ºé”™: {e}")
            return {}
    
    def process_audio_file(self, audio_path: str) -> Dict:
        """å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        try:
            print(f"\nğŸµ å¤„ç†éŸ³é¢‘æ–‡ä»¶: {Path(audio_path).name}")
            
            # åŠ è½½éŸ³é¢‘
            audio, sr = torchaudio.load(audio_path)
            
            # ç¡®ä¿å•å£°é“
            if audio.shape[0] > 1:
                audio = torch.mean(audio, dim=0, keepdim=True)
              # é‡é‡‡æ ·åˆ°48kHz (CLAPç‰¹å¾æå–å™¨è¦æ±‚)
            target_sr = 48000
            if sr != target_sr:
                resampler = torchaudio.transforms.Resample(sr, target_sr)
                audio = resampler(audio)
                sr = target_sr
            
            # è£å‰ªåˆ°åˆé€‚é•¿åº¦ï¼ˆ10ç§’ï¼‰
            max_length = 10 * sr
            if audio.shape[1] > max_length:
                audio = audio[:, :max_length]
              # é¢„å¤„ç†ï¼ˆmelé¢‘è°±ï¼‰
            inputs = self.pipeline.feature_extractor(
                audio.squeeze().numpy(), 
                sampling_rate=sr, 
                return_tensors="pt"
            )
            audio_tensor = inputs.input_features.to(self.device).float()
            
            # åŸå§‹VAEé‡å»º
            start_time = time.time()
            with torch.no_grad():
                original_latents = self.pipeline.vae.encode(audio_tensor).latent_dist.sample()
                original_latents = original_latents * self.pipeline.vae.config.scaling_factor
                original_latents = original_latents / self.pipeline.vae.config.scaling_factor
                original_reconstructed = self.pipeline.vae.decode(original_latents).sample
            original_time = time.time() - start_time
            
            # å¢å¼ºVAEé‡å»º
            start_time = time.time()
            with torch.no_grad():
                enhanced_reconstructed = self.enhanced_vae_reconstruction(audio_tensor)
            enhanced_time = time.time() - start_time
            
            if enhanced_reconstructed is None:
                return None
            
            # è½¬æ¢ä¸ºnumpyè¿›è¡Œåˆ†æ
            original_np = audio.squeeze().numpy()
            original_recon_np = self.mel_to_audio(original_reconstructed)
            enhanced_recon_np = self.mel_to_audio(enhanced_reconstructed)
            
            # è®¡ç®—æŒ‡æ ‡
            original_metrics = self.calculate_metrics(original_np, original_recon_np, sr)
            enhanced_metrics = self.calculate_metrics(original_np, enhanced_recon_np, sr)
            
            # è®¡ç®—æ”¹è¿›
            improvements = {}
            for key in original_metrics:
                if key in ['snr', 'correlation']:
                    improvements[f"{key}_improvement"] = enhanced_metrics[key] - original_metrics[key]
                else:
                    improvements[f"{key}_improvement"] = (enhanced_metrics[key] / original_metrics[key] - 1) * 100
            
            return {
                'filename': Path(audio_path).name,
                'audio_length': audio.shape[1] / sr,
                'processing_time_original': original_time,
                'processing_time_enhanced': enhanced_time,
                'original_metrics': original_metrics,
                'enhanced_metrics': enhanced_metrics,
                'improvements': improvements,
                'audio_original': original_np,
                'audio_original_recon': original_recon_np,
                'audio_enhanced_recon': enhanced_recon_np,
                'sample_rate': sr
            }
              except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {audio_path} æ—¶å‡ºé”™: {e}")
            return None
    
    def mel_to_audio(self, mel_tensor: torch.Tensor) -> np.ndarray:
        """å°†melé¢‘è°±è½¬æ¢å›éŸ³é¢‘ï¼ˆä½¿ç”¨AudioLDM2çš„vocoderï¼‰"""
        try:
            # ä½¿ç”¨AudioLDM2çš„vocoderè¿›è¡Œè½¬æ¢
            with torch.no_grad():
                # mel_tensoråº”è¯¥å·²ç»æ˜¯æ­£ç¡®æ ¼å¼çš„melé¢‘è°±
                audio_tensor = self.pipeline.vocoder(mel_tensor.unsqueeze(0))
                audio_np = audio_tensor.squeeze().detach().cpu().numpy()
                
            return audio_np
            
        except Exception as e:
            print(f"âŒ melè½¬éŸ³é¢‘å‡ºé”™: {e}")
            # å¦‚æœvocoderå¤±è´¥ï¼Œè¿”å›åŒ¹é…é•¿åº¦çš„é™éŸ³
            try:
                # ä¼°ç®—åˆç†çš„éŸ³é¢‘é•¿åº¦
                estimated_length = 16384  # é»˜è®¤é•¿åº¦
                return np.zeros(estimated_length)
            except:
                return np.zeros(16000)  # å›é€€åˆ°1ç§’é™éŸ³
    
    def batch_validate(self, audio_files: List[str]) -> Dict:
        """æ‰¹é‡éªŒè¯å¤šä¸ªéŸ³é¢‘æ–‡ä»¶"""
        print(f"\nğŸ¯ å¼€å§‹æ‰¹é‡éªŒè¯")
        print(f"   ğŸ“ æ–‡ä»¶æ•°é‡: {len(audio_files)}")
        print(f"   âš¡ å¢å¼ºç³»æ•°: {self.best_boost_factor}x")
        
        results = []
        successful_files = 0
        
        for i, audio_file in enumerate(audio_files):
            print(f"\nğŸ“Š è¿›åº¦: {i+1}/{len(audio_files)}")
            
            result = self.process_audio_file(audio_file)
            if result is not None:
                results.append(result)
                successful_files += 1
                
                # ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ
                self.save_individual_result(result, i)
        
        if not results:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ–‡ä»¶")
            return {}
        
        # è®¡ç®—ç»Ÿè®¡æ‘˜è¦
        summary = self.calculate_batch_summary(results)
        
        # ç”Ÿæˆæ‰¹é‡æŠ¥å‘Š
        self.generate_batch_report(results, summary)
        
        print(f"\nâœ… æ‰¹é‡éªŒè¯å®Œæˆ")
        print(f"   âœ… æˆåŠŸå¤„ç†: {successful_files}/{len(audio_files)} æ–‡ä»¶")
        print(f"   ğŸ“Š å¹³å‡SNRæ”¹è¿›: {summary['avg_snr_improvement']:.2f} dB")
        print(f"   ğŸµ å¹³å‡é«˜é¢‘æ”¹è¿›: {summary['avg_high_freq_improvement']:.1f}%")
        
        return summary
    
    def save_individual_result(self, result: Dict, index: int):
        """ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ"""
        try:
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            filename_base = Path(result['filename']).stem
            
            # åŸå§‹é‡å»º
            sf.write(
                self.output_dir / f"{filename_base}_original_reconstruction.wav",
                result['audio_original_recon'],
                result['sample_rate']
            )
            
            # å¢å¼ºé‡å»º
            sf.write(
                self.output_dir / f"{filename_base}_enhanced_reconstruction.wav",
                result['audio_enhanced_recon'],
                result['sample_rate']
            )
            
            # ç”Ÿæˆå¯¹æ¯”å›¾
            self.plot_comparison(result, index)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")
    
    def plot_comparison(self, result: Dict, index: int):
        """ç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # æ—¶åŸŸå¯¹æ¯”
            time_axis = np.linspace(0, len(result['audio_original']) / result['sample_rate'], 
                                  len(result['audio_original']))
            
            axes[0, 0].plot(time_axis, result['audio_original'], label='åŸå§‹', alpha=0.7)
            axes[0, 0].plot(time_axis[:len(result['audio_original_recon'])], 
                           result['audio_original_recon'], label='VAEé‡å»º', alpha=0.7)
            axes[0, 0].plot(time_axis[:len(result['audio_enhanced_recon'])], 
                           result['audio_enhanced_recon'], label='å¢å¼ºé‡å»º', alpha=0.7)
            axes[0, 0].set_title('æ—¶åŸŸå¯¹æ¯”')
            axes[0, 0].set_xlabel('æ—¶é—´ (s)')
            axes[0, 0].set_ylabel('å¹…åº¦')
            axes[0, 0].legend()
            axes[0, 0].grid(True)
            
            # é¢‘åŸŸå¯¹æ¯”
            freqs, psd_orig = signal.welch(result['audio_original'], result['sample_rate'])
            _, psd_orig_recon = signal.welch(result['audio_original_recon'], result['sample_rate'])
            _, psd_enhanced = signal.welch(result['audio_enhanced_recon'], result['sample_rate'])
            
            axes[0, 1].semilogy(freqs, psd_orig, label='åŸå§‹', alpha=0.7)
            axes[0, 1].semilogy(freqs, psd_orig_recon, label='VAEé‡å»º', alpha=0.7)
            axes[0, 1].semilogy(freqs, psd_enhanced, label='å¢å¼ºé‡å»º', alpha=0.7)
            axes[0, 1].set_title('åŠŸç‡è°±å¯†åº¦å¯¹æ¯”')
            axes[0, 1].set_xlabel('é¢‘ç‡ (Hz)')
            axes[0, 1].set_ylabel('åŠŸç‡è°±å¯†åº¦')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
            
            # æŒ‡æ ‡å¯¹æ¯”
            metrics = ['snr', 'correlation', 'high_freq_retention']
            original_values = [result['original_metrics'][m] for m in metrics]
            enhanced_values = [result['enhanced_metrics'][m] for m in metrics]
            
            x = np.arange(len(metrics))
            width = 0.35
            
            axes[1, 0].bar(x - width/2, original_values, width, label='åŸå§‹VAE', alpha=0.7)
            axes[1, 0].bar(x + width/2, enhanced_values, width, label='å¢å¼ºVAE', alpha=0.7)
            axes[1, 0].set_title('å…³é”®æŒ‡æ ‡å¯¹æ¯”')
            axes[1, 0].set_xlabel('æŒ‡æ ‡')
            axes[1, 0].set_ylabel('æ•°å€¼')
            axes[1, 0].set_xticks(x)
            axes[1, 0].set_xticklabels(['SNR', 'ç›¸å…³ç³»æ•°', 'é«˜é¢‘ä¿æŒç‡'])
            axes[1, 0].legend()
            axes[1, 0].grid(True)
            
            # æ”¹è¿›ç™¾åˆ†æ¯”
            improvements = [
                result['improvements']['snr_improvement'],
                result['improvements']['correlation_improvement'],
                result['improvements']['high_freq_retention_improvement']
            ]
            
            colors = ['green' if x > 0 else 'red' for x in improvements]
            axes[1, 1].bar(metrics, improvements, color=colors, alpha=0.7)
            axes[1, 1].set_title('æ”¹è¿›ç™¾åˆ†æ¯”')
            axes[1, 1].set_xlabel('æŒ‡æ ‡')
            axes[1, 1].set_ylabel('æ”¹è¿› (%/dB)')
            axes[1, 1].set_xticklabels(['SNR (dB)', 'ç›¸å…³ç³»æ•° (%)', 'é«˜é¢‘ä¿æŒç‡ (%)'])
            axes[1, 1].grid(True)
            axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
            
            plt.tight_layout()
            
            filename_base = Path(result['filename']).stem
            plt.savefig(self.output_dir / f"{filename_base}_comparison.png", dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¯¹æ¯”å›¾æ—¶å‡ºé”™: {e}")
    
    def calculate_batch_summary(self, results: List[Dict]) -> Dict:
        """è®¡ç®—æ‰¹é‡å¤„ç†çš„ç»Ÿè®¡æ‘˜è¦"""
        try:
            # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
            snr_improvements = [r['improvements']['snr_improvement'] for r in results]
            correlation_improvements = [r['improvements']['correlation_improvement'] for r in results]
            high_freq_improvements = [r['improvements']['high_freq_retention_improvement'] for r in results]
            
            processing_times = [r['processing_time_enhanced'] for r in results]
            
            return {
                'total_files': len(results),
                'avg_snr_improvement': np.mean(snr_improvements),
                'std_snr_improvement': np.std(snr_improvements),
                'avg_correlation_improvement': np.mean(correlation_improvements),
                'std_correlation_improvement': np.std(correlation_improvements),
                'avg_high_freq_improvement': np.mean(high_freq_improvements),
                'std_high_freq_improvement': np.std(high_freq_improvements),
                'avg_processing_time': np.mean(processing_times),
                'std_processing_time': np.std(processing_times),
                'snr_improvements': snr_improvements,
                'correlation_improvements': correlation_improvements,
                'high_freq_improvements': high_freq_improvements,
                'boost_factor_used': self.best_boost_factor
            }
            
        except Exception as e:
            print(f"âŒ è®¡ç®—æ‘˜è¦æ—¶å‡ºé”™: {e}")
            return {}
    
    def generate_batch_report(self, results: List[Dict], summary: Dict):
        """ç”Ÿæˆæ‰¹é‡éªŒè¯æŠ¥å‘Š"""
        try:
            timestamp = int(time.time())
            report_file = self.output_dir / f"batch_validation_report_{timestamp}.md"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(f"# AudioLDM2 VAEå¢å¼º - æ‰¹é‡éªŒè¯æŠ¥å‘Š\n\n")
                f.write(f"## éªŒè¯æ¦‚å†µ\n")
                f.write(f"- éªŒè¯æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"- å¤„ç†æ–‡ä»¶æ•°: {summary['total_files']}\n")
                f.write(f"- ä½¿ç”¨å¢å¼ºç³»æ•°: {summary['boost_factor_used']}x\n\n")
                
                f.write(f"## æ€»ä½“æ€§èƒ½\n")
                f.write(f"- **å¹³å‡SNRæ”¹è¿›**: {summary['avg_snr_improvement']:.2f} Â± {summary['std_snr_improvement']:.2f} dB\n")
                f.write(f"- **å¹³å‡ç›¸å…³ç³»æ•°æ”¹è¿›**: {summary['avg_correlation_improvement']:.1f} Â± {summary['std_correlation_improvement']:.1f}%\n")
                f.write(f"- **å¹³å‡é«˜é¢‘æ”¹è¿›**: {summary['avg_high_freq_improvement']:.1f} Â± {summary['std_high_freq_improvement']:.1f}%\n")
                f.write(f"- **å¹³å‡å¤„ç†æ—¶é—´**: {summary['avg_processing_time']:.2f} Â± {summary['std_processing_time']:.2f} ç§’\n\n")
                
                f.write(f"## è¯¦ç»†ç»“æœ\n\n")
                f.write(f"| æ–‡ä»¶å | SNRæ”¹è¿›(dB) | ç›¸å…³æ€§æ”¹è¿›(%) | é«˜é¢‘æ”¹è¿›(%) | å¤„ç†æ—¶é—´(s) |\n")
                f.write(f"|--------|-------------|---------------|-------------|-------------|\n")
                
                for result in results:
                    f.write(f"| {result['filename']} | "
                           f"{result['improvements']['snr_improvement']:.2f} | "
                           f"{result['improvements']['correlation_improvement']:.1f} | "
                           f"{result['improvements']['high_freq_retention_improvement']:.1f} | "
                           f"{result['processing_time_enhanced']:.2f} |\n")
                
                f.write(f"\n## æ€§èƒ½åˆ†æ\n")
                
                # æ€§èƒ½è¯„çº§
                avg_snr = summary['avg_snr_improvement']
                avg_high_freq = summary['avg_high_freq_improvement']
                
                if avg_high_freq > 50 and avg_snr > -5:
                    rating = "ğŸ† ä¼˜ç§€"
                elif avg_high_freq > 30 and avg_snr > -7:
                    rating = "ğŸ¥‡ è‰¯å¥½"
                elif avg_high_freq > 10:
                    rating = "ğŸ¥‰ å¯ç”¨"
                else:
                    rating = "âŒ éœ€è¦æ”¹è¿›"
                
                f.write(f"### æ•´ä½“è¯„çº§: {rating}\n\n")
                
                f.write(f"### æŠ€æœ¯è¯„ä¼°\n")
                f.write(f"- **é«˜é¢‘æ¢å¤æ•ˆæœ**: {'ä¼˜ç§€' if avg_high_freq > 50 else 'è‰¯å¥½' if avg_high_freq > 20 else 'ä¸€èˆ¬'}\n")
                f.write(f"- **è´¨é‡ä¿æŒ**: {'è‰¯å¥½' if avg_snr > -5 else 'å¯æ¥å—' if avg_snr > -8 else 'éœ€è¦æ”¹è¿›'}\n")
                f.write(f"- **å¤„ç†æ•ˆç‡**: {'å¿«é€Ÿ' if summary['avg_processing_time'] < 2 else 'ä¸­ç­‰' if summary['avg_processing_time'] < 5 else 'è¾ƒæ…¢'}\n\n")
                
                f.write(f"## åº”ç”¨å»ºè®®\n")
                f.write(f"åŸºäºæ‰¹é‡éªŒè¯ç»“æœï¼Œæ¨èåœ¨ä»¥ä¸‹åœºæ™¯ä½¿ç”¨å¢å¼ºç³»æ•° {summary['boost_factor_used']}xï¼š\n\n")
                
                if avg_high_freq > 50:
                    f.write(f"âœ… **æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ**\n")
                    f.write(f"- é«˜é¢‘æ¢å¤æ•ˆæœæ˜¾è‘—\n")
                    f.write(f"- è´¨é‡æƒè¡¡å¯æ¥å—\n")
                    f.write(f"- é€‚åˆéŸ³ä¹å’Œè¯­éŸ³å¢å¼ºåº”ç”¨\n\n")
                else:
                    f.write(f"âš ï¸ **å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–**\n")
                    f.write(f"- å¯èƒ½éœ€è¦è°ƒæ•´å¢å¼ºå‚æ•°\n")
                    f.write(f"- è€ƒè™‘é’ˆå¯¹ä¸åŒéŸ³é¢‘ç±»å‹ä½¿ç”¨ä¸åŒå‚æ•°\n\n")
                
                f.write(f"## æ–‡ä»¶è¾“å‡º\n")
                f.write(f"æœ¬æ¬¡éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶ï¼š\n")
                f.write(f"- é‡å»ºéŸ³é¢‘: `*_original_reconstruction.wav`, `*_enhanced_reconstruction.wav`\n")
                f.write(f"- å¯¹æ¯”å›¾è¡¨: `*_comparison.png`\n")
                f.write(f"- éªŒè¯æŠ¥å‘Š: `batch_validation_report_{timestamp}.md`\n")
            
            print(f"ğŸ“ æ‰¹é‡éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ AudioLDM2 VAEå¢å¼º - Step 4: æ‰¹é‡éªŒè¯")
    print("=" * 60)
    
    # é…ç½®Pythonç¯å¢ƒ
    from pathlib import Path
    workspace_path = Path(__file__).parent
    
    # åˆå§‹åŒ–æ‰¹é‡éªŒè¯ç®¡é“
    validator = BatchValidationPipeline()
    
    # æŸ¥æ‰¾å¯ç”¨çš„éŸ³é¢‘æ–‡ä»¶
    audio_extensions = ['.wav', '.mp3', '.flac', '.m4a']
    audio_files = []
    
    # åœ¨å½“å‰ç›®å½•æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
    for ext in audio_extensions:
        audio_files.extend(list(workspace_path.glob(f"*{ext}")))
    
    # åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾å‚è€ƒéŸ³é¢‘
    output_dirs = ['simple_vae_reconstruction', 'simple_latent_enhanced', 'final_parameter_optimization']
    for output_dir in output_dirs:
        output_path = workspace_path / output_dir
        if output_path.exists():
            for ext in audio_extensions:
                audio_files.extend(list(output_path.glob(f"*{ext}")))
    
    # è¿‡æ»¤æ‰é‡å»ºæ–‡ä»¶ï¼Œåªä¿ç•™åŸå§‹æ–‡ä»¶
    original_files = []
    for f in audio_files:
        filename = f.name.lower()
        if not any(keyword in filename for keyword in ['reconstruction', 'enhanced', 'output', 'generated']):
            original_files.append(str(f))
    
    if not original_files:
        print("âŒ æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶è¿›è¡ŒéªŒè¯")
        print("ğŸ” å°è¯•åˆ›å»ºä¸€ä¸ªæµ‹è¯•éŸ³é¢‘æ–‡ä»¶...")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•éŸ³é¢‘
        test_audio_path = workspace_path / "test_validation.wav"
        duration = 5  # 5ç§’
        sample_rate = 16000
        t = np.linspace(0, duration, duration * sample_rate)
        
        # åˆ›å»ºåŒ…å«å¤šä¸ªé¢‘ç‡æˆåˆ†çš„æµ‹è¯•ä¿¡å·
        signal_440 = 0.3 * np.sin(2 * np.pi * 440 * t)  # A4éŸ³ç¬¦
        signal_880 = 0.2 * np.sin(2 * np.pi * 880 * t)  # A5éŸ³ç¬¦
        signal_1760 = 0.1 * np.sin(2 * np.pi * 1760 * t)  # A6éŸ³ç¬¦ï¼ˆé«˜é¢‘ï¼‰
        
        test_signal = signal_440 + signal_880 + signal_1760
        
        # æ·»åŠ ä¸€äº›å™ªå£°å’ŒåŒ…ç»œ
        envelope = np.exp(-t / 2)  # è¡°å‡åŒ…ç»œ
        noise = 0.05 * np.random.randn(len(t))
        test_signal = (test_signal * envelope + noise) * 0.5
        
        sf.write(test_audio_path, test_signal, sample_rate)
        original_files = [str(test_audio_path)]
        print(f"âœ… åˆ›å»ºæµ‹è¯•éŸ³é¢‘: {test_audio_path}")
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(original_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶ç”¨äºéªŒè¯:")
    for f in original_files[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
        print(f"   ğŸ“„ {Path(f).name}")
    if len(original_files) > 5:
        print(f"   ... è¿˜æœ‰ {len(original_files) - 5} ä¸ªæ–‡ä»¶")
    
    # æ‰§è¡Œæ‰¹é‡éªŒè¯
    try:
        summary = validator.batch_validate(original_files)
        
        if summary:
            print(f"\nğŸ‰ æ‰¹é‡éªŒè¯æˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“Š å…³é”®ç»“æœ:")
            print(f"   ğŸ“ˆ å¹³å‡SNRæ”¹è¿›: {summary['avg_snr_improvement']:.2f} dB")
            print(f"   ğŸµ å¹³å‡é«˜é¢‘æ”¹è¿›: {summary['avg_high_freq_improvement']:.1f}%")
            print(f"   â±ï¸ å¹³å‡å¤„ç†æ—¶é—´: {summary['avg_processing_time']:.2f} ç§’")
            
            # è¯„ä¼°æ•´ä½“æ€§èƒ½
            if summary['avg_high_freq_improvement'] > 50:
                print(f"   ğŸ† è¯„çº§: ä¼˜ç§€ - æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ")
            elif summary['avg_high_freq_improvement'] > 20:
                print(f"   ğŸ¥‡ è¯„çº§: è‰¯å¥½ - é€‚åˆå¤§å¤šæ•°åº”ç”¨")
            else:
                print(f"   ğŸ¥‰ è¯„çº§: éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            print("âŒ æ‰¹é‡éªŒè¯å¤±è´¥")
    
    except Exception as e:
        print(f"âŒ æ‰¹é‡éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
