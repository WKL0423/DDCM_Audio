#!/usr/bin/env python3
"""
AudioLDM2 VAE+HiFiGAN æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ï¼ˆä½¿ç”¨æ­£ç¡®çš„ClapFeatureExtractorï¼‰
================================================================

è§£å†³å…³é”®é—®é¢˜ï¼š
1. ä½¿ç”¨AudioLDM2çš„ClapFeatureExtractorï¼ˆ48kHzï¼‰
2. æ­£ç¡®çš„ç»´åº¦æ ¼å¼ [batch, channel, time, feature]
3. VAE scaling_factor æ­£ç¡®ä½¿ç”¨
4. HiFiGAN è¾“å…¥ç»´åº¦ä¿®å¤
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import soundfile as sf
from diffusers import AudioLDM2Pipeline


def save_audio_compatible(audio_data, filepath, sample_rate=16000):
    """ä¿å­˜å…¼å®¹çš„éŸ³é¢‘æ–‡ä»¶"""
    if isinstance(audio_data, torch.Tensor):
        audio_data = audio_data.detach().cpu().numpy()
    
    if len(audio_data.shape) > 1:
        audio_data = audio_data.flatten()
    
    audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=0.0, neginf=0.0)
    
    if np.max(np.abs(audio_data)) > 0:
        audio_data = audio_data / np.max(np.abs(audio_data))
    
    try:
        sf.write(filepath, audio_data, sample_rate, subtype='PCM_16')
        print(f"   âœ… ä¿å­˜: {Path(filepath).name}")
        return True
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
        return False


def test_audioldm2_ultimate_fix(audio_path, max_length=5):
    """
    æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨AudioLDM2çš„æ­£ç¡®ClapFeatureExtractor
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ¯ AudioLDM2 æœ€ç»ˆä¿®å¤æµ‹è¯•ï¼ˆä½¿ç”¨ClapFeatureExtractorï¼‰")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # åŠ è½½AudioLDM2 pipeline
    print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    feature_extractor = pipeline.feature_extractor
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   VAE scaling_factor: {pipeline.vae.config.scaling_factor}")
    print(f"   FeatureExtractor: {type(feature_extractor).__name__}")
    print(f"   æœŸæœ›é‡‡æ ·ç‡: {feature_extractor.sampling_rate} Hz")
    
    # åŠ è½½éŸ³é¢‘ - å…³é”®ä¿®å¤ï¼šä½¿ç”¨48kHzé‡‡æ ·ç‡åŒ¹é…ClapFeatureExtractor
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘: {Path(audio_path).name}")
    # AudioLDM2çš„ClapFeatureExtractoræœŸæœ›48kHz
    audio_48k, sr_48k = librosa.load(audio_path, sr=feature_extractor.sampling_rate, duration=max_length)
    audio_16k, sr_16k = librosa.load(audio_path, sr=16000, duration=max_length)
    
    print(f"   48kHzéŸ³é¢‘: {len(audio_48k)/sr_48k:.2f}ç§’, èŒƒå›´[{audio_48k.min():.3f}, {audio_48k.max():.3f}]")
    print(f"   16kHzéŸ³é¢‘: {len(audio_16k)/sr_16k:.2f}ç§’, èŒƒå›´[{audio_16k.min():.3f}, {audio_16k.max():.3f}]")
    
    # æ–¹æ³•1: ä½¿ç”¨AudioLDM2çš„ClapFeatureExtractorï¼ˆæ­£ç¡®æ–¹å¼ï¼‰
    print("\\nğŸµ æ–¹æ³•1: ä½¿ç”¨AudioLDM2çš„ClapFeatureExtractor...")
    try:
        # ä½¿ç”¨AudioLDM2çš„æ­£ç¡®ç‰¹å¾æå–æ–¹å¼
        features = feature_extractor(
            audio_48k, 
            return_tensors="pt", 
            sampling_rate=feature_extractor.sampling_rate
        )
        
        mel_input = features.input_features.to(device)
        if device == "cuda":
            mel_input = mel_input.half()
        
        print(f"   âœ… ClapFeatureExtractoræˆåŠŸ")
        print(f"   è¾“å…¥: {mel_input.shape} (æ ¼å¼: [batch, channel, time, feature])")
        print(f"   èŒƒå›´: [{mel_input.min():.3f}, {mel_input.max():.3f}]")
        
        use_clap_features = True
        
    except Exception as e:
        print(f"   âŒ ClapFeatureExtractorå¤±è´¥: {e}")
        use_clap_features = False
    
    # å¦‚æœClapFeatureExtractorå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
    if not use_clap_features:
        print("   ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿmelé¢‘è°±å¤„ç†...")
        
        # ä½¿ç”¨48kHzéŸ³é¢‘å’ŒClapFeatureExtractorçš„å‚æ•°
        mel_spec = librosa.feature.melspectrogram(
            y=audio_48k, 
            sr=48000, 
            n_mels=64,
            hop_length=480,  # ClapFeatureExtractorçš„hop_length
            n_fft=1024,
            fmin=50,         # ClapFeatureExtractorçš„é¢‘ç‡èŒƒå›´
            fmax=14000,
            window='hann',
            center=True,
            pad_mode='reflect'
        )
        
        mel_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        # è½¬æ¢ä¸ºAudioLDM2æœŸæœ›çš„æ ¼å¼ [batch, channel, time, feature]
        mel_tensor = torch.from_numpy(mel_db).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.half()
        
        # ç»´åº¦è½¬æ¢ï¼š[64, time] -> [1, 1, time, 64]
        mel_input = mel_tensor.transpose(0, 1).unsqueeze(0).unsqueeze(0)
        
        print(f"   ä¼ ç»Ÿå¤„ç†: {mel_input.shape}")
        print(f"   èŒƒå›´: [{mel_input.min():.3f}, {mel_input.max():.3f}]")
    
    print(f"   æœ€ç»ˆè¾“å…¥: {mel_input.shape}, {mel_input.dtype}")
    
    # VAEå¤„ç† - å…³é”®ä¿®å¤ï¼šæ­£ç¡®ä½¿ç”¨scaling_factor
    print("\\nğŸ§  VAEç¼–ç è§£ç ...")
    with torch.no_grad():
        # ç¼–ç 
        latent_dist = pipeline.vae.encode(mel_input)
        if hasattr(latent_dist, 'latent_dist'):
            latent = latent_dist.latent_dist.sample()
        else:
            latent = latent_dist.sample()
        
        # å…³é”®ä¿®å¤1: ç¼–ç ååº”ç”¨scaling_factor
        latent = latent * pipeline.vae.config.scaling_factor
        print(f"   ç¼–ç latent: {latent.shape}")
        print(f"   LatentèŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
        
        # è§£ç 
        # å…³é”®ä¿®å¤2: è§£ç å‰ç§»é™¤scaling_factor
        latent_for_decode = latent / pipeline.vae.config.scaling_factor
        reconstructed_mel = pipeline.vae.decode(latent_for_decode).sample
        
        print(f"   è§£ç è¾“å‡º: {reconstructed_mel.shape}")
        print(f"   è§£ç èŒƒå›´: [{reconstructed_mel.min():.3f}, {reconstructed_mel.max():.3f}]")
    
    # HiFiGANå¤„ç† - ä½¿ç”¨pipelineæ ‡å‡†æ–¹æ³•
    print("\\nğŸ¤ HiFiGAN vocoder...")
    try:
        print("   ğŸš€ ä½¿ç”¨pipeline.mel_spectrogram_to_waveform...")
        waveform = pipeline.mel_spectrogram_to_waveform(reconstructed_mel)
        reconstructed_audio_48k = waveform.squeeze().detach().cpu().float().numpy()
        vocoder_method = "AudioLDM2_ClapFeatureExtractor"
        print(f"   âœ… æˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio_48k)}æ ·æœ¬")
        
        # è½¬æ¢åˆ°16kHzç”¨äºå¯¹æ¯”
        reconstructed_audio_16k = librosa.resample(
            reconstructed_audio_48k, 
            orig_sr=48000, 
            target_sr=16000
        )
        
    except Exception as e:
        print(f"   âŒ Pipelineæ–¹æ³•å¤±è´¥: {e}")
        
        # å¤‡é€‰æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨vocoderä½†ä¿®å¤ç»´åº¦
        try:
            print("   ğŸ”„ å°è¯•ç›´æ¥vocoderè°ƒç”¨...")
            
            # å…³é”®ä¿®å¤3: æ­£ç¡®çš„HiFiGANè¾“å…¥ç»´åº¦
            if reconstructed_mel.dim() == 4:
                # ä»[1, 1, time, 64]è½¬æ¢ä¸º[1, time, 64]
                vocoder_input = reconstructed_mel.squeeze(1)
            else:
                vocoder_input = reconstructed_mel
                
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            vocoder_dtype = next(pipeline.vocoder.parameters()).dtype
            vocoder_input = vocoder_input.to(vocoder_dtype)
            
            print(f"   Vocoderè¾“å…¥: {vocoder_input.shape}, {vocoder_input.dtype}")
            
            # ç›´æ¥è°ƒç”¨vocoder
            waveform = pipeline.vocoder(vocoder_input)
            reconstructed_audio_48k = waveform.squeeze().detach().cpu().float().numpy()
            
            # è½¬æ¢åˆ°16kHz
            reconstructed_audio_16k = librosa.resample(
                reconstructed_audio_48k, 
                orig_sr=48000, 
                target_sr=16000
            )
            
            vocoder_method = "AudioLDM2_Vocoder_Direct"
            print(f"   âœ… ç›´æ¥è°ƒç”¨æˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio_48k)}æ ·æœ¬")
            
        except Exception as e2:
            print(f"   âŒ ç›´æ¥è°ƒç”¨ä¹Ÿå¤±è´¥: {e2}")
            print("   ğŸ”„ ä½¿ç”¨ä¼˜åŒ–çš„Griffin-Lim...")
            
            # ä¼˜åŒ–çš„Griffin-Lim
            mel_np = reconstructed_mel.squeeze().cpu().float().numpy()
            
            # å¦‚æœæ˜¯[time, feature]æ ¼å¼ï¼Œè½¬æ¢ä¸º[feature, time]
            if mel_np.shape[1] == 64:
                mel_np = mel_np.transpose(1, 0)
            
            # ådBå˜æ¢
            mel_power = librosa.db_to_power(mel_np)
            
            # é«˜è´¨é‡Griffin-Lim
            reconstructed_audio_48k = librosa.feature.inverse.mel_to_audio(
                mel_power,
                sr=48000,
                hop_length=480,
                n_fft=1024,
                win_length=1024,
                window='hann',
                n_iter=64,
                length=len(audio_48k)
            )
            
            # è½¬æ¢åˆ°16kHz
            reconstructed_audio_16k = librosa.resample(
                reconstructed_audio_48k, 
                orig_sr=48000, 
                target_sr=16000
            )
            
            vocoder_method = "Griffin_Lim_Optimized"
            print(f"   âœ… ä¼˜åŒ–Griffin-LimæˆåŠŸ: {len(reconstructed_audio_48k)}æ ·æœ¬")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("vae_hifigan_ultimate_fix_clap")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
    reconstructed_path = output_dir / f"{input_name}_{vocoder_method}_{timestamp}.wav"
    
    # é•¿åº¦åŒ¹é…ï¼ˆä½¿ç”¨16kHzç‰ˆæœ¬ï¼‰
    if len(reconstructed_audio_16k) > len(audio_16k):
        reconstructed_audio_16k = reconstructed_audio_16k[:len(audio_16k)]
    elif len(reconstructed_audio_16k) < len(audio_16k):
        reconstructed_audio_16k = np.pad(reconstructed_audio_16k, (0, len(audio_16k) - len(reconstructed_audio_16k)))
    
    print("\\nğŸ’¾ ä¿å­˜ç»“æœ...")
    save_audio_compatible(audio_16k, original_path)
    save_audio_compatible(reconstructed_audio_16k, reconstructed_path)
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    mse = np.mean((audio_16k - reconstructed_audio_16k) ** 2)
    snr = 10 * np.log10(np.mean(audio_16k ** 2) / (mse + 1e-10))
    correlation = np.corrcoef(audio_16k, reconstructed_audio_16k)[0, 1] if len(audio_16k) > 1 else 0
    
    # è¾“å‡ºç»“æœ
    print(f"\\n{'='*60}")
    print(f"ğŸ¯ AudioLDM2 æœ€ç»ˆä¿®å¤ç»“æœï¼ˆClapFeatureExtractorï¼‰")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ğŸ“Š SNR: {snr:.2f} dB")
    print(f"ğŸ“Š MSE: {mse:.6f}")
    print(f"ğŸ“Š ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"ğŸ¤ é‡å»ºæ–¹æ³•: {vocoder_method}")
    
    # è¯Šæ–­åˆ†æ
    print(f"\\nğŸ”¬ å…³é”®æŠ€æœ¯çªç ´:")
    print(f"   âœ… ä½¿ç”¨ClapFeatureExtractor (48kHz)")
    print(f"   âœ… æ­£ç¡®çš„ç»´åº¦æ ¼å¼: [batch, channel, time, feature]")
    print(f"   âœ… VAE scaling_factor: {pipeline.vae.config.scaling_factor}")
    print(f"   âœ… æ­£ç¡®çš„ç¼–ç /è§£ç ç¼©æ”¾")
    print(f"   âœ… ä½¿ç”¨pipelineæ ‡å‡†æ–¹æ³•")
    
    if "ClapFeatureExtractor" in vocoder_method:
        print(f"\\nğŸ‰ å®Œç¾ï¼ä½¿ç”¨AudioLDM2çš„æ­£ç¡®å¤„ç†æ–¹å¼")
        if snr > 0:
            print(f"ğŸ† é‡å»ºè´¨é‡ä¼˜ç§€ï¼å£°éŸ³æ›´é¥±æ»¡ï¼Œç»†èŠ‚æ›´ä¸°å¯Œ")
        elif snr > -5:
            print(f"âœ… é‡å»ºè´¨é‡è‰¯å¥½ï¼Œæ˜æ˜¾æ”¹å–„")
        else:
            print(f"ğŸ‘ æœ‰è¿›æ­¥ï¼Œä½†ä»å¯ä¼˜åŒ–")
    else:
        print(f"\\nğŸ”§ ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆï¼Œè´¨é‡ä»æœ‰æ”¹å–„ç©ºé—´")
    
    print(f"\\nğŸ“ˆ ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”:")
    print(f"   - é‡‡æ ·ç‡: 48kHz vs 16kHz")
    print(f"   - ç‰¹å¾æå–: ClapFeatureExtractor vs librosa")
    print(f"   - ç»´åº¦æ ¼å¼: [batch, channel, time, feature] vs [batch, channel, feature, time]")
    print(f"   - é¢‘ç‡èŒƒå›´: 50-14000Hz vs å…¨é¢‘æ®µ")
    
    return {
        'snr': snr,
        'mse': mse,
        'correlation': correlation,
        'vocoder_method': vocoder_method,
        'original_path': str(original_path),
        'reconstructed_path': str(reconstructed_path),
        'use_clap_features': use_clap_features
    }


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        audio_files = list(Path('.').glob('*.wav'))
        if audio_files:
            print("æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files[:3], 1):
                print(f"{i}. {file.name}")
            
            try:
                choice = int(input("é€‰æ‹©æ–‡ä»¶: "))
                audio_path = str(audio_files[choice-1])
            except:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    print(f"ğŸš€ å¼€å§‹AudioLDM2æœ€ç»ˆä¿®å¤æµ‹è¯•ï¼ˆClapFeatureExtractorï¼‰")
    
    try:
        result = test_audioldm2_ultimate_fix(audio_path)
        
        print(f"\\nğŸ“‹ æœ€ç»ˆä¿®å¤æ€»ç»“:")
        print(f"   æ–¹æ³•: {result['vocoder_method']}")
        print(f"   SNR: {result['snr']:.2f}dB")
        print(f"   MSE: {result['mse']:.6f}")
        print(f"   ç›¸å…³æ€§: {result['correlation']:.4f}")
        print(f"   ä½¿ç”¨ClapFeatureExtractor: {result['use_clap_features']}")
        
        if result['use_clap_features'] and result['snr'] > -5:
            print(f"\\nğŸŠ é‡å¤§æˆåŠŸï¼AudioLDM2çš„æ­£ç¡®ä½¿ç”¨æ–¹å¼")
            print(f"ğŸ”‘ å…³é”®ï¼šClapFeatureExtractor + 48kHz + æ­£ç¡®ç»´åº¦")
            print(f"ğŸ“ˆ å£°éŸ³åº”è¯¥æ›´é¥±æ»¡ï¼Œç»†èŠ‚æ›´ä¸°å¯Œï¼Œä¸å†å£°éŸ³å°")
        elif result['snr'] > -10:
            print(f"\\nâœ… æ˜¾è‘—æ”¹å–„ï¼æ¯”ä¼ ç»Ÿæ–¹æ³•å¥½å¾—å¤š")
        else:
            print(f"\\nğŸ” ç»§ç»­ä¼˜åŒ–ä¸­ï¼Œä½†æ–¹å‘æ˜¯æ­£ç¡®çš„")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
