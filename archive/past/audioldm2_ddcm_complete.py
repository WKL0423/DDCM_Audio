#!/usr/bin/env python3
"""
AudioLDM2 DDCM (Denoising Diffusion Codebook Model) å®Œæ•´å®ç°
åŸºäº DDCM è®ºæ–‡ï¼šç”¨é¢„å®šä¹‰ç æœ¬æ›¿æ¢éšæœºå™ªå£°çš„diffusionè¿‡ç¨‹
æ ¸å¿ƒæ€æƒ³ï¼šä½¿ç”¨ç æœ¬å™ªå£°å‘é‡æ›¿ä»£éšæœºé«˜æ–¯å™ªå£°ï¼Œå®ç°é«˜è´¨é‡å‹ç¼©ç”Ÿæˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from diffusers import AudioLDM2Pipeline, DDPMScheduler
import torchaudio
from pathlib import Path
import json
import time
import soundfile as sf
import warnings
warnings.filterwarnings("ignore")

class DDCMCodebook(nn.Module):
    """
    DDCMç æœ¬ï¼šä¸ºdiffusionè¿‡ç¨‹æä¾›é¢„å®šä¹‰çš„å™ªå£°å‘é‡
    æ›¿ä»£æ ‡å‡†diffusionä¸­çš„éšæœºé«˜æ–¯å™ªå£°
    """
    
    def __init__(self, 
                 codebook_size: int = 1024,
                 latent_shape: Tuple[int, int, int] = (8, 250, 16),
                 noise_schedule: str = "cosine"):
        """
        åˆå§‹åŒ–DDCMç æœ¬
        
        Args:
            codebook_size: ç æœ¬å¤§å°
            latent_shape: AudioLDM2 latentå½¢çŠ¶ (channels, height, width)
            noise_schedule: å™ªå£°è°ƒåº¦ç±»å‹
        """
        super().__init__()
        
        self.codebook_size = codebook_size
        self.latent_shape = latent_shape
        self.latent_dim = np.prod(latent_shape)
        
        # åˆ›å»ºç æœ¬ï¼š[codebook_size, channels, height, width]
        # ä½¿ç”¨æ ‡å‡†é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ï¼Œä½†å›ºå®šè¿™äº›å™ªå£°å‘é‡
        self.register_buffer("codebook", torch.randn(codebook_size, *latent_shape))
        
        # ä½¿ç”¨è®¡æ•°å™¨è¿½è¸ªä½¿ç”¨æƒ…å†µ
        self.register_buffer("usage_count", torch.zeros(codebook_size))
        
        print(f"ğŸ”§ DDCMç æœ¬åˆå§‹åŒ–:")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {codebook_size}")
        print(f"   ğŸ“ Latentå½¢çŠ¶: {latent_shape}")
        print(f"   ğŸ“Š æ€»å‚æ•°: {self.latent_dim * codebook_size:,}")
    
    def get_noise_for_timestep(self, batch_size: int, timestep: int, device: str = "cuda") -> torch.Tensor:
        """
        ä¸ºç‰¹å®šæ—¶é—´æ­¥è·å–ç æœ¬å™ªå£°å‘é‡
        DDCMæ ¸å¿ƒï¼šç”¨ç æœ¬å™ªå£°æ›¿ä»£éšæœºå™ªå£°
        
        Args:
            batch_size: æ‰¹å¤§å°
            timestep: å½“å‰æ—¶é—´æ­¥
            device: è®¾å¤‡
            
        Returns:
            noise: ç æœ¬å™ªå£°å‘é‡ [batch_size, channels, height, width]
        """
        # æ ¹æ®æ—¶é—´æ­¥é€‰æ‹©ç æœ¬ç´¢å¼•
        # å¯ä»¥ä½¿ç”¨ä¸åŒçš„é€‰æ‹©ç­–ç•¥
        indices = self._select_codebook_indices(batch_size, timestep)
        
        # è·å–å¯¹åº”çš„ç æœ¬å‘é‡
        noise = self.codebook[indices].to(device)
        
        # æ›´æ–°ä½¿ç”¨è®¡æ•°
        with torch.no_grad():
            for idx in indices:
                self.usage_count[idx] += 1
        
        return noise
    
    def _select_codebook_indices(self, batch_size: int, timestep: int) -> torch.Tensor:
        """
        é€‰æ‹©ç æœ¬ç´¢å¼•çš„ç­–ç•¥
        å¯ä»¥å®ç°å¤šç§é€‰æ‹©æ–¹æ³•ï¼šéšæœºã€åŸºäºæ—¶é—´æ­¥ã€åŸºäºå†…å®¹ç­‰
        """
        # ç­–ç•¥1: åŸºäºæ—¶é—´æ­¥çš„ç¡®å®šæ€§é€‰æ‹©
        base_idx = (timestep * 7) % self.codebook_size  # ä½¿ç”¨è´¨æ•°é¿å…å‘¨æœŸæ€§
        indices = [(base_idx + i) % self.codebook_size for i in range(batch_size)]
        
        # ç­–ç•¥2: åŠéšæœºé€‰æ‹©ï¼ˆå¯é€‰ï¼‰
        # åœ¨æ—¶é—´æ­¥é™„è¿‘é€‰æ‹©ï¼Œå¢åŠ ä¸€äº›éšæœºæ€§
        if hasattr(self, 'enable_random') and self.enable_random:
            random_offset = torch.randint(-5, 6, (batch_size,))
            indices = [(base_idx + offset.item()) % self.codebook_size 
                      for offset in random_offset]
        
        return torch.tensor(indices, dtype=torch.long)
    
    def find_best_noise_for_target(self, target_latent: torch.Tensor, timestep: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ä¸ºç›®æ ‡latentæ‰¾åˆ°æœ€ä½³çš„ç æœ¬å™ªå£°å‘é‡
        è¿™æ˜¯DDCMç”¨äºå‹ç¼©çš„æ ¸å¿ƒæ–¹æ³•
        
        Args:
            target_latent: ç›®æ ‡latent [batch, channels, height, width]
            timestep: æ—¶é—´æ­¥
            
        Returns:
            best_noise: æœ€ä½³å™ªå£°å‘é‡
            best_indices: æœ€ä½³ç æœ¬ç´¢å¼•
        """
        batch_size = target_latent.shape[0]
        
        # è®¡ç®—æ‰€æœ‰ç æœ¬å‘é‡ä¸ç›®æ ‡çš„è·ç¦»
        target_flat = target_latent.view(batch_size, -1)  # [batch, latent_dim]
        codebook_flat = self.codebook.view(self.codebook_size, -1)  # [codebook_size, latent_dim]
        
        # è®¡ç®—L2è·ç¦»
        distances = torch.cdist(target_flat, codebook_flat.to(target_latent.device))  # [batch, codebook_size]
        
        # é€‰æ‹©æœ€è¿‘çš„ç æœ¬å‘é‡
        best_indices = torch.argmin(distances, dim=1)  # [batch]
        best_noise = self.codebook[best_indices].to(target_latent.device)
        
        # æ›´æ–°ä½¿ç”¨è®¡æ•°
        with torch.no_grad():
            for idx in best_indices:
                self.usage_count[idx] += 1
        
        return best_noise, best_indices
    
    def get_usage_stats(self) -> Dict:
        """è·å–ç æœ¬ä½¿ç”¨ç»Ÿè®¡"""
        used_codes = (self.usage_count > 0).sum().item()
        total_usage = self.usage_count.sum().item()
        
        return {
            "used_codes": used_codes,
            "total_codes": self.codebook_size,
            "usage_rate": used_codes / self.codebook_size,
            "total_usage": total_usage,
            "avg_usage": total_usage / max(used_codes, 1),
            "most_used": self.usage_count.max().item(),
            "least_used": self.usage_count.min().item()
        }

class AudioLDM2_DDCM_Pipeline:
    """
    AudioLDM2 DDCMå®Œæ•´ç®¡é“
    é›†æˆç æœ¬åŒ–æ‰©æ•£æ¨¡å‹åˆ°AudioLDM2çš„ç”Ÿæˆè¿‡ç¨‹
    """
    
    def __init__(self, 
                 model_name: str = "cvssp/audioldm2-music",
                 codebook_size: int = 512,
                 enable_compression: bool = True):
        """
        åˆå§‹åŒ–AudioLDM2 DDCMç®¡é“
        
        Args:
            model_name: AudioLDM2æ¨¡å‹åç§°
            codebook_size: DDCMç æœ¬å¤§å°
            enable_compression: æ˜¯å¦å¯ç”¨å‹ç¼©æ¨¡å¼
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.enable_compression = enable_compression
        
        print(f"ğŸµ åˆå§‹åŒ–AudioLDM2 DDCMç®¡é“")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {codebook_size}")
        print(f"   ğŸ—œï¸ å‹ç¼©æ¨¡å¼: {enable_compression}")
        
        # åŠ è½½åŸºç¡€AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        # è·å–latentç»´åº¦
        self.latent_shape = (8, 250, 16)  # AudioLDM2æ ‡å‡†å½¢çŠ¶
        
        # åˆ›å»ºDDCMç æœ¬
        self.ddcm_codebook = DDCMCodebook(
            codebook_size=codebook_size,
            latent_shape=self.latent_shape
        ).to(self.device)
        
        # åˆ›å»ºæ”¹è¿›çš„è°ƒåº¦å™¨
        self.scheduler = DDPMScheduler.from_pretrained(model_name, subfolder="scheduler")
        
        # å‹ç¼©å†å²
        self.compression_results = []
        
        print(f"âœ… AudioLDM2 DDCMç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    def generate_with_ddcm(self, 
                          prompt: str,
                          audio_length_in_s: float = 10.0,
                          num_inference_steps: int = 20,
                          guidance_scale: float = 7.5,
                          use_codebook_noise: bool = True) -> Dict:
        """
        ä½¿ç”¨DDCMç”ŸæˆéŸ³é¢‘
        
        Args:
            prompt: æ–‡æœ¬æç¤º
            audio_length_in_s: éŸ³é¢‘é•¿åº¦ï¼ˆç§’ï¼‰
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            use_codebook_noise: æ˜¯å¦ä½¿ç”¨ç æœ¬å™ªå£°
            
        Returns:
            result: ç”Ÿæˆç»“æœå­—å…¸
        """
        print(f"ğŸµ DDCMç”ŸæˆéŸ³é¢‘")
        print(f"   ğŸ“ æç¤º: {prompt}")
        print(f"   â±ï¸ æ—¶é•¿: {audio_length_in_s}s")
        print(f"   ğŸ”„ æ­¥æ•°: {num_inference_steps}")
        print(f"   ğŸ“š ä½¿ç”¨ç æœ¬å™ªå£°: {use_codebook_noise}")
        
        start_time = time.time()
        
        if use_codebook_noise:
            # DDCMæ¨¡å¼ï¼šä½¿ç”¨ç æœ¬å™ªå£°
            audio = self._generate_with_codebook_noise(
                prompt, audio_length_in_s, num_inference_steps, guidance_scale
            )
            method = "DDCM_Codebook"
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šä½¿ç”¨éšæœºå™ªå£°
            result = self.pipeline(
                prompt=prompt,
                audio_length_in_s=audio_length_in_s,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale
            )
            audio = result.audios[0]
            method = "Standard_Diffusion"
        
        generation_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        output_dir = Path("ddcm_output")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = int(time.time())
        output_file = output_dir / f"ddcm_{method}_{timestamp}.wav"
        
        # ç¡®ä¿éŸ³é¢‘æ ¼å¼æ­£ç¡®
        if isinstance(audio, torch.Tensor):
            audio_np = audio.cpu().numpy()
        else:
            audio_np = np.array(audio)
        
        if audio_np.ndim > 1:
            audio_np = audio_np.squeeze()
        
        # å½’ä¸€åŒ–
        if audio_np.max() > 1.0 or audio_np.min() < -1.0:
            audio_np = audio_np / np.max(np.abs(audio_np))
        
        sf.write(str(output_file), audio_np, 16000)
        
        result = {
            "prompt": prompt,
            "method": method,
            "audio_length": audio_length_in_s,
            "generation_time": generation_time,
            "output_file": str(output_file),
            "audio_data": audio_np,
            "sample_rate": 16000
        }
        
        if use_codebook_noise:
            # æ·»åŠ ç æœ¬ä½¿ç”¨ç»Ÿè®¡
            result["codebook_stats"] = self.ddcm_codebook.get_usage_stats()
          print(f"   âœ… ç”Ÿæˆå®Œæˆ: {output_file}")
        print(f"   â±ï¸ ç”¨æ—¶: {generation_time:.2f}ç§’")
        
        return result
    
    def _generate_with_codebook_noise(self, 
                                     prompt: str,
                                     audio_length_in_s: float,
                                     num_inference_steps: int,
                                     guidance_scale: float) -> np.ndarray:
        """
        ä½¿ç”¨ç æœ¬å™ªå£°è¿›è¡ŒDDCMç”Ÿæˆ
        è¿™æ˜¯DDCMçš„æ ¸å¿ƒå®ç°
        """
        # ç¼–ç æ–‡æœ¬æç¤º - ä½¿ç”¨AudioLDM2çš„æ­£ç¡®æ–¹æ³•
        with torch.no_grad():
            # ä½¿ç”¨AudioLDM2ç®¡é“çš„å†…ç½®ç¼–ç æ–¹æ³•
            prompt_embeds, negative_prompt_embeds = self.pipeline.encode_prompt(
                prompt=prompt,
                device=self.device,
                num_images_per_prompt=1,
                do_classifier_free_guidance=True,
                negative_prompt=None
            )
            
            # æ‹¼æ¥æ¡ä»¶å’Œæ— æ¡ä»¶åµŒå…¥
            text_embeddings = torch.cat([negative_prompt_embeds, prompt_embeds])
        
        # è®¾ç½®è°ƒåº¦å™¨
        self.scheduler.set_timesteps(num_inference_steps)
        timesteps = self.scheduler.timesteps
        
        # åˆå§‹åŒ–latentï¼šä½¿ç”¨ç æœ¬å™ªå£°ï¼
        batch_size = 1
        latents_shape = (batch_size, *self.latent_shape)
        
        # DDCMå…³é”®ï¼šä½¿ç”¨ç æœ¬å™ªå£°è€Œä¸æ˜¯éšæœºå™ªå£°
        latents = self.ddcm_codebook.get_noise_for_timestep(
            batch_size, timesteps[0].item(), self.device
        )
        
        # è°ƒåº¦å™¨ç¼©æ”¾
        latents = latents * self.scheduler.init_noise_sigma
        
        print(f"   ğŸ”§ DDCMå»å™ªè¿‡ç¨‹å¼€å§‹...")
        
        # DDCMå»å™ªå¾ªç¯
        for i, t in enumerate(timesteps):
            print(f"     æ­¥éª¤ {i+1}/{len(timesteps)}, t={t}", end="\r")
            
            # æ‰©å±•latentsç”¨äºclassifier-free guidance
            latent_model_input = torch.cat([latents] * 2)
            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)
            
            # é¢„æµ‹å™ªå£°
            with torch.no_grad():
                noise_pred = self.pipeline.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=text_embeddings,
                ).sample
            
            # Classifier-free guidance
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
            
            # DDCMç‰¹è‰²ï¼šåœ¨æŸäº›æ­¥éª¤æ³¨å…¥ç æœ¬å™ªå£°
            if i < len(timesteps) // 4:  # åœ¨å‰25%æ­¥éª¤ä¸­ä½¿ç”¨ç æœ¬å™ªå£°
                # è·å–æ–°çš„ç æœ¬å™ªå£°
                codebook_noise = self.ddcm_codebook.get_noise_for_timestep(
                    batch_size, t.item(), self.device
                )
                # æ··åˆé¢„æµ‹å™ªå£°å’Œç æœ¬å™ªå£°
                noise_pred = 0.9 * noise_pred + 0.1 * codebook_noise
            
            # è°ƒåº¦å™¨æ­¥éª¤
            latents = self.scheduler.step(noise_pred, t, latents).prev_sample
        
        print(f"\n   ğŸµ VAEè§£ç ...")
        
        # VAEè§£ç 
        with torch.no_grad():
            latents = latents / self.pipeline.vae.config.scaling_factor
            mel_spectrogram = self.pipeline.vae.decode(latents).sample
            
            # Vocoder
            audio = self.pipeline.mel_spectrogram_to_waveform(mel_spectrogram)
            audio = audio.squeeze().cpu().numpy()
        
        return audio
    
    def compress_audio_with_ddcm(self, audio_path: str) -> Dict:
        """
        ä½¿ç”¨DDCMå‹ç¼©éŸ³é¢‘
        æ‰¾åˆ°æœ€ä½³çš„ç æœ¬è¡¨ç¤º
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            
        Returns:
            compression_result: å‹ç¼©ç»“æœ
        """
        print(f"ğŸ—œï¸ DDCMéŸ³é¢‘å‹ç¼©: {Path(audio_path).name}")
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = torchaudio.load(audio_path)
        if sr != 48000:
            resampler = torchaudio.transforms.Resample(sr, 48000)
            audio = resampler(audio)
        
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # é™åˆ¶é•¿åº¦
        max_length = 48000 * 10  # 10ç§’
        if audio.shape[-1] > max_length:
            audio = audio[..., :max_length]
        
        print(f"   ğŸ“Š éŸ³é¢‘: {audio.shape}, 48kHz")
        
        # ç¼–ç ä¸ºlatent
        with torch.no_grad():
            audio_np = audio.squeeze(0).numpy()
            
            # ä½¿ç”¨ClapFeatureExtractor
            inputs = self.pipeline.feature_extractor(
                audio_np,
                sampling_rate=48000,
                return_tensors="pt"
            )
            
            mel_features = inputs["input_features"].to(self.device)
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            # VAEç¼–ç 
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode()
            latent = latent * self.pipeline.vae.config.scaling_factor
        
        print(f"   ğŸ§  Latent: {latent.shape}")
        
        # æ‰¾åˆ°æœ€ä½³ç æœ¬è¡¨ç¤º
        best_noise, best_indices = self.ddcm_codebook.find_best_noise_for_target(latent, timestep=0)
        
        # è®¡ç®—å‹ç¼©æ•ˆæœ
        original_size = latent.numel() * 4  # float32å­—èŠ‚
        compressed_size = len(best_indices) * 4  # int32å­—èŠ‚
        compression_ratio = original_size / compressed_size
        
        # æµ‹è¯•é‡å»ºè´¨é‡
        with torch.no_grad():
            # ä½¿ç”¨ç æœ¬å‘é‡é‡å»º
            reconstructed_latent = best_noise / self.pipeline.vae.config.scaling_factor
            reconstructed_mel = self.pipeline.vae.decode(reconstructed_latent).sample
            reconstructed_audio = self.pipeline.mel_spectrogram_to_waveform(reconstructed_mel)
            reconstructed_audio = reconstructed_audio.squeeze().cpu().numpy()
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        original_audio = audio.squeeze().numpy()
        if len(reconstructed_audio) > len(original_audio):
            reconstructed_audio = reconstructed_audio[:len(original_audio)]
        elif len(reconstructed_audio) < len(original_audio):
            reconstructed_audio = np.pad(
                reconstructed_audio, 
                (0, len(original_audio) - len(reconstructed_audio))
            )
        
        mse = np.mean((original_audio - reconstructed_audio) ** 2)
        snr = 10 * np.log10(np.mean(original_audio ** 2) / (mse + 1e-10))
        correlation = np.corrcoef(original_audio, reconstructed_audio)[0, 1]
        
        result = {
            "input_file": audio_path,
            "codebook_indices": best_indices.cpu().tolist(),
            "compression_ratio": compression_ratio,
            "original_size_bytes": original_size,
            "compressed_size_bytes": compressed_size,
            "reconstruction_snr": snr,
            "reconstruction_correlation": correlation,
            "reconstruction_mse": mse
        }
        
        self.compression_results.append(result)
        
        print(f"   âœ… å‹ç¼©å®Œæˆ")
        print(f"   ğŸ“Š å‹ç¼©æ¯”: {compression_ratio:.2f}:1")
        print(f"   ğŸ“Š é‡å»ºSNR: {snr:.2f} dB")
        print(f"   ğŸ“Š ç›¸å…³æ€§: {correlation:.4f}")
        
        return result

def demo_ddcm_complete():
    """å®Œæ•´çš„DDCMæ¼”ç¤º"""
    print("ğŸ¯ AudioLDM2 DDCM å®Œæ•´æ¼”ç¤º")
    print("=" * 50)
    
    # åˆå§‹åŒ–DDCMç®¡é“
    ddcm_pipeline = AudioLDM2_DDCM_Pipeline(
        model_name="cvssp/audioldm2-music",
        codebook_size=256,
        enable_compression=True
    )
    
    # 1. æ ‡å‡†diffusionç”Ÿæˆ
    print("\nğŸµ ç¬¬1æ­¥: æ ‡å‡†Diffusionç”Ÿæˆ")
    standard_result = ddcm_pipeline.generate_with_ddcm(
        prompt="ambient electronic music with soft synths",
        audio_length_in_s=10.0,
        num_inference_steps=20,
        use_codebook_noise=False
    )
    
    # 2. DDCMç”Ÿæˆ
    print("\nğŸµ ç¬¬2æ­¥: DDCMç”Ÿæˆ")
    ddcm_result = ddcm_pipeline.generate_with_ddcm(
        prompt="ambient electronic music with soft synths",
        audio_length_in_s=10.0,
        num_inference_steps=20,
        use_codebook_noise=True
    )
    
    # 3. å¦‚æœæœ‰AudioLDM2_Music_output.wavï¼Œè¿›è¡Œå‹ç¼©æµ‹è¯•
    input_file = "AudioLDM2_Music_output.wav"
    if Path(input_file).exists():
        print("\nğŸ—œï¸ ç¬¬3æ­¥: DDCMå‹ç¼©æµ‹è¯•")
        compression_result = ddcm_pipeline.compress_audio_with_ddcm(input_file)
    
    # 4. æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    print("\nğŸ“Š ç»“æœå¯¹æ¯”")
    print("-" * 50)
    print(f"æ ‡å‡†ç”Ÿæˆç”¨æ—¶: {standard_result['generation_time']:.2f}ç§’")
    print(f"DDCMç”Ÿæˆç”¨æ—¶: {ddcm_result['generation_time']:.2f}ç§’")
    print(f"ç æœ¬ä½¿ç”¨ç»Ÿè®¡: {ddcm_result.get('codebook_stats', {})}")
    
    if Path(input_file).exists():
        print(f"å‹ç¼©æ¯”: {compression_result['compression_ratio']:.2f}:1")
        print(f"é‡å»ºè´¨é‡SNR: {compression_result['reconstruction_snr']:.2f} dB")
    
    print(f"\nâœ… DDCMå®Œæ•´æ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   æ ‡å‡†: {standard_result['output_file']}")
    print(f"   DDCM: {ddcm_result['output_file']}")

if __name__ == "__main__":
    demo_ddcm_complete()
