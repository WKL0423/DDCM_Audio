"""
AudioLDM2 VAE + HiFiGAN ä¸“é¡¹ä¿®å¤
================================

é’ˆå¯¹HiFiGANç»´åº¦ä¸åŒ¹é…é—®é¢˜çš„ä¸“é—¨è§£å†³æ–¹æ¡ˆ
ç›®æ ‡: æˆåŠŸé›†æˆç¥ç»vocoderï¼Œçªç ´Griffin-Limçš„92%ä¿¡æ¯æŸå¤±ç“¶é¢ˆ
"""

import torch
import torchaudio
import librosa
import numpy as np
import time
from pathlib import Path
from typing import Dict, Optional
from diffusers import AudioLDM2Pipeline
from transformers import SpeechT5HifiGan
import scipy.io.wavfile
import warnings

warnings.filterwarnings("ignore")

class HiFiGANVAEFixer:
    """HiFiGAN + VAE ä¸“é¡¹ä¿®å¤å™¨"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ¯ HiFiGAN + VAE ä¸“é¡¹ä¿®å¤å™¨")
        print(f"ğŸ“± è®¾å¤‡: {self.device}")
        
        # åŠ è½½AudioLDM2
        print("ğŸ“¦ åŠ è½½AudioLDM2...")
        self.pipe = AudioLDM2Pipeline.from_pretrained(
            "cvssp/audioldm2-music",
            torch_dtype=torch.float32 if self.device.type == "cpu" else torch.float16
        ).to(self.device)
        
        self.vae = self.pipe.vae
        
        # åŠ è½½HiFiGAN
        print("ğŸ¤ åŠ è½½HiFiGAN...")
        self.hifigan = SpeechT5HifiGan.from_pretrained(
            "microsoft/speecht5_hifigan"
        ).to(self.device)
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_audio(self, audio_path: str, duration: float = 5.0) -> tuple:
        """åŠ è½½éŸ³é¢‘"""
        audio, sr = librosa.load(audio_path, sr=16000, duration=duration)
        print(f"ğŸ“Š éŸ³é¢‘: {len(audio)/sr:.2f}ç§’")
        return audio, sr
    
    def create_hifigan_compatible_mel(self, audio: np.ndarray, sr: int) -> torch.Tensor:
        """åˆ›å»ºHiFiGANå…¼å®¹çš„80ç»´melé¢‘è°±"""
        # HiFiGANä¸“ç”¨å‚æ•° (åŸºäºSpeechT5çš„æ ‡å‡†é…ç½®)
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sr,
            n_mels=80,  # HiFiGANæ ‡å‡†
            n_fft=1024,
            hop_length=256,
            win_length=1024,
            fmin=0,
            fmax=8000
        )
        
        # è½¬æ¢ä¸ºå¯¹æ•°å°ºåº¦
        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        # å½’ä¸€åŒ–åˆ°[-1, 1] (AudioLDM2 VAEæ ‡å‡†)
        mel_spec = 2.0 * (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min()) - 1.0
        
        # è½¬æ¢ä¸ºtensor
        mel_tensor = torch.from_numpy(mel_spec).float().unsqueeze(0).unsqueeze(0)
        print(f"ğŸ¼ åˆ›å»º80ç»´mel: {mel_tensor.shape}")
        
        return mel_tensor.to(self.device)
    
    def vae_process(self, mel_tensor: torch.Tensor) -> torch.Tensor:
        """VAEç¼–ç è§£ç """
        with torch.no_grad():
            # æ•°æ®ç±»å‹åŒ¹é…
            if next(self.vae.parameters()).dtype == torch.float16:
                mel_tensor = mel_tensor.half()
            
            # ç¡®ä¿å°ºå¯¸æ˜¯4çš„å€æ•° (VAEè¦æ±‚)
            if mel_tensor.shape[-1] % 4 != 0:
                pad_length = 4 - (mel_tensor.shape[-1] % 4)
                mel_tensor = torch.nn.functional.pad(mel_tensor, (0, pad_length))
            
            # VAEç¼–ç è§£ç 
            latent = self.vae.encode(mel_tensor).latent_dist.sample()
            reconstructed = self.vae.decode(latent).sample
            
            print(f"ğŸ”„ VAEé‡å»º: {mel_tensor.shape} â†’ {latent.shape} â†’ {reconstructed.shape}")
            return reconstructed.float()
    
    def hifigan_vocoder_fixed(self, mel_tensor: torch.Tensor) -> np.ndarray:
        """ä¿®å¤çš„HiFiGAN vocoderè°ƒç”¨"""
        with torch.no_grad():
            # ä»VAEè¾“å‡ºæ ¼å¼è½¬æ¢ä¸ºHiFiGANè¾“å…¥æ ¼å¼
            mel_data = mel_tensor.squeeze()  # [1, 1, 80, time] â†’ [80, time]
            
            if mel_data.dim() == 3:
                mel_data = mel_data.squeeze(0)  # [1, 80, time] â†’ [80, time]
            
            print(f"ğŸ”§ HiFiGANè¾“å…¥å‡†å¤‡: {mel_data.shape}")
            
            # ç¡®ä¿æ˜¯80ç»´
            if mel_data.shape[0] != 80:
                print(f"âš ï¸ ç»´åº¦è°ƒæ•´: {mel_data.shape[0]} â†’ 80")
                mel_data = torch.nn.functional.interpolate(
                    mel_data.unsqueeze(0).unsqueeze(0),
                    size=(80, mel_data.shape[1]),
                    mode='bilinear',
                    align_corners=False
                ).squeeze(0).squeeze(0)
            
            # åå½’ä¸€åŒ– (ä»[-1,1]æ¢å¤åˆ°melå°ºåº¦)
            mel_data = (mel_data + 1.0) / 2.0  # [0, 1]
            mel_data = mel_data * 80 - 80  # [-80, 0] dBèŒƒå›´
            
            # HiFiGANéœ€è¦batchç»´åº¦: [batch, mel_bins, time]
            mel_input = mel_data.unsqueeze(0)
            print(f"ğŸ¤ HiFiGANè¾“å…¥: {mel_input.shape}")
            
            # ä½¿ç”¨HiFiGANç”ŸæˆéŸ³é¢‘
            audio_tensor = self.hifigan(mel_input)
            audio = audio_tensor.squeeze().cpu().numpy()
            
            print(f"ğŸµ HiFiGANè¾“å‡º: {audio.shape}")
            return audio
    
    def griffin_lim_baseline(self, mel_tensor: torch.Tensor, sr: int) -> np.ndarray:
        """Griffin-LimåŸºçº¿å¯¹æ¯”"""
        mel_np = mel_tensor.squeeze().cpu().numpy()
        
        # åå½’ä¸€åŒ–
        mel_np = (mel_np + 1.0) / 2.0
        mel_np = mel_np * 80 - 80  # æ¢å¤dBèŒƒå›´
        mel_linear = librosa.db_to_power(mel_np)
        
        # Griffin-Limé‡å»º
        audio = librosa.feature.inverse.mel_to_audio(
            mel_linear,
            sr=sr,
            n_iter=64,
            hop_length=256,
            win_length=1024,
            n_fft=1024
        )
        
        return audio
    
    def calculate_metrics(self, original: np.ndarray, reconstructed: np.ndarray) -> Dict:
        """è®¡ç®—éŸ³é¢‘è´¨é‡æŒ‡æ ‡"""
        min_len = min(len(original), len(reconstructed))
        orig = original[:min_len]
        recon = reconstructed[:min_len]
        
        mse = np.mean((orig - recon) ** 2)
        snr = 10 * np.log10(np.mean(orig ** 2) / (mse + 1e-8))
        correlation = np.corrcoef(orig, recon)[0, 1] if len(orig) > 1 else 0.0
        
        return {
            'mse': float(mse),
            'snr': float(snr),
            'correlation': float(correlation if not np.isnan(correlation) else 0)
        }
    
    def comprehensive_comparison(self, audio_path: str) -> Dict:
        """å…¨é¢å¯¹æ¯”æµ‹è¯•"""
        print(\"\\nğŸ¯ HiFiGAN vs Griffin-Lim å…¨é¢å¯¹æ¯”æµ‹è¯•\")\n        print(\"=\"*60)\n        \n        # åˆ›å»ºè¾“å‡ºç›®å½•\n        output_dir = Path(\"hifigan_vae_fix_test\")\n        output_dir.mkdir(exist_ok=True)\n        \n        # åŠ è½½éŸ³é¢‘\n        audio, sr = self.load_audio(audio_path)\n        \n        # ä¿å­˜åŸå§‹éŸ³é¢‘\n        audio_name = Path(audio_path).stem\n        timestamp = int(time.time())\n        original_path = output_dir / f\"{audio_name}_original_{timestamp}.wav\"\n        scipy.io.wavfile.write(original_path, sr, (audio * 32767).astype(np.int16))\n        \n        results = {}\n        \n        print(\"\\nğŸ§ª å¼€å§‹æµ‹è¯•æµç¨‹...\")\n        \n        try:\n            # åˆ›å»º80ç»´mel (HiFiGANå…¼å®¹)\n            mel_tensor = self.create_hifigan_compatible_mel(audio, sr)\n            \n            # VAEé‡å»º\n            print(\"\\nğŸ”„ æ‰§è¡ŒVAEé‡å»º...\")\n            vae_reconstructed_mel = self.vae_process(mel_tensor)\n            \n            # æµ‹è¯•1: Griffin-Limé‡å»º\n            print(\"\\nğŸ“Š æµ‹è¯•1: Griffin-Limé‡å»º\")\n            try:\n                griffin_audio = self.griffin_lim_baseline(vae_reconstructed_mel, sr)\n                griffin_audio = griffin_audio / (np.max(np.abs(griffin_audio)) + 1e-8)\n                griffin_metrics = self.calculate_metrics(audio, griffin_audio)\n                \n                griffin_file = output_dir / f\"{audio_name}_griffin_lim_{timestamp}.wav\"\n                scipy.io.wavfile.write(griffin_file, sr, (griffin_audio * 32767).astype(np.int16))\n                \n                results['griffin_lim'] = {\n                    'metrics': griffin_metrics,\n                    'file': str(griffin_file)\n                }\n                \n                print(f\"   âœ… Griffin-Lim: SNR={griffin_metrics['snr']:.2f}dB, ç›¸å…³æ€§={griffin_metrics['correlation']:.4f}\")\n                \n            except Exception as e:\n                print(f\"   âŒ Griffin-Limå¤±è´¥: {e}\")\n            \n            # æµ‹è¯•2: HiFiGANé‡å»º\n            print(\"\\nğŸ¤ æµ‹è¯•2: HiFiGANé‡å»º\")\n            try:\n                hifigan_audio = self.hifigan_vocoder_fixed(vae_reconstructed_mel)\n                hifigan_audio = hifigan_audio / (np.max(np.abs(hifigan_audio)) + 1e-8)\n                hifigan_metrics = self.calculate_metrics(audio, hifigan_audio)\n                \n                hifigan_file = output_dir / f\"{audio_name}_hifigan_{timestamp}.wav\"\n                scipy.io.wavfile.write(hifigan_file, sr, (hifigan_audio * 32767).astype(np.int16))\n                \n                results['hifigan'] = {\n                    'metrics': hifigan_metrics,\n                    'file': str(hifigan_file)\n                }\n                \n                print(f\"   âœ… HiFiGAN: SNR={hifigan_metrics['snr']:.2f}dB, ç›¸å…³æ€§={hifigan_metrics['correlation']:.4f}\")\n                \n            except Exception as e:\n                print(f\"   âŒ HiFiGANå¤±è´¥: {e}\")\n            \n        except Exception as e:\n            print(f\"âŒ æ•´ä½“æµç¨‹å¤±è´¥: {e}\")\n        \n        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š\n        self._generate_comparison_report(results, output_dir)\n        \n        return results\n    \n    def _generate_comparison_report(self, results: Dict, output_dir: Path):\n        \"\"\"ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"ğŸ¯ HiFiGAN vs Griffin-Lim å¯¹æ¯”ç»“æœ\")\n        print(\"=\"*60)\n        \n        if not results:\n            print(\"âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ\")\n            return\n        \n        # åŸºçº¿Griffin-Limç»“æœ\n        if 'griffin_lim' in results:\n            gl_metrics = results['griffin_lim']['metrics']\n            print(f\"\\nğŸ“Š Griffin-Lim (åŸºçº¿):\")\n            print(f\"   ğŸ“ˆ SNR: {gl_metrics['snr']:.2f}dB\")\n            print(f\"   ğŸ”— ç›¸å…³æ€§: {gl_metrics['correlation']:.4f}\")\n            print(f\"   ğŸ“ æ–‡ä»¶: {Path(results['griffin_lim']['file']).name}\")\n        \n        # HiFiGANç»“æœ\n        if 'hifigan' in results:\n            hg_metrics = results['hifigan']['metrics']\n            print(f\"\\nğŸ¤ HiFiGAN (ç¥ç»vocoder):\")\n            print(f\"   ğŸ“ˆ SNR: {hg_metrics['snr']:.2f}dB\")\n            print(f\"   ğŸ”— ç›¸å…³æ€§: {hg_metrics['correlation']:.4f}\")\n            print(f\"   ğŸ“ æ–‡ä»¶: {Path(results['hifigan']['file']).name}\")\n            \n            # å¯¹æ¯”åˆ†æ\n            if 'griffin_lim' in results:\n                snr_improvement = hg_metrics['snr'] - gl_metrics['snr']\n                corr_improvement = hg_metrics['correlation'] - gl_metrics['correlation']\n                \n                print(f\"\\nğŸš€ HiFiGAN vs Griffin-Lim æ”¹è¿›:\")\n                print(f\"   ğŸ“ˆ SNRæ”¹è¿›: {snr_improvement:+.2f}dB\")\n                print(f\"   ğŸ”— ç›¸å…³æ€§æ”¹è¿›: {corr_improvement:+.4f}\")\n                \n                if snr_improvement > 5:\n                    print(\"   âœ… æ˜¾è‘—æ”¹å–„ï¼ç¥ç»vocoderæ•ˆæœæ˜æ˜¾\")\n                elif snr_improvement > 1:\n                    print(\"   âœ… æœ‰æ‰€æ”¹å–„ï¼Œç¥ç»vocoderæœ‰æ•ˆ\")\n                else:\n                    print(\"   âš ï¸ æ”¹å–„æœ‰é™ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–\")\n        \n        print(f\"\\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}\")\n        print(\"ğŸ§ å»ºè®®æ’­æ”¾å¯¹æ¯”éŸ³é¢‘è¿›è¡Œä¸»è§‚è¯„ä¼°\")\n        \n        # ä¸‹ä¸€æ­¥å»ºè®®\n        print(f\"\\nğŸ’¡ ä¼˜åŒ–å»ºè®®:\")\n        if 'hifigan' in results and results['hifigan']['metrics']['snr'] > -5:\n            print(\"   âœ… HiFiGANé›†æˆæˆåŠŸï¼Œå¯æ¢ç´¢æ›´å¤šç¥ç»vocoder\")\n        else:\n            print(\"   âš ï¸ å°è¯•å…¶ä»–ç¥ç»vocoderæ¨¡å‹\")\n        print(\"   ğŸ“ˆ è€ƒè™‘ç«¯åˆ°ç«¯é‡å»ºæ¨¡å‹\")\n        print(\"   ğŸ”§ ä¼˜åŒ–VAEå‹ç¼©å‚æ•°\")\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"ä½¿ç”¨æ–¹æ³•: python hifigan_vae_fixer.py <éŸ³é¢‘æ–‡ä»¶>\")\n        \n        # åˆ—å‡ºéŸ³é¢‘æ–‡ä»¶\n        audio_files = list(Path('.').glob('*.wav'))\n        if audio_files:\n            print(\"æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:\")\n            for i, file in enumerate(audio_files[:3], 1):\n                print(f\"{i}. {file.name}\")\n            \n            try:\n                choice = int(input(\"é€‰æ‹©æ–‡ä»¶: \"))\n                audio_path = str(audio_files[choice-1])\n            except:\n                print(\"âŒ æ— æ•ˆé€‰æ‹©\")\n                return\n        else:\n            print(\"âŒ æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶\")\n            return\n    else:\n        audio_path = sys.argv[1]\n    \n    # è¿è¡Œä¿®å¤æµ‹è¯•\n    fixer = HiFiGANVAEFixer()\n    results = fixer.comprehensive_comparison(audio_path)\n    \n    print(\"\\nâœ… HiFiGANä¸“é¡¹ä¿®å¤æµ‹è¯•å®Œæˆ!\")\n\nif __name__ == \"__main__\":\n    main()"
