"""
AudioLDM2 VAE æ”¹è¿›ç‰ˆéŸ³é¢‘é‡å»ºæµ‹è¯•è„šæœ¬
ä½¿ç”¨æ›´å¥½çš„æ–¹æ³•æé«˜é‡å»ºè´¨é‡

æ”¹è¿›ç‚¹ï¼š
1. ä½¿ç”¨AudioLDM2çš„vocoderè¿›è¡Œæ›´å¥½çš„éŸ³é¢‘é‡å»º
2. ä¼˜åŒ–mel-spectrogramå‚æ•°
3. æ·»åŠ æ›´å¤šè´¨é‡è¯„ä¼°æŒ‡æ ‡
4. æä¾›å¤šç§é‡å»ºæ–¹æ³•æ¯”è¾ƒ
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F
from scipy.signal import correlate
import matplotlib.pyplot as plt

from diffusers import AudioLDM2Pipeline


def improved_vae_test(audio_path, model_id="cvssp/audioldm2-music", max_length=10):
    """
    æ”¹è¿›çš„VAEéŸ³é¢‘é‡å»ºæµ‹è¯•
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    if not os.path.exists(audio_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ {audio_path}")
        return
    
    print(f"æ­£åœ¨åŠ è½½ AudioLDM2 æ¨¡å‹: {model_id}")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    print(f"æ­£åœ¨åŠ è½½éŸ³é¢‘: {audio_path}")
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    
    if len(audio) > max_length * sample_rate:
        audio = audio[:int(max_length * sample_rate)]
        print(f"éŸ³é¢‘å·²è£å‰ªåˆ° {max_length} ç§’")
    
    print(f"éŸ³é¢‘ä¿¡æ¯: é•¿åº¦={len(audio)/sample_rate:.2f}ç§’, æ ·æœ¬æ•°={len(audio)}")
    
    # æ–¹æ³•1: ä½¿ç”¨AudioLDM2çš„å®˜æ–¹mel-spectrogramå‚æ•°
    print("\n=== æ–¹æ³•1: ä½¿ç”¨AudioLDM2å®˜æ–¹å‚æ•° ===")
    result1 = test_with_official_params(audio, vae, vocoder, device, sample_rate)
    
    # æ–¹æ³•2: ä½¿ç”¨ä¼˜åŒ–çš„mel-spectrogramå‚æ•°
    print("\n=== æ–¹æ³•2: ä½¿ç”¨ä¼˜åŒ–å‚æ•° ===")
    result2 = test_with_optimized_params(audio, vae, vocoder, device, sample_rate)
    
    # æ–¹æ³•3: å°è¯•ç›´æ¥ä½¿ç”¨vocoderï¼ˆå¦‚æœå¯èƒ½ï¼‰
    print("\n=== æ–¹æ³•3: å°è¯•vocoderé‡å»º ===")
    result3 = test_with_vocoder_direct(audio, vae, vocoder, device, sample_rate)
    
    # ä¿å­˜æ‰€æœ‰ç»“æœ
    output_dir = "vae_improved_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    audio_save = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio
    torchaudio.save(original_path, torch.from_numpy(audio_save).unsqueeze(0), sample_rate)
    
    # ä¿å­˜é‡å»ºç»“æœ
    results = {}
    for i, (method_name, result) in enumerate([
        ("å®˜æ–¹å‚æ•°", result1),
        ("ä¼˜åŒ–å‚æ•°", result2), 
        ("vocoderç›´æ¥", result3)
    ], 1):
        if result is not None:
            recon_path = os.path.join(output_dir, f"{input_name}_method{i}_{method_name}_{timestamp}.wav")
            
            # å½’ä¸€åŒ–å¹¶ä¿å­˜
            recon_audio = result['audio']
            if len(recon_audio) > 0 and np.max(np.abs(recon_audio)) > 0:
                recon_audio = recon_audio / np.max(np.abs(recon_audio))
            
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            if len(recon_audio) > len(audio):
                recon_audio = recon_audio[:len(audio)]
            elif len(recon_audio) < len(audio):
                recon_audio = np.pad(recon_audio, (0, len(audio) - len(recon_audio)))
            
            torchaudio.save(recon_path, torch.from_numpy(recon_audio).unsqueeze(0), sample_rate)
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            metrics = calculate_quality_metrics(audio, recon_audio)
            
            results[f"æ–¹æ³•{i}_{method_name}"] = {
                'path': recon_path,
                'metrics': metrics,
                'encode_time': result.get('encode_time', 0),
                'decode_time': result.get('decode_time', 0)
            }
    
    # æ‰“å°æ¯”è¾ƒç»“æœ
    print(f"\n{'='*60}")
    print(f"é‡å»ºè´¨é‡æ¯”è¾ƒç»“æœ")
    print(f"{'='*60}")
    print(f"åŸå§‹éŸ³é¢‘: {original_path}")
    print()
    
    for method_name, data in results.items():
        metrics = data['metrics']
        print(f"{method_name}:")
        print(f"  æ–‡ä»¶: {data['path']}")
        print(f"  SNR: {metrics['snr']:.2f} dB")
        print(f"  ç›¸å…³ç³»æ•°: {metrics['correlation']:.4f}")
        print(f"  é¢‘è°±ç›¸å…³æ€§: {metrics['spectral_correlation']:.4f}")
        print(f"  å¤„ç†æ—¶é—´: {data['encode_time'] + data['decode_time']:.2f}ç§’")
        print()
    
    # æ‰¾å‡ºæœ€å¥½çš„æ–¹æ³•
    best_method = max(results.items(), key=lambda x: x[1]['metrics']['snr'])
    print(f"ğŸ† æœ€ä½³é‡å»ºæ–¹æ³•: {best_method[0]} (SNR: {best_method[1]['metrics']['snr']:.2f} dB)")
    
    return results


def test_with_official_params(audio, vae, vocoder, device, sample_rate):
    """ä½¿ç”¨AudioLDM2å®˜æ–¹å‚æ•°"""
    try:
        # AudioLDM2å®˜æ–¹å‚æ•°
        n_fft = 1024
        hop_length = 160
        win_length = 1024
        n_mels = 64
        fmin = 0
        fmax = 8000
        
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            win_length=win_length,
            n_mels=n_mels,
            fmin=fmin,
            fmax=fmax,
            power=2.0
        )
        
        # è½¬æ¢ä¸ºå¯¹æ•°å°ºåº¦
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        # æ›´å¥½çš„å½’ä¸€åŒ–æ–¹æ³•
        mel_mean = mel_spec_db.mean()
        mel_std = mel_spec_db.std()
        mel_spec_normalized = (mel_spec_db - mel_mean) / (mel_std + 1e-8)
        mel_spec_normalized = np.clip(mel_spec_normalized, -5, 5)  # é™åˆ¶åœ¨åˆç†èŒƒå›´
        
        norm_params = {'mean': mel_mean, 'std': mel_std}
        
        return vae_encode_decode_with_vocoder(mel_spec_normalized, norm_params, vae, vocoder, device, 
                                            n_fft, hop_length, win_length, sample_rate, fmin, fmax)
    except Exception as e:
        print(f"å®˜æ–¹å‚æ•°æ–¹æ³•å¤±è´¥: {e}")
        return None


def test_with_optimized_params(audio, vae, vocoder, device, sample_rate):
    """ä½¿ç”¨ä¼˜åŒ–å‚æ•°"""
    try:
        # ä¼˜åŒ–å‚æ•° - æ›´é«˜è´¨é‡
        n_fft = 2048
        hop_length = 256
        win_length = 2048
        n_mels = 80
        fmin = 0
        fmax = 8000
        
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            win_length=win_length,
            n_mels=n_mels,
            fmin=fmin,
            fmax=fmax,
            power=2.0
        )
        
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        # ä½¿ç”¨åˆ†ä½æ•°å½’ä¸€åŒ–
        p5, p95 = np.percentile(mel_spec_db, [5, 95])
        mel_spec_normalized = 2 * (mel_spec_db - p5) / (p95 - p5) - 1
        mel_spec_normalized = np.clip(mel_spec_normalized, -1, 1)
        
        norm_params = {'p5': p5, 'p95': p95}
        
        # è°ƒæ•´åˆ°æ ‡å‡†å°ºå¯¸
        if n_mels != 64:
            # æ’å€¼åˆ°64ç»´
            from scipy.interpolate import interp1d
            old_indices = np.linspace(0, 1, n_mels)
            new_indices = np.linspace(0, 1, 64)
            interpolator = interp1d(old_indices, mel_spec_normalized, axis=0, kind='linear')
            mel_spec_normalized = interpolator(new_indices)
        
        return vae_encode_decode_with_vocoder(mel_spec_normalized, norm_params, vae, vocoder, device,
                                            1024, 160, 1024, sample_rate, fmin, fmax)
    except Exception as e:
        print(f"ä¼˜åŒ–å‚æ•°æ–¹æ³•å¤±è´¥: {e}")
        return None


def test_with_vocoder_direct(audio, vae, vocoder, device, sample_rate):
    """å°è¯•ç›´æ¥ä½¿ç”¨vocoder"""
    try:
        # å…ˆå°è¯•çœ‹çœ‹èƒ½å¦ç›´æ¥ç”¨vocoderçš„é€†è¿‡ç¨‹
        print("å°è¯•ä½¿ç”¨vocoderçš„melæå–åŠŸèƒ½...")
        
        # ä½¿ç”¨æ ‡å‡†å‚æ•°
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sample_rate,
            n_fft=1024,
            hop_length=160,
            n_mels=64,
            fmin=0,
            fmax=8000
        )
        
        mel_spec_db = librosa.power_to_db(mel_spec, ref=1.0)
        
        # å°è¯•ä½¿ç”¨AudioLDM2è®­ç»ƒæ—¶çš„å½’ä¸€åŒ–æ–¹å¼
        mel_spec_normalized = (mel_spec_db + 80) / 80  # å‡è®¾è®­ç»ƒæ—¶ä½¿ç”¨è¿™ç§å½’ä¸€åŒ–
        mel_spec_normalized = np.clip(mel_spec_normalized, 0, 1) * 2 - 1  # è½¬åˆ°[-1,1]
        
        norm_params = {'method': 'audioldm2_style'}
        
        return vae_encode_decode_improved(mel_spec_normalized, norm_params, vae, vocoder, device, sample_rate)
    except Exception as e:
        print(f"vocoderç›´æ¥æ–¹æ³•å¤±è´¥: {e}")
        return None


def vae_encode_decode_with_vocoder(mel_spec_normalized, norm_params, vae, vocoder, device, 
                                 n_fft, hop_length, win_length, sample_rate, fmin, fmax):
    """VAEç¼–ç è§£ç  + æ›´å¥½çš„éŸ³é¢‘é‡å»º"""
    start_time = time.time()
    
    # è½¬æ¢ä¸ºå¼ é‡
    mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.to(torch.float16)
    else:
        mel_tensor = mel_tensor.to(torch.float32)
    
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)
    
    # ç¡®ä¿å°ºå¯¸é€‚åˆVAE
    pad_width = (8 - (mel_input.shape[-1] % 8)) % 8
    if pad_width > 0:
        mel_input = F.pad(mel_input, (0, pad_width))
    
    with torch.no_grad():
        # VAEç¼–ç 
        latents = vae.encode(mel_input).latent_dist.sample()
        latents = latents * vae.config.scaling_factor
        
        encode_time = time.time() - start_time
        
        # VAEè§£ç 
        decode_start = time.time()
        latents = latents / vae.config.scaling_factor
        reconstructed_mel = vae.decode(latents).sample
        decode_time = time.time() - decode_start
        
        # åå½’ä¸€åŒ–
        recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
        
        # æ ¹æ®å½’ä¸€åŒ–æ–¹æ³•è¿›è¡Œåå½’ä¸€åŒ–
        if 'mean' in norm_params:
            recon_mel_denorm = recon_mel_np * norm_params['std'] + norm_params['mean']
        elif 'p5' in norm_params:
            recon_mel_denorm = (recon_mel_np + 1) / 2 * (norm_params['p95'] - norm_params['p5']) + norm_params['p5']
        else:
            recon_mel_denorm = recon_mel_np * 80 - 80
        
        # ä½¿ç”¨æ”¹è¿›çš„Griffin-Lim
        recon_mel_power = librosa.db_to_power(recon_mel_denorm)
        recon_mel_power = np.clip(recon_mel_power, 1e-10, None)
        
        # ä½¿ç”¨æ›´å¤šè¿­ä»£çš„Griffin-Lim
        reconstructed_audio = librosa.feature.inverse.mel_to_audio(
            recon_mel_power,
            sr=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            win_length=win_length,
            fmin=fmin,
            fmax=fmax,
            n_iter=60,  # å¢åŠ è¿­ä»£æ¬¡æ•°
            length=None
        )
        
        # åå¤„ç†ï¼šé™å™ª
        reconstructed_audio = apply_simple_denoising(reconstructed_audio)
    
    return {
        'audio': reconstructed_audio,
        'encode_time': encode_time,
        'decode_time': decode_time
    }


def vae_encode_decode_improved(mel_spec_normalized, norm_params, vae, vocoder, device, sample_rate):
    """æ”¹è¿›çš„VAEç¼–ç è§£ç """
    start_time = time.time()
    
    mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.to(torch.float16)
    else:
        mel_tensor = mel_tensor.to(torch.float32)
    
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)
    
    # ç¡®ä¿å°ºå¯¸é€‚åˆVAE
    pad_width = (8 - (mel_input.shape[-1] % 8)) % 8
    if pad_width > 0:
        mel_input = F.pad(mel_input, (0, pad_width))
    
    with torch.no_grad():
        # VAEç¼–ç 
        latents = vae.encode(mel_input).latent_dist.sample()
        latents = latents * vae.config.scaling_factor
        
        encode_time = time.time() - start_time
        
        # VAEè§£ç 
        decode_start = time.time()
        latents = latents / vae.config.scaling_factor
        reconstructed_mel = vae.decode(latents).sample
        decode_time = time.time() - decode_start
        
        # å°è¯•ç›´æ¥ä½¿ç”¨vocoderï¼ˆå¦‚æœå…¼å®¹ï¼‰
        try:
            # è°ƒæ•´å½¢çŠ¶ç»™vocoder
            vocoder_input = reconstructed_mel
            
            # æ£€æŸ¥vocoderæ˜¯å¦èƒ½ç›´æ¥å¤„ç†
            if hasattr(vocoder, 'decode') or hasattr(vocoder, '__call__'):
                # å°è¯•ä½¿ç”¨vocoder
                reconstructed_audio = vocoder(vocoder_input).squeeze().cpu().numpy()
                if len(reconstructed_audio.shape) > 1:
                    reconstructed_audio = reconstructed_audio[0]
                    
                print("æˆåŠŸä½¿ç”¨vocoderè¿›è¡ŒéŸ³é¢‘é‡å»ºï¼")
            else:
                # å›é€€åˆ°Griffin-Lim
                raise Exception("Vocoderä¸å…¼å®¹ï¼Œä½¿ç”¨Griffin-Lim")
                
        except Exception as e:
            print(f"Vocoderå¤±è´¥ï¼Œä½¿ç”¨Griffin-Lim: {e}")
            
            # åå½’ä¸€åŒ–å¹¶ä½¿ç”¨Griffin-Lim
            recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
            recon_mel_denorm = (recon_mel_np + 1) / 2 * 80 - 80
            recon_mel_power = librosa.db_to_power(recon_mel_denorm)
            recon_mel_power = np.clip(recon_mel_power, 1e-10, None)
            
            reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                recon_mel_power,
                sr=sample_rate,
                hop_length=160,
                n_fft=1024,
                n_iter=60
            )
    
    return {
        'audio': reconstructed_audio,
        'encode_time': encode_time,
        'decode_time': decode_time
    }


def apply_simple_denoising(audio, alpha=0.1):
    """ç®€å•çš„é™å™ªå¤„ç†"""
    try:
        # ä½¿ç”¨ä½é€šæ»¤æ³¢å‡å°‘é«˜é¢‘å™ªå£°
        from scipy.signal import butter, filtfilt
        
        nyquist = 8000  # sample_rate / 2
        cutoff = 7500   # å»é™¤7.5kHzä»¥ä¸Šçš„é¢‘ç‡
        order = 4
        
        b, a = butter(order, cutoff / nyquist, btype='low')
        filtered_audio = filtfilt(b, a, audio)
        
        # è½»å¾®çš„å¹³æ»‘
        smoothed_audio = alpha * filtered_audio + (1 - alpha) * audio
        
        return smoothed_audio
    except:
        return audio


def calculate_quality_metrics(original, reconstructed):
    """è®¡ç®—æ›´å…¨é¢çš„è´¨é‡æŒ‡æ ‡"""
    min_len = min(len(original), len(reconstructed))
    orig = original[:min_len]
    recon = reconstructed[:min_len]
    
    # åŸºæœ¬æŒ‡æ ‡
    mse = np.mean((orig - recon) ** 2)
    signal_power = np.mean(orig ** 2)
    noise_power = np.mean((orig - recon) ** 2)
    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
    correlation = np.corrcoef(orig, recon)[0, 1] if len(orig) > 1 else 0
    
    # é¢‘è°±ç›¸å…³æ€§
    orig_fft = np.abs(np.fft.fft(orig))
    recon_fft = np.abs(np.fft.fft(recon))
    spectral_correlation = np.corrcoef(orig_fft, recon_fft)[0, 1] if len(orig_fft) > 1 else 0
    
    # é¢‘è°±è´¨å¿ƒæ¯”è¾ƒ
    orig_centroid = librosa.feature.spectral_centroid(y=orig, sr=16000)[0].mean()
    recon_centroid = librosa.feature.spectral_centroid(y=recon, sr=16000)[0].mean()
    centroid_diff = abs(orig_centroid - recon_centroid) / orig_centroid
    
    return {
        'mse': mse,
        'snr': snr,
        'correlation': correlation,
        'spectral_correlation': spectral_correlation,
        'centroid_difference': centroid_diff
    }


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python improved_vae_test.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„> [æœ€å¤§é•¿åº¦ç§’æ•°]")
        
        # æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
        audio_files = []
        for ext in ['.wav', '.mp3', '.flac']:
            audio_files.extend(Path('.').glob(f'*{ext}'))
        
        if audio_files:
            print(f"\næ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files, 1):
                print(f"{i}. {file}")
            
            try:
                choice = input("è¯·é€‰æ‹©æ–‡ä»¶åºå·: ").strip()
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(audio_files):
                    audio_path = str(audio_files[file_idx])
                else:
                    print("æ— æ•ˆé€‰æ‹©")
                    return
            except (ValueError, KeyboardInterrupt):
                print("å–æ¶ˆæ“ä½œ")
                return
        else:
            print("å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    max_length = 10
    if len(sys.argv) >= 3:
        try:
            max_length = float(sys.argv[2])
        except ValueError:
            print(f"æ— æ•ˆçš„é•¿åº¦å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {max_length} ç§’")
    
    print(f"å¼€å§‹æ”¹è¿›ç‰ˆVAEæµ‹è¯•: {audio_path}")
    print(f"æœ€å¤§é•¿åº¦: {max_length} ç§’")
    
    try:
        results = improved_vae_test(audio_path, max_length=max_length)
        if results:
            print("\nâœ… æ”¹è¿›ç‰ˆæµ‹è¯•å®Œæˆï¼")
            print("è¯·æ’­æ”¾ä¸åŒæ–¹æ³•çš„é‡å»ºéŸ³é¢‘æ¥æ¯”è¾ƒè´¨é‡å·®å¼‚ã€‚")
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
