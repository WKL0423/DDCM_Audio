"""
AudioLDM2 VAE ÊîπËøõÁâàÈü≥È¢ëÈáçÂª∫ÊµãËØïËÑöÊú¨
‰ΩøÁî®Êõ¥Â•ΩÁöÑÊñπÊ≥ïÊèêÈ´òÈáçÂª∫Ë¥®Èáè

ÊîπËøõÁÇπÔºö
1. ‰ΩøÁî®AudioLDM2ÁöÑvocoderËøõË°åÊõ¥Â•ΩÁöÑÈü≥È¢ëÈáçÂª∫
2. ‰ºòÂåñmel-spectrogramÂèÇÊï∞
3. Ê∑ªÂä†Êõ¥Â§öË¥®ÈáèËØÑ‰º∞ÊåáÊ†á
4. Êèê‰æõÂ§öÁßçÈáçÂª∫ÊñπÊ≥ïÊØîËæÉ
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F
from scipy.signal import correlate
import matplotlib.pyplot as plt

from diffusers import AudioLDM2Pipeline


def improved_vae_test(audio_path, model_id="cvssp/audioldm2-music", max_length=10):
    """
    ÊîπËøõÁöÑVAEÈü≥È¢ëÈáçÂª∫ÊµãËØï
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"‰ΩøÁî®ËÆæÂ§á: {device}")
    
    if not os.path.exists(audio_path):
        print(f"ÈîôËØØ: Êâæ‰∏çÂà∞Èü≥È¢ëÊñá‰ª∂ {audio_path}")
        return
    
    print(f"Ê≠£Âú®Âä†ËΩΩ AudioLDM2 Ê®°Âûã: {model_id}")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    print(f"Ê≠£Âú®Âä†ËΩΩÈü≥È¢ë: {audio_path}")
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    
    if len(audio) > max_length * sample_rate:
        audio = audio[:int(max_length * sample_rate)]
        print(f"Èü≥È¢ëÂ∑≤Ë£ÅÂâ™Âà∞ {max_length} Áßí")
    
    print(f"Èü≥È¢ë‰ø°ÊÅØ: ÈïøÂ∫¶={len(audio)/sample_rate:.2f}Áßí, Ê†∑Êú¨Êï∞={len(audio)}")
    
    # ÊñπÊ≥ï1: ‰ΩøÁî®AudioLDM2ÁöÑÂÆòÊñπmel-spectrogramÂèÇÊï∞
    print("\n=== ÊñπÊ≥ï1: ‰ΩøÁî®AudioLDM2ÂÆòÊñπÂèÇÊï∞ ===")
    result1 = test_with_official_params(audio, vae, vocoder, device, sample_rate)
    
    # ÊñπÊ≥ï2: ‰ΩøÁî®‰ºòÂåñÁöÑmel-spectrogramÂèÇÊï∞
    print("\n=== ÊñπÊ≥ï2: ‰ΩøÁî®‰ºòÂåñÂèÇÊï∞ ===")
    result2 = test_with_optimized_params(audio, vae, vocoder, device, sample_rate)
    
    # ÊñπÊ≥ï3: Â∞ùËØïÁõ¥Êé•‰ΩøÁî®vocoderÔºàÂ¶ÇÊûúÂèØËÉΩÔºâ
    print("\n=== ÊñπÊ≥ï3: Â∞ùËØïvocoderÈáçÂª∫ ===")
    result3 = test_with_vocoder_direct(audio, vae, vocoder, device, sample_rate)
    
    # ‰øùÂ≠òÊâÄÊúâÁªìÊûú
    output_dir = "vae_improved_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ‰øùÂ≠òÂéüÂßãÈü≥È¢ë
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    audio_save = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio
    torchaudio.save(original_path, torch.from_numpy(audio_save).unsqueeze(0), sample_rate)
    
    # ‰øùÂ≠òÈáçÂª∫ÁªìÊûú
    results = {}
    for i, (method_name, result) in enumerate([
        ("ÂÆòÊñπÂèÇÊï∞", result1),
        ("‰ºòÂåñÂèÇÊï∞", result2), 
        ("vocoderÁõ¥Êé•", result3)
    ], 1):
        if result is not None:
            recon_path = os.path.join(output_dir, f"{input_name}_method{i}_{method_name}_{timestamp}.wav")
            
            # ÂΩí‰∏ÄÂåñÂπ∂‰øùÂ≠ò
            recon_audio = result['audio']
            if len(recon_audio) > 0 and np.max(np.abs(recon_audio)) > 0:
                recon_audio = recon_audio / np.max(np.abs(recon_audio))
            
            # Á°Æ‰øùÈïøÂ∫¶‰∏ÄËá¥
            if len(recon_audio) > len(audio):
                recon_audio = recon_audio[:len(audio)]
            elif len(recon_audio) < len(audio):
                recon_audio = np.pad(recon_audio, (0, len(audio) - len(recon_audio)))
            
            torchaudio.save(recon_path, torch.from_numpy(recon_audio).unsqueeze(0), sample_rate)
            
            # ËÆ°ÁÆóË¥®ÈáèÊåáÊ†á
            metrics = calculate_quality_metrics(audio, recon_audio)
            
            results[f"ÊñπÊ≥ï{i}_{method_name}"] = {
                'path': recon_path,
                'metrics': metrics,
                'encode_time': result.get('encode_time', 0),
                'decode_time': result.get('decode_time', 0)
            }
    
    # ÊâìÂç∞ÊØîËæÉÁªìÊûú
    print(f"\n{'='*60}")
    print(f"ÈáçÂª∫Ë¥®ÈáèÊØîËæÉÁªìÊûú")
    print(f"{'='*60}")
    print(f"ÂéüÂßãÈü≥È¢ë: {original_path}")
    print()
    
    for method_name, data in results.items():
        metrics = data['metrics']
        print(f"{method_name}:")
        print(f"  Êñá‰ª∂: {data['path']}")
        print(f"  SNR: {metrics['snr']:.2f} dB")
        print(f"  Áõ∏ÂÖ≥Á≥ªÊï∞: {metrics['correlation']:.4f}")
        print(f"  È¢ëË∞±Áõ∏ÂÖ≥ÊÄß: {metrics['spectral_correlation']:.4f}")
        print(f"  Â§ÑÁêÜÊó∂Èó¥: {data['encode_time'] + data['decode_time']:.2f}Áßí")
        print()
    
    # ÊâæÂá∫ÊúÄÂ•ΩÁöÑÊñπÊ≥ï
    best_method = max(results.items(), key=lambda x: x[1]['metrics']['snr'])
    print(f"üèÜ ÊúÄ‰Ω≥ÈáçÂª∫ÊñπÊ≥ï: {best_method[0]} (SNR: {best_method[1]['metrics']['snr']:.2f} dB)")
    
    return results


def test_with_official_params(audio, vae, vocoder, device, sample_rate):
    """‰ΩøÁî®AudioLDM2ÂÆòÊñπÂèÇÊï∞"""
    try:
        # AudioLDM2ÂÆòÊñπÂèÇÊï∞
        n_fft = 1024
        hop_length = 160
        win_length = 1024
        n_mels = 64
        fmin = 0
        fmax = 8000
        
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            win_length=win_length,
            n_mels=n_mels,
            fmin=fmin,
            fmax=fmax,
            power=2.0
        )
        
        # ËΩ¨Êç¢‰∏∫ÂØπÊï∞Â∞∫Â∫¶
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        # Êõ¥Â•ΩÁöÑÂΩí‰∏ÄÂåñÊñπÊ≥ï
        mel_mean = mel_spec_db.mean()
        mel_std = mel_spec_db.std()
        mel_spec_normalized = (mel_spec_db - mel_mean) / (mel_std + 1e-8)
        mel_spec_normalized = np.clip(mel_spec_normalized, -5, 5)  # ÈôêÂà∂Âú®ÂêàÁêÜËåÉÂõ¥
        
        norm_params = {'mean': mel_mean, 'std': mel_std}
        
        return vae_encode_decode_with_vocoder(mel_spec_normalized, norm_params, vae, vocoder, device, 
                                            n_fft, hop_length, win_length, sample_rate, fmin, fmax)
    except Exception as e:
        print(f"ÂÆòÊñπÂèÇÊï∞ÊñπÊ≥ïÂ§±Ë¥•: {e}")
        return None


def test_with_optimized_params(audio, vae, vocoder, device, sample_rate):
    """‰ΩøÁî®‰ºòÂåñÂèÇÊï∞"""
    try:
        # ‰ºòÂåñÂèÇÊï∞ - Êõ¥È´òË¥®Èáè
        n_fft = 2048
        hop_length = 256
        win_length = 2048
        n_mels = 80
        fmin = 0
        fmax = 8000
        
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            win_length=win_length,
            n_mels=n_mels,
            fmin=fmin,
            fmax=fmax,
            power=2.0
        )
        
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        # ‰ΩøÁî®ÂàÜ‰ΩçÊï∞ÂΩí‰∏ÄÂåñ
        p5, p95 = np.percentile(mel_spec_db, [5, 95])
        mel_spec_normalized = 2 * (mel_spec_db - p5) / (p95 - p5) - 1
        mel_spec_normalized = np.clip(mel_spec_normalized, -1, 1)
        
        norm_params = {'p5': p5, 'p95': p95}
        
        # Ë∞ÉÊï¥Âà∞Ê†áÂáÜÂ∞∫ÂØ∏
        if n_mels != 64:
            # ÊèíÂÄºÂà∞64Áª¥
            from scipy.interpolate import interp1d
            old_indices = np.linspace(0, 1, n_mels)
            new_indices = np.linspace(0, 1, 64)
            interpolator = interp1d(old_indices, mel_spec_normalized, axis=0, kind='linear')
            mel_spec_normalized = interpolator(new_indices)
        
        return vae_encode_decode_with_vocoder(mel_spec_normalized, norm_params, vae, vocoder, device,
                                            1024, 160, 1024, sample_rate, fmin, fmax)
    except Exception as e:
        print(f"‰ºòÂåñÂèÇÊï∞ÊñπÊ≥ïÂ§±Ë¥•: {e}")
        return None


def test_with_vocoder_direct(audio, vae, vocoder, device, sample_rate):
    """Â∞ùËØïÁõ¥Êé•‰ΩøÁî®vocoder"""
    try:
        # ÂÖàÂ∞ùËØïÁúãÁúãËÉΩÂê¶Áõ¥Êé•Áî®vocoderÁöÑÈÄÜËøáÁ®ã
        print("Â∞ùËØï‰ΩøÁî®vocoderÁöÑmelÊèêÂèñÂäüËÉΩ...")
        
        # ‰ΩøÁî®Ê†áÂáÜÂèÇÊï∞
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sample_rate,
            n_fft=1024,
            hop_length=160,
            n_mels=64,
            fmin=0,
            fmax=8000
        )
        
        mel_spec_db = librosa.power_to_db(mel_spec, ref=1.0)
        
        # Â∞ùËØï‰ΩøÁî®AudioLDM2ËÆ≠ÁªÉÊó∂ÁöÑÂΩí‰∏ÄÂåñÊñπÂºè
        mel_spec_normalized = (mel_spec_db + 80) / 80  # ÂÅáËÆæËÆ≠ÁªÉÊó∂‰ΩøÁî®ËøôÁßçÂΩí‰∏ÄÂåñ
        mel_spec_normalized = np.clip(mel_spec_normalized, 0, 1) * 2 - 1  # ËΩ¨Âà∞[-1,1]
        
        norm_params = {'method': 'audioldm2_style'}
        
        return vae_encode_decode_improved(mel_spec_normalized, norm_params, vae, vocoder, device, sample_rate)
    except Exception as e:
        print(f"vocoderÁõ¥Êé•ÊñπÊ≥ïÂ§±Ë¥•: {e}")
        return None


def vae_encode_decode_with_vocoder(mel_spec_normalized, norm_params, vae, vocoder, device, 
                                 n_fft, hop_length, win_length, sample_rate, fmin, fmax):
    """VAEÁºñÁ†ÅËß£Á†Å + Êõ¥Â•ΩÁöÑÈü≥È¢ëÈáçÂª∫"""
    start_time = time.time()
    
    # ËΩ¨Êç¢‰∏∫Âº†Èáè
    mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.to(torch.float16)
    else:
        mel_tensor = mel_tensor.to(torch.float32)
    
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)
    
    # Á°Æ‰øùÂ∞∫ÂØ∏ÈÄÇÂêàVAE
    pad_width = (8 - (mel_input.shape[-1] % 8)) % 8
    if pad_width > 0:
        mel_input = F.pad(mel_input, (0, pad_width))
    
    with torch.no_grad():
        # VAEÁºñÁ†Å
        latents = vae.encode(mel_input).latent_dist.sample()
        latents = latents * vae.config.scaling_factor
        
        encode_time = time.time() - start_time
        
        # VAEËß£Á†Å
        decode_start = time.time()
        latents = latents / vae.config.scaling_factor
        reconstructed_mel = vae.decode(latents).sample
        decode_time = time.time() - decode_start
        
        # ÂèçÂΩí‰∏ÄÂåñ
        recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
        
        # Ê†πÊçÆÂΩí‰∏ÄÂåñÊñπÊ≥ïËøõË°åÂèçÂΩí‰∏ÄÂåñ
        if 'mean' in norm_params:
            recon_mel_denorm = recon_mel_np * norm_params['std'] + norm_params['mean']
        elif 'p5' in norm_params:
            recon_mel_denorm = (recon_mel_np + 1) / 2 * (norm_params['p95'] - norm_params['p5']) + norm_params['p5']
        else:
            recon_mel_denorm = recon_mel_np * 80 - 80
        
        # ‰ΩøÁî®ÊîπËøõÁöÑGriffin-Lim
        recon_mel_power = librosa.db_to_power(recon_mel_denorm)
        recon_mel_power = np.clip(recon_mel_power, 1e-10, None)
        
        # ‰ΩøÁî®Êõ¥Â§öËø≠‰ª£ÁöÑGriffin-Lim
        reconstructed_audio = librosa.feature.inverse.mel_to_audio(
            recon_mel_power,
            sr=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            win_length=win_length,
            fmin=fmin,
            fmax=fmax,
            n_iter=60,  # Â¢ûÂä†Ëø≠‰ª£Ê¨°Êï∞
            length=None
        )
        
        # ÂêéÂ§ÑÁêÜÔºöÈôçÂô™
        reconstructed_audio = apply_simple_denoising(reconstructed_audio)
    
    return {
        'audio': reconstructed_audio,
        'encode_time': encode_time,
        'decode_time': decode_time
    }


def vae_encode_decode_improved(mel_spec_normalized, norm_params, vae, vocoder, device, sample_rate):
    """ÊîπËøõÁöÑVAEÁºñÁ†ÅËß£Á†Å"""
    start_time = time.time()
    
    mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.to(torch.float16)
    else:
        mel_tensor = mel_tensor.to(torch.float32)
    
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)
    
    # Á°Æ‰øùÂ∞∫ÂØ∏ÈÄÇÂêàVAE
    pad_width = (8 - (mel_input.shape[-1] % 8)) % 8
    if pad_width > 0:
        mel_input = F.pad(mel_input, (0, pad_width))
    
    with torch.no_grad():
        # VAEÁºñÁ†Å
        latents = vae.encode(mel_input).latent_dist.sample()
        latents = latents * vae.config.scaling_factor
        
        encode_time = time.time() - start_time
        
        # VAEËß£Á†Å
        decode_start = time.time()
        latents = latents / vae.config.scaling_factor
        reconstructed_mel = vae.decode(latents).sample
        decode_time = time.time() - decode_start
        
        # Â∞ùËØïÁõ¥Êé•‰ΩøÁî®vocoderÔºàÂ¶ÇÊûúÂÖºÂÆπÔºâ
        try:
            # Ë∞ÉÊï¥ÂΩ¢Áä∂Áªôvocoder
            vocoder_input = reconstructed_mel
            
            # Ê£ÄÊü•vocoderÊòØÂê¶ËÉΩÁõ¥Êé•Â§ÑÁêÜ
            if hasattr(vocoder, 'decode') or hasattr(vocoder, '__call__'):
                # Â∞ùËØï‰ΩøÁî®vocoder
                reconstructed_audio = vocoder(vocoder_input).squeeze().cpu().numpy()
                if len(reconstructed_audio.shape) > 1:
                    reconstructed_audio = reconstructed_audio[0]
                    
                print("ÊàêÂäü‰ΩøÁî®vocoderËøõË°åÈü≥È¢ëÈáçÂª∫ÔºÅ")
            else:
                # ÂõûÈÄÄÂà∞Griffin-Lim
                raise Exception("Vocoder‰∏çÂÖºÂÆπÔºå‰ΩøÁî®Griffin-Lim")
                
        except Exception as e:
            print(f"VocoderÂ§±Ë¥•Ôºå‰ΩøÁî®Griffin-Lim: {e}")
            
            # ÂèçÂΩí‰∏ÄÂåñÂπ∂‰ΩøÁî®Griffin-Lim
            recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
            recon_mel_denorm = (recon_mel_np + 1) / 2 * 80 - 80
            recon_mel_power = librosa.db_to_power(recon_mel_denorm)
            recon_mel_power = np.clip(recon_mel_power, 1e-10, None)
            
            reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                recon_mel_power,
                sr=sample_rate,
                hop_length=160,
                n_fft=1024,
                n_iter=60
            )
    
    return {
        'audio': reconstructed_audio,
        'encode_time': encode_time,
        'decode_time': decode_time
    }


def apply_simple_denoising(audio, alpha=0.1):
    """ÁÆÄÂçïÁöÑÈôçÂô™Â§ÑÁêÜ"""
    try:
        # ‰ΩøÁî®‰ΩéÈÄöÊª§Ê≥¢ÂáèÂ∞ëÈ´òÈ¢ëÂô™Â£∞
        from scipy.signal import butter, filtfilt
        
        nyquist = 8000  # sample_rate / 2
        cutoff = 7500   # ÂéªÈô§7.5kHz‰ª•‰∏äÁöÑÈ¢ëÁéá
        order = 4
        
        b, a = butter(order, cutoff / nyquist, btype='low')
        filtered_audio = filtfilt(b, a, audio)
        
        # ËΩªÂæÆÁöÑÂπ≥Êªë
        smoothed_audio = alpha * filtered_audio + (1 - alpha) * audio
        
        return smoothed_audio
    except:
        return audio


def calculate_quality_metrics(original, reconstructed):
    """ËÆ°ÁÆóÊõ¥ÂÖ®Èù¢ÁöÑË¥®ÈáèÊåáÊ†á"""
    min_len = min(len(original), len(reconstructed))
    orig = original[:min_len]
    recon = reconstructed[:min_len]
    
    # Âü∫Êú¨ÊåáÊ†á
    mse = np.mean((orig - recon) ** 2)
    signal_power = np.mean(orig ** 2)
    noise_power = np.mean((orig - recon) ** 2)
    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
    correlation = np.corrcoef(orig, recon)[0, 1] if len(orig) > 1 else 0
    
    # È¢ëË∞±Áõ∏ÂÖ≥ÊÄß
    orig_fft = np.abs(np.fft.fft(orig))
    recon_fft = np.abs(np.fft.fft(recon))
    spectral_correlation = np.corrcoef(orig_fft, recon_fft)[0, 1] if len(orig_fft) > 1 else 0
    
    # È¢ëË∞±Ë¥®ÂøÉÊØîËæÉ
    orig_centroid = librosa.feature.spectral_centroid(y=orig, sr=16000)[0].mean()
    recon_centroid = librosa.feature.spectral_centroid(y=recon, sr=16000)[0].mean()
    centroid_diff = abs(orig_centroid - recon_centroid) / orig_centroid
    
    return {
        'mse': mse,
        'snr': snr,
        'correlation': correlation,
        'spectral_correlation': spectral_correlation,
        'centroid_difference': centroid_diff
    }


def main():
    """‰∏ªÂáΩÊï∞"""
    if len(sys.argv) < 2:
        print("‰ΩøÁî®ÊñπÊ≥ï: python improved_vae_test.py <Èü≥È¢ëÊñá‰ª∂Ë∑ØÂæÑ> [ÊúÄÂ§ßÈïøÂ∫¶ÁßíÊï∞]")
        
        # Êü•ÊâæÈü≥È¢ëÊñá‰ª∂
        audio_files = []
        for ext in ['.wav', '.mp3', '.flac']:
            audio_files.extend(Path('.').glob(f'*{ext}'))
        
        if audio_files:
            print(f"\nÊâæÂà∞Èü≥È¢ëÊñá‰ª∂:")
            for i, file in enumerate(audio_files, 1):
                print(f"{i}. {file}")
            
            try:
                choice = input("ËØ∑ÈÄâÊã©Êñá‰ª∂Â∫èÂè∑: ").strip()
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(audio_files):
                    audio_path = str(audio_files[file_idx])
                else:
                    print("Êó†ÊïàÈÄâÊã©")
                    return
            except (ValueError, KeyboardInterrupt):
                print("ÂèñÊ∂àÊìç‰Ωú")
                return
        else:
            print("ÂΩìÂâçÁõÆÂΩïÊ≤°ÊúâÊâæÂà∞Èü≥È¢ëÊñá‰ª∂")
            return
    else:
        audio_path = sys.argv[1]
    
    max_length = 10
    if len(sys.argv) >= 3:
        try:
            max_length = float(sys.argv[2])
        except ValueError:
            print(f"Êó†ÊïàÁöÑÈïøÂ∫¶ÂèÇÊï∞Ôºå‰ΩøÁî®ÈªòËÆ§ÂÄº {max_length} Áßí")
    
    print(f"ÂºÄÂßãÊîπËøõÁâàVAEÊµãËØï: {audio_path}")
    print(f"ÊúÄÂ§ßÈïøÂ∫¶: {max_length} Áßí")
    
    try:
        results = improved_vae_test(audio_path, max_length=max_length)
        if results:
            print("\n‚úÖ ÊîπËøõÁâàÊµãËØïÂÆåÊàêÔºÅ")
            print("ËØ∑Êí≠Êîæ‰∏çÂêåÊñπÊ≥ïÁöÑÈáçÂª∫Èü≥È¢ëÊù•ÊØîËæÉË¥®ÈáèÂ∑ÆÂºÇ„ÄÇ")
    except Exception as e:
        print(f"ÊµãËØïÂ§±Ë¥•: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
