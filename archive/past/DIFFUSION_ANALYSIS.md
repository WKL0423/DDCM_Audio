# ğŸ”¬ AudioLDM2 é¡¹ç›® Diffusion èƒ½åŠ›åˆ†æ

## â“ ä½ çš„é—®é¢˜ï¼š"ç°åœ¨çš„è„šæœ¬é‡Œï¼Œæœ‰diffusionè¿‡ç¨‹å—ï¼Ÿ"

## âœ… **ç­”æ¡ˆï¼šæ˜¯çš„ï¼Œæœ‰å®Œæ•´çš„diffusionè¿‡ç¨‹ï¼**

---

## ğŸ“Š **Diffusion èƒ½åŠ›åˆ†å¸ƒ**

### ğŸ¯ **å®Œæ•´Diffusion Pipeline** (Text â†’ Audio)

#### âœ… æœ‰å®Œæ•´diffusionè¿‡ç¨‹çš„è„šæœ¬ï¼š

1. **`main.py`** â­â­â­â­â­
   ```python
   # åŒ…å«å®Œæ•´çš„text-to-audio diffusionç”Ÿæˆ
   prompt = "Techno music with a strong, upbeat tempo and high melodic riffs."
   audio = pipe(prompt, num_inference_steps=200, audio_length_in_s=10.0).audios[0]
   ```

2. **`main_enhanced_fixed.py`** â­â­â­â­â­
   - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§AudioLDM2æ¨¡å‹
   - å®Œæ•´çš„diffusion pipeline

3. **`main_multi_model.py`** â­â­â­â­â­
   - å¤šæ¨¡å‹æ”¯æŒç‰ˆæœ¬
   - å®Œæ•´çš„diffusion pipeline

4. **`New_pipeline_audioldm2.py`** â­â­â­â­â­
   - **æ ¸å¿ƒdiffusionæ¡†æ¶å®ç°**
   - åŒ…å«å®Œæ•´çš„diffusionç»„ä»¶

### ğŸ”§ **ä»…VAEæµ‹è¯•** (Audio â†’ Latent â†’ Audio)

#### âŒ æ²¡æœ‰diffusionè¿‡ç¨‹çš„è„šæœ¬ï¼š

1. **`simple_vae_test.py`** - ä»…VAEç¼–ç /è§£ç 
2. **`vae_final_noise_fix.py`** - ä»…VAEé‡å»º
3. **`vae_hifigan_ultimate.py`** - ä»…VAE+HiFiGANæµ‹è¯•

---

## ğŸ§  **å®Œæ•´Diffusionç»„ä»¶åˆ†æ**

### åœ¨ `New_pipeline_audioldm2.py` ä¸­åŒ…å«ï¼š

#### âœ… **æ ¸å¿ƒDiffusionç»„ä»¶**ï¼š
- **UNet2DConditionModel** - æ‰©æ•£å»å™ªç½‘ç»œ
- **Scheduler (å™ªéŸ³è°ƒåº¦å™¨)** - DDIM/LMS/PNDMç­‰
- **VAE (å˜åˆ†è‡ªç¼–ç å™¨)** - æ½œåœ¨ç©ºé—´ç¼–ç /è§£ç 
- **HiFiGAN Vocoder** - éŸ³é¢‘ç”Ÿæˆå™¨

#### âœ… **æ–‡æœ¬ç†è§£ç»„ä»¶**ï¼š
- **CLAPæ–‡æœ¬ç¼–ç å™¨** - éŸ³é¢‘-æ–‡æœ¬è”åˆåµŒå…¥
- **T5æ–‡æœ¬ç¼–ç å™¨** - é«˜è´¨é‡æ–‡æœ¬ç†è§£
- **GPT2è¯­è¨€æ¨¡å‹** - æ–‡æœ¬åºåˆ—ç”Ÿæˆ

#### âœ… **å®Œæ•´Diffusionæµç¨‹**ï¼š
```python
# 1. æ–‡æœ¬ç¼–ç  (Text Encoding)
prompt_embeds = self.encode_prompt(prompt, ...)

# 2. åˆå§‹åŒ–å™ªéŸ³ (Noise Initialization)  
latents = randn_tensor(shape, generator=generator, device=device)

# 3. æ‰©æ•£å»å™ªå¾ªç¯ (Diffusion Denoising Loop)
for i, t in enumerate(timesteps):
    noise_pred = self.unet(latents, t, encoder_hidden_states=prompt_embeds)
    latents = self.scheduler.step(noise_pred, t, latents).prev_sample

# 4. VAEè§£ç  (VAE Decoding)
mel_spectrogram = self.vae.decode(latents).sample

# 5. éŸ³é¢‘ç”Ÿæˆ (Audio Generation)
audio = self.mel_spectrogram_to_waveform(mel_spectrogram)
```

---

## ğŸ­ **åŠŸèƒ½å¯¹æ¯”è¡¨**

| è„šæœ¬ç±»å‹ | Diffusionè¿‡ç¨‹ | UNetå»å™ª | æ–‡æœ¬æ¡ä»¶ | ç”Ÿæˆç±»å‹ |
|---------|--------------|---------|---------|---------|
| **main.pyç­‰** | âœ… å®Œæ•´ | âœ… æœ‰ | âœ… æœ‰ | Textâ†’Audio |
| **vae_*.py** | âŒ æ—  | âŒ æ—  | âŒ æ—  | Audioâ†’Audio |
| **New_pipeline** | âœ… æ¡†æ¶ | âœ… æœ‰ | âœ… æœ‰ | æ¡†æ¶å®šä¹‰ |

---

## ğŸ¯ **æ€»ç»“å›ç­”**

### âœ… **æ˜¯çš„ï¼Œä½ çš„é¡¹ç›®ä¸­æœ‰å®Œæ•´çš„diffusionè¿‡ç¨‹ï¼**

**ä¸»è¦ä½“ç°åœ¨**ï¼š
1. **`main.py`** - å¯ä»¥ç›´æ¥è¿è¡Œtext-to-audioç”Ÿæˆ
2. **`New_pipeline_audioldm2.py`** - åŒ…å«å®Œæ•´çš„diffusionæ¡†æ¶å®ç°
3. **å¤šä¸ªmainè„šæœ¬** - éƒ½æ”¯æŒå®Œæ•´çš„diffusionç”Ÿæˆ

**ä½†æ˜¯**ï¼š
- **å½“å‰ä½ æŸ¥çœ‹çš„VAEè„šæœ¬** (`simple_vae_test.py`, `vae_final_noise_fix.py` ç­‰) **æ²¡æœ‰diffusionè¿‡ç¨‹**
- è¿™äº›è„šæœ¬ä¸“æ³¨äºVAEé‡å»ºæµ‹è¯•ï¼Œè·³è¿‡äº†UNetæ‰©æ•£æ­¥éª¤

### ğŸš€ **å¦‚æœä½ æƒ³è¿è¡Œå®Œæ•´çš„diffusionç”Ÿæˆ**ï¼š
```bash
# è¿è¡Œå®Œæ•´çš„text-to-audio diffusion
python main.py

# æˆ–è€…ä½¿ç”¨å¢å¼ºç‰ˆæœ¬
python main_enhanced_fixed.py
```

**è¿™äº›ä¼šæ‰§è¡Œå®Œæ•´çš„diffusionè¿‡ç¨‹ï¼šæ–‡æœ¬ç†è§£ â†’ UNetå»å™ª â†’ VAEè§£ç  â†’ éŸ³é¢‘ç”Ÿæˆ** ğŸµ
