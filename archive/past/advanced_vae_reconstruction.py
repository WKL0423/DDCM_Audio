"""
AudioLDM2 VAEé«˜è´¨é‡éŸ³é¢‘é‡å»º - ç»ˆæè§£å†³æ–¹æ¡ˆ
==================================================

ç›®æ ‡: è§£å†³"èƒ½å¬å‡ºè”ç³»ä½†è´¨é‡ä¸€èˆ¬"çš„é—®é¢˜
æ ¸å¿ƒç­–ç•¥: æ›¿æ¢Griffin-Lim (92%ä¿¡æ¯æŸå¤±) â†’ é«˜è´¨é‡ç¥ç»vocoder
ä½œè€…: AudioLDM2 ç ”ç©¶å›¢é˜Ÿ
ç‰ˆæœ¬: v3.0 - ç»ˆæç‰ˆæœ¬
"""

import torch
import torchaudio
import librosa
import numpy as np
import os
import time
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from diffusers import AudioLDM2Pipeline
from transformers import SpeechT5HifiGan
import scipy.io.wavfile
from datetime import datetime

warnings.filterwarnings("ignore")

class AdvancedVAEReconstructor:
    """é«˜çº§VAEéŸ³é¢‘é‡å»ºå™¨ - è§£å†³è´¨é‡ç“¶é¢ˆçš„ç»ˆææ–¹æ¡ˆ"""
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """
        åˆå§‹åŒ–é«˜çº§é‡å»ºå™¨
        
        Args:
            model_name: AudioLDM2æ¨¡å‹åç§°
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ åˆå§‹åŒ–é«˜çº§VAEé‡å»ºå™¨")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸµ æ¨¡å‹: {model_name}")
        
        # åŠ è½½AudioLDM2ä¸»æ¨¡å‹
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipe = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float32 if self.device.type == "cpu" else torch.float16
        ).to(self.device)
        
        self.vae = self.pipe.vae
        self.vocoder = self.pipe.vocoder
        
        # é«˜è´¨é‡vocoderæ± 
        self.vocoders = {}
        self._load_advanced_vocoders()
        
        # ä¼˜åŒ–çš„melå‚æ•°é…ç½®
        self.mel_configs = {
            'ultra_high': {
                'n_mels': 128,
                'n_fft': 2048,
                'hop_length': 256,
                'win_length': 1024,
                'fmin': 0,
                'fmax': 8000
            },
            'high_quality': {
                'n_mels': 80,
                'n_fft': 1024,
                'hop_length': 256,
                'win_length': 1024,
                'fmin': 0,
                'fmax': 8000
            },
            'balanced': {
                'n_mels': 64,
                'n_fft': 1024,
                'hop_length': 256,
                'win_length': 1024,
                'fmin': 0,
                'fmax': 8000
            }
        }
        
        print("âœ… é«˜çº§é‡å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_advanced_vocoders(self):
        """åŠ è½½å¤šç§é«˜è´¨é‡vocoder"""
        print("ğŸ¤ åŠ è½½é«˜è´¨é‡vocoderæ± ...")
        
        try:
            # 1. AudioLDM2å†…ç½®vocoder (å·²ä¿®å¤)
            self.vocoders['audioldm2'] = self.vocoder
            print("âœ… AudioLDM2å†…ç½®vocoderå·²åŠ è½½")
        except Exception as e:
            print(f"âŒ AudioLDM2 vocoderåŠ è½½å¤±è´¥: {e}")
        
        try:
            # 2. Microsoft SpeechT5 HiFiGAN
            self.vocoders['hifigan'] = SpeechT5HifiGan.from_pretrained(
                "microsoft/speecht5_hifigan"
            ).to(self.device)
            print("âœ… Microsoft HiFiGANå·²åŠ è½½")
        except Exception as e:
            print(f"âŒ HiFiGANåŠ è½½å¤±è´¥: {e}")
        
        # 3. Griffin-Lim (åŸºçº¿å¯¹æ¯”)
        self.vocoders['griffin_lim'] = 'griffin_lim'
        print("âœ… Griffin-LimåŸºçº¿å·²å‡†å¤‡")
        
        print(f"ğŸ¯ å·²åŠ è½½ {len(self.vocoders)} ä¸ªvocoder")
    
    def load_audio(self, audio_path: str, max_duration: float = 10.0) -> Tuple[np.ndarray, int]:
        """
        åŠ è½½éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            max_duration: æœ€å¤§æ—¶é•¿(ç§’)
            
        Returns:
            (audio_data, sample_rate)
        """
        try:
            audio, sr = librosa.load(audio_path, sr=16000, duration=max_duration)
            print(f"ğŸ“Š éŸ³é¢‘åŠ è½½æˆåŠŸ: {len(audio)/sr:.2f}ç§’, {len(audio)}é‡‡æ ·ç‚¹")
            return audio, sr
        except Exception as e:
            raise RuntimeError(f"éŸ³é¢‘åŠ è½½å¤±è´¥: {e}")
    
    def audio_to_mel(self, audio: np.ndarray, sr: int, config_name: str = 'balanced') -> torch.Tensor:
        """
        å°†éŸ³é¢‘è½¬æ¢ä¸ºmelé¢‘è°±å›¾
        
        Args:
            audio: éŸ³é¢‘æ•°æ®
            sr: é‡‡æ ·ç‡
            config_name: melé…ç½®åç§°
            
        Returns:
            melé¢‘è°±å›¾å¼ é‡
        """
        config = self.mel_configs[config_name]
        
        # è®¡ç®—melé¢‘è°±å›¾
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sr,
            n_mels=config['n_mels'],
            n_fft=config['n_fft'],
            hop_length=config['hop_length'],
            win_length=config['win_length'],
            fmin=config['fmin'],
            fmax=config['fmax']
        )
        
        # è½¬æ¢ä¸ºå¯¹æ•°å°ºåº¦
        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        mel_spec = 2.0 * (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min()) - 1.0
        
        # è½¬æ¢ä¸ºtensorå¹¶è°ƒæ•´ç»´åº¦
        mel_tensor = torch.from_numpy(mel_spec).float().unsqueeze(0).unsqueeze(0)
        
        print(f"ğŸ¼ Melé¢‘è°±å›¾ç”Ÿæˆ: {mel_tensor.shape} ({config_name}é…ç½®)")
        return mel_tensor.to(self.device)
    
    def vae_encode_decode(self, mel_tensor: torch.Tensor) -> torch.Tensor:
        """
        VAEç¼–ç è§£ç è¿‡ç¨‹
        
        Args:
            mel_tensor: è¾“å…¥melé¢‘è°±å›¾
            
        Returns:
            é‡å»ºçš„melé¢‘è°±å›¾
        """
        with torch.no_grad():
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            if next(self.vae.parameters()).dtype == torch.float16:
                mel_tensor = mel_tensor.half()
            
            # è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…VAEè¾“å…¥è¦æ±‚
            if mel_tensor.shape[-1] % 4 != 0:
                pad_length = 4 - (mel_tensor.shape[-1] % 4)
                mel_tensor = torch.nn.functional.pad(mel_tensor, (0, pad_length))
            
            # VAEç¼–ç 
            start_time = time.time()
            latent = self.vae.encode(mel_tensor).latent_dist.sample()
            encode_time = time.time() - start_time
            
            # VAEè§£ç 
            start_time = time.time()
            reconstructed = self.vae.decode(latent).sample
            decode_time = time.time() - start_time
            
            print(f"ğŸ”„ VAEç¼–ç : {encode_time:.3f}s, è§£ç : {decode_time:.3f}s")
            print(f"ğŸ“¦ æ½œåœ¨ç©ºé—´: {latent.shape}")
            
            return reconstructed.float()
    
    def mel_to_audio_advanced(self, mel_tensor: torch.Tensor, vocoder_name: str, 
                             sr: int = 16000, **kwargs) -> np.ndarray:
        """
        ä½¿ç”¨é«˜çº§vocoderå°†melé¢‘è°±å›¾è½¬æ¢ä¸ºéŸ³é¢‘
        
        Args:
            mel_tensor: melé¢‘è°±å›¾
            vocoder_name: vocoderåç§°
            sr: ç›®æ ‡é‡‡æ ·ç‡
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            éŸ³é¢‘æ•°æ®
        """
        with torch.no_grad():
            if vocoder_name == 'griffin_lim':
                return self._griffin_lim_advanced(mel_tensor, sr, **kwargs)
            elif vocoder_name in self.vocoders:
                return self._neural_vocoder(mel_tensor, vocoder_name, **kwargs)
            else:
                raise ValueError(f"æœªçŸ¥çš„vocoder: {vocoder_name}")
    
    def _griffin_lim_advanced(self, mel_tensor: torch.Tensor, sr: int, 
                             n_iter: int = 128, **kwargs) -> np.ndarray:
        """é«˜çº§Griffin-Limç®—æ³•"""
        # è½¬æ¢å›numpy
        mel_np = mel_tensor.squeeze().cpu().numpy()
        
        # åå½’ä¸€åŒ–
        mel_np = (mel_np + 1.0) / 2.0
        mel_min, mel_max = -80, 0  # å‡è®¾çš„melèŒƒå›´
        mel_np = mel_np * (mel_max - mel_min) + mel_min
        
        # è½¬æ¢ä¸ºçº¿æ€§å°ºåº¦
        mel_linear = librosa.db_to_power(mel_np)
          # é«˜çº§Griffin-Limé‡å»º
        audio = librosa.feature.inverse.mel_to_audio(
            mel_linear,
            sr=sr,
            n_iter=n_iter,
            hop_length=256,
            win_length=1024,
            n_fft=1024,
            fmin=0,
            fmax=8000
        )
        
        return audio
      def _neural_vocoder(self, mel_tensor: torch.Tensor, vocoder_name: str, **kwargs) -> np.ndarray:
        """ç¥ç»ç½‘ç»œvocoder"""
        vocoder = self.vocoders[vocoder_name]
        
        try:
            if vocoder_name == 'audioldm2':
                # AudioLDM2å†…ç½®vocoder (å·²ä¿®å¤ç»´åº¦é—®é¢˜)
                mel_input = mel_tensor.squeeze(0)  # [1, 64, time] -> [64, time]
                mel_input = mel_input.transpose(-2, -1)  # [64, time] -> [time, 64]
                mel_input = mel_input.unsqueeze(0)  # [time, 64] -> [1, time, 64]
                
                # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
                if hasattr(vocoder, 'dtype'):
                    mel_input = mel_input.to(vocoder.dtype)
                elif next(vocoder.parameters()).dtype == torch.float16:
                    mel_input = mel_input.half()
                
                audio_tensor = vocoder(mel_input.to(self.device))
                audio = audio_tensor.squeeze().cpu().numpy()
                
            elif vocoder_name == 'hifigan':
                # Microsoft HiFiGAN
                mel_input = mel_tensor.squeeze()  # [1, 1, 64, time] -> [64, time]
                if mel_input.dim() == 3:
                    mel_input = mel_input.squeeze(0)  # [1, 64, time] -> [64, time]
                
                # HiFiGANé€šå¸¸éœ€è¦80ç»´mel
                if mel_input.shape[0] != 80:
                    # æ’å€¼åˆ°80ç»´
                    mel_input = torch.nn.functional.interpolate(
                        mel_input.unsqueeze(0), 
                        size=(80, mel_input.shape[1]), 
                        mode='bilinear', 
                        align_corners=False
                    ).squeeze(0)
                
                mel_input = mel_input.unsqueeze(0)  # [80, time] -> [1, 80, time]
                audio_tensor = vocoder(mel_input)
                audio = audio_tensor.squeeze().cpu().numpy()
                
            else:
                raise ValueError(f"æœªå®ç°çš„vocoder: {vocoder_name}")
                
            return audio
            
        except Exception as e:
            print(f"âŒ {vocoder_name} vocoderå¤±è´¥: {e}")
            # é™çº§åˆ°Griffin-Lim
            return self._griffin_lim_advanced(mel_tensor, 16000)
    
    def post_process_audio(self, audio: np.ndarray, method: str = 'normalize') -> np.ndarray:
        """
        éŸ³é¢‘åå¤„ç†
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘
            method: å¤„ç†æ–¹æ³•
            
        Returns:
            å¤„ç†åçš„éŸ³é¢‘
        """
        if method == 'normalize':
            # å½’ä¸€åŒ–
            audio = audio / (np.max(np.abs(audio)) + 1e-8)
        elif method == 'dynamic_range':
            # åŠ¨æ€èŒƒå›´å‹ç¼©
            audio = np.tanh(audio * 2.0) * 0.8
        elif method == 'spectral_enhance':
            # é¢‘è°±å¢å¼º (ç®€å•é«˜é€šæ»¤æ³¢)
            from scipy import signal
            b, a = signal.butter(3, 100, 'high', fs=16000)
            audio = signal.filtfilt(b, a, audio)
            audio = audio / (np.max(np.abs(audio)) + 1e-8)
        
        return audio
    
    def calculate_metrics(self, original: np.ndarray, reconstructed: np.ndarray) -> Dict:
        """è®¡ç®—é‡å»ºè´¨é‡æŒ‡æ ‡"""
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(original), len(reconstructed))
        orig = original[:min_len]
        recon = reconstructed[:min_len]
        
        # è®¡ç®—æŒ‡æ ‡
        mse = np.mean((orig - recon) ** 2)
        snr = 10 * np.log10(np.mean(orig ** 2) / (mse + 1e-8))
        correlation = np.corrcoef(orig, recon)[0, 1]
        
        return {
            'mse': float(mse),
            'snr': float(snr),
            'correlation': float(correlation if not np.isnan(correlation) else 0)
        }
    
    def comprehensive_test(self, audio_path: str, output_dir: str = "advanced_vae_test") -> Dict:
        """
        å…¨é¢æµ‹è¯•ä¸åŒé‡å»ºç­–ç•¥
        
        Args:
            audio_path: æµ‹è¯•éŸ³é¢‘è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print("ğŸ¯ å¼€å§‹å…¨é¢VAEé‡å»ºè´¨é‡æµ‹è¯•")
        print("="*60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = self.load_audio(audio_path, max_duration=5.0)
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘
        audio_name = Path(audio_path).stem
        timestamp = int(time.time())
        original_path = output_path / f"{audio_name}_original_{timestamp}.wav"
        scipy.io.wavfile.write(original_path, sr, (audio * 32767).astype(np.int16))
        
        results = {}
        
        # æµ‹è¯•ä¸åŒé…ç½®ç»„åˆ
        test_configs = [
            ('balanced', 'griffin_lim', 'normalize'),
            ('balanced', 'hifigan', 'normalize'),
            ('balanced', 'audioldm2', 'normalize'),
            ('high_quality', 'griffin_lim', 'dynamic_range'),
            ('high_quality', 'hifigan', 'spectral_enhance'),
            ('ultra_high', 'griffin_lim', 'normalize'),
        ]
        
        print(f"\nğŸ§ª æµ‹è¯• {len(test_configs)} ç§é…ç½®ç»„åˆ...")
        
        for i, (mel_config, vocoder, post_process) in enumerate(test_configs):
            print(f"\nğŸ“Š é…ç½® {i+1}/{len(test_configs)}: {mel_config} + {vocoder} + {post_process}")
            
            try:
                # éŸ³é¢‘ â†’ mel
                mel_tensor = self.audio_to_mel(audio, sr, mel_config)
                
                # VAE ç¼–ç è§£ç 
                reconstructed_mel = self.vae_encode_decode(mel_tensor)
                
                # mel â†’ éŸ³é¢‘
                reconstructed_audio = self.mel_to_audio_advanced(
                    reconstructed_mel, vocoder, sr
                )
                
                # åå¤„ç†
                final_audio = self.post_process_audio(reconstructed_audio, post_process)
                
                # è®¡ç®—æŒ‡æ ‡
                metrics = self.calculate_metrics(audio, final_audio)
                
                # ä¿å­˜ç»“æœ
                config_name = f"{mel_config}_{vocoder}_{post_process}"
                output_file = output_path / f"{audio_name}_{config_name}_{timestamp}.wav"
                scipy.io.wavfile.write(output_file, sr, (final_audio * 32767).astype(np.int16))
                
                results[config_name] = {
                    'metrics': metrics,
                    'file': str(output_file),
                    'config': {
                        'mel': mel_config,
                        'vocoder': vocoder,
                        'post_process': post_process
                    }
                }
                
                print(f"   âœ… SNR: {metrics['snr']:.2f}dB, ç›¸å…³æ€§: {metrics['correlation']:.4f}")
                
            except Exception as e:
                print(f"   âŒ é…ç½®å¤±è´¥: {e}")
                continue
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report(results, audio_path, output_path)
        
        return results
    
    def _generate_report(self, results: Dict, audio_path: str, output_path: Path):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ¯ é«˜çº§VAEé‡å»ºæµ‹è¯•ç»“æœåˆ†æ")
        print("="*60)
        
        if not results:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
            return
        
        # æŒ‰SNRæ’åº
        sorted_results = sorted(
            results.items(),
            key=lambda x: x[1]['metrics']['snr'],
            reverse=True
        )
        
        print(f"\nğŸ† è´¨é‡æ’å (å‰{min(5, len(sorted_results))}å):")
        for i, (config, data) in enumerate(sorted_results[:5]):
            metrics = data['metrics']
            print(f"   #{i+1} {config}")
            print(f"       ğŸ“ˆ SNR: {metrics['snr']:.2f}dB")
            print(f"       ğŸ”— ç›¸å…³æ€§: {metrics['correlation']:.4f}")
            print(f"       ğŸ“ æ–‡ä»¶: {data['file']}")
        
        # æœ€ä½³ç»“æœ
        if sorted_results:
            best_config, best_data = sorted_results[0]
            print(f"\nğŸš€ æœ€ä½³é…ç½®: {best_config}")
            print(f"   ğŸ“ˆ æœ€é«˜SNR: {best_data['metrics']['snr']:.2f}dB")
            print(f"   ğŸ”— æœ€é«˜ç›¸å…³æ€§: {best_data['metrics']['correlation']:.4f}")
        
        # æŒ‰vocoderåˆ†æ
        vocoder_stats = {}
        for config, data in results.items():
            vocoder = data['config']['vocoder']
            if vocoder not in vocoder_stats:
                vocoder_stats[vocoder] = []
            vocoder_stats[vocoder].append(data['metrics']['snr'])
        
        print(f"\nğŸ“Š ä¸åŒVocoderæ•ˆæœå¯¹æ¯”:")
        for vocoder, snrs in vocoder_stats.items():
            avg_snr = np.mean(snrs)
            max_snr = np.max(snrs)
            print(f"   ğŸ¤ {vocoder}: å¹³å‡{avg_snr:.2f}dB, æœ€ä½³{max_snr:.2f}dB")
        
        print(f"\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_path}")
        print("ğŸ§ å»ºè®®æ’­æ”¾æœ€ä½³ç»“æœè¿›è¡Œä¸»è§‚è¯„ä¼°")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python advanced_vae_reconstruction.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„>")
        
        # åˆ—å‡ºå¯ç”¨éŸ³é¢‘æ–‡ä»¶
        audio_files = []
        for ext in ['*.wav', '*.mp3', '*.flac']:
            audio_files.extend(Path('.').glob(ext))
        
        if audio_files:
            print("æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files, 1):
                print(f"{i}. {file.name}")
            
            try:
                choice = int(input("è¯·é€‰æ‹©æ–‡ä»¶åºå·:"))
                audio_path = str(audio_files[choice-1])
            except (ValueError, IndexError):
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
        else:
            print("âŒ å½“å‰ç›®å½•ä¸‹æ²¡æœ‰éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    # åˆ›å»ºé‡å»ºå™¨å¹¶è¿è¡Œæµ‹è¯•
    reconstructor = AdvancedVAEReconstructor()
    results = reconstructor.comprehensive_test(audio_path)
    
    print("\nâœ… é«˜çº§VAEé‡å»ºæµ‹è¯•å®Œæˆ!")
    print("ğŸ‰ ç»“æœå·²ä¿å­˜ï¼Œè¯·æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶å¤¹è¿›è¡Œä¸»è§‚è¯„ä¼°")

if __name__ == "__main__":
    main()
