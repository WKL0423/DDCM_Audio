"""
AudioLDM2 VAE ä¿®å¤ç‰ˆæœ¬ - ä½¿ç”¨å†…ç½®HiFiGAN
=============================================

æ­£ç¡®ä½¿ç”¨AudioLDM2å†…ç½®çš„HiFiGAN vocoder
è§£å†³ç»´åº¦ä¸åŒ¹é…é—®é¢˜
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F
from diffusers import AudioLDM2Pipeline

def load_and_test_vae_fixed(audio_path, model_id="cvssp/audioldm2-music", max_length=10):
    """
    ä¿®å¤ç‰ˆæœ¬ï¼šæ­£ç¡®ä½¿ç”¨AudioLDM2å†…ç½®HiFiGAN
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    if not os.path.exists(audio_path):
        print(f"âŒ æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ {audio_path}")
        return
    
    print(f"ğŸ“¦ åŠ è½½ AudioLDM2 æ¨¡å‹: {model_id}")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder  # è¿™æ˜¯AudioLDM2å†…ç½®çš„HiFiGAN
    sample_rate = 16000
    
    print(f"ğŸ¤ Vocoderç±»å‹: {type(vocoder)}")
    print(f"ğŸ“Š åŠ è½½éŸ³é¢‘: {audio_path}")
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    if len(audio) > max_length * sample_rate:
        audio = audio[:int(max_length * sample_rate)]
        print(f"âœ‚ï¸ éŸ³é¢‘è£å‰ªåˆ° {max_length} ç§’")
    
    print(f"ğŸ“ˆ éŸ³é¢‘ä¿¡æ¯: {len(audio)/sample_rate:.2f}ç§’, {len(audio)}æ ·æœ¬")
    
    # åˆ›å»ºmelé¢‘è°±å›¾
    print("ğŸ¼ åˆ›å»ºmelé¢‘è°±å›¾...")
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sample_rate,
        n_mels=64,  # AudioLDM2ä½¿ç”¨64ç»´
        hop_length=160,
        n_fft=1024,
        fmin=0,
        fmax=8000
    )
    
    # è½¬æ¢ä¸ºå¯¹æ•°å°ºåº¦å¹¶å½’ä¸€åŒ–
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_min = mel_spec_db.min()
    mel_max = mel_spec_db.max()
    mel_spec_normalized = 2 * (mel_spec_db - mel_min) / (mel_max - mel_min) - 1
    mel_spec_normalized = np.clip(mel_spec_normalized, -1, 1)
    
    # ä¿å­˜å½’ä¸€åŒ–å‚æ•°
    norm_params = {'min_val': mel_min, 'max_val': mel_max}
    
    # è½¬æ¢ä¸ºtensor
    mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.to(torch.float16)
    else:
        mel_tensor = mel_tensor.to(torch.float32)
    
    # è°ƒæ•´ä¸ºVAEæ ¼å¼: [batch, channels, height, width]
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)
    print(f"ğŸµ Melé¢‘è°±å½¢çŠ¶: {mel_input.shape}")
    
    # VAEç¼–ç è§£ç 
    print("ğŸ”„ VAEç¼–ç è§£ç ...")
    start_time = time.time()
    
    with torch.no_grad():
        try:
            # ç¼–ç 
            latents = vae.encode(mel_input).latent_dist.sample()
            latents = latents * vae.config.scaling_factor
            encode_time = time.time() - start_time
            print(f"âœ… ç¼–ç å®Œæˆ: {encode_time:.3f}ç§’, æ½œåœ¨å½¢çŠ¶: {latents.shape}")
            
            # è§£ç 
            decode_start = time.time()
            latents = latents / vae.config.scaling_factor
            reconstructed_mel = vae.decode(latents).sample
            decode_time = time.time() - decode_start
            print(f"âœ… è§£ç å®Œæˆ: {decode_time:.3f}ç§’, é‡å»ºå½¢çŠ¶: {reconstructed_mel.shape}")
            
        except Exception as e:
            print(f"âŒ VAEå¤±è´¥: {e}")
            # å°è¯•è°ƒæ•´å°ºå¯¸
            pad_width = (8 - (mel_input.shape[-1] % 8)) % 8
            if pad_width > 0:
                mel_input = F.pad(mel_input, (0, pad_width))
            
            latents = vae.encode(mel_input).latent_dist.sample()
            latents = latents * vae.config.scaling_factor
            encode_time = time.time() - start_time
            
            decode_start = time.time()
            latents = latents / vae.config.scaling_factor
            reconstructed_mel = vae.decode(latents).sample
            decode_time = time.time() - decode_start
            print(f"âœ… é‡è¯•æˆåŠŸ: ç¼–ç {encode_time:.3f}s, è§£ç {decode_time:.3f}s")
    
    # éŸ³é¢‘é‡å»ºï¼šä¼˜å…ˆä½¿ç”¨AudioLDM2å†…ç½®HiFiGAN
    print("ğŸ¤ éŸ³é¢‘é‡å»º...")
    vocoder_method = "Unknown"
    
    with torch.no_grad():
        try:
            # æ–¹æ³•1: AudioLDM2å†…ç½®HiFiGAN
            print("   ğŸ¯ å°è¯•AudioLDM2å†…ç½®HiFiGAN...")
            
            # å‡†å¤‡è¾“å…¥: [batch, time, mel_dim]
            vocoder_input = reconstructed_mel.squeeze(0)  # [1, 64, time] -> [64, time]
            vocoder_input = vocoder_input.transpose(-2, -1)  # [64, time] -> [time, 64]
            vocoder_input = vocoder_input.unsqueeze(0)  # [time, 64] -> [1, time, 64]
            
            print(f"      HiFiGANè¾“å…¥: {vocoder_input.shape}")
            
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            if hasattr(vocoder, 'dtype'):
                vocoder_input = vocoder_input.to(vocoder.dtype)
            elif next(vocoder.parameters()).dtype == torch.float16:
                vocoder_input = vocoder_input.half()
            
            audio_tensor = vocoder(vocoder_input)
            reconstructed_audio = audio_tensor.squeeze().cpu().numpy()
            vocoder_method = "AudioLDM2_HiFiGAN"
            print(f"   âœ… AudioLDM2 HiFiGANæˆåŠŸ: {len(reconstructed_audio)}æ ·æœ¬")
            
        except Exception as e:
            print(f"   âŒ AudioLDM2 HiFiGANå¤±è´¥: {e}")
            print("   ğŸ”„ é™çº§åˆ°Griffin-Lim...")
            
            # æ–¹æ³•2: Griffin-Limé™çº§
            recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
            recon_mel_denorm = (recon_mel_np + 1) / 2 * (norm_params['max_val'] - norm_params['min_val']) + norm_params['min_val']
            recon_mel_denorm = np.clip(recon_mel_denorm, -80.0, 0.0)
            recon_mel_power = librosa.db_to_power(recon_mel_denorm)
            
            try:
                reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                    recon_mel_power,
                    sr=sample_rate,
                    hop_length=160,
                    n_fft=1024,
                    fmin=0,
                    fmax=8000
                )
                vocoder_method = "Griffin_Lim"
                print(f"   âœ… Griffin-LimæˆåŠŸ: {len(reconstructed_audio)}æ ·æœ¬")
            except Exception as e2:
                print(f"   âŒ Griffin-Limä¹Ÿå¤±è´¥: {e2}")
                reconstructed_audio = np.random.randn(len(audio)) * 0.1
                vocoder_method = "Fallback_Noise"
        
        # åå¤„ç†
        if len(reconstructed_audio) > 0:
            reconstructed_audio = np.nan_to_num(reconstructed_audio, nan=0.0)
        else:
            reconstructed_audio = np.zeros_like(audio)
    
    # ä¿å­˜ç»“æœ
    output_dir = "vae_hifigan_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    audio_normalized = audio / (np.max(np.abs(audio)) + 1e-8)
    torchaudio.save(original_path, torch.from_numpy(audio_normalized).unsqueeze(0), sample_rate)
    
    # ä¿å­˜é‡å»ºéŸ³é¢‘
    reconstructed_path = os.path.join(output_dir, f"{input_name}_reconstructed_{vocoder_method}_{timestamp}.wav")
    recon_normalized = reconstructed_audio / (np.max(np.abs(reconstructed_audio)) + 1e-8)
    
    # ç¡®ä¿é•¿åº¦ä¸€è‡´
    if len(recon_normalized) > len(audio):
        recon_normalized = recon_normalized[:len(audio)]
    elif len(recon_normalized) < len(audio):
        recon_normalized = np.pad(recon_normalized, (0, len(audio) - len(recon_normalized)))
    
    torchaudio.save(reconstructed_path, torch.from_numpy(recon_normalized).unsqueeze(0), sample_rate)
    
    # è®¡ç®—æŒ‡æ ‡
    min_len = min(len(audio), len(recon_normalized))
    orig = audio[:min_len]
    recon = recon_normalized[:min_len]
    
    mse = np.mean((orig - recon) ** 2)
    correlation = np.corrcoef(orig, recon)[0, 1] if len(orig) > 1 else 0
    signal_power = np.mean(orig ** 2)
    noise_power = np.mean((orig - recon) ** 2)
    snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
    
    # å‹ç¼©æ¯”
    compression_ratio = mel_input.numel() / latents.numel()
    
    # è¾“å‡ºç»“æœ
    print(f"\n{'='*60}")
    print(f"ğŸ¯ AudioLDM2 VAE + HiFiGAN æµ‹è¯•ç»“æœ")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"â±ï¸ ç¼–ç æ—¶é—´: {encode_time:.3f}ç§’")
    print(f"â±ï¸ è§£ç æ—¶é—´: {decode_time:.3f}ç§’")
    print(f"â±ï¸ æ€»æ—¶é—´: {encode_time + decode_time:.3f}ç§’")
    print(f"ğŸ“Š MSE: {mse:.6f}")
    print(f"ğŸ“Š SNR: {snr:.2f} dB")
    print(f"ğŸ“Š ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"ğŸµ Melé¢‘è°±å½¢çŠ¶: {mel_input.shape}")
    print(f"ğŸ—œï¸ æ½œåœ¨è¡¨ç¤ºå½¢çŠ¶: {latents.shape}")
    print(f"ğŸ“¦ å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
    print(f"ğŸ¤ é‡å»ºæ–¹æ³•: {vocoder_method}")
    
    # åˆ†æç»“æœ
    print(f"\nğŸ’¡ ç»“æœåˆ†æ:")
    if vocoder_method == "AudioLDM2_HiFiGAN":
        print("âœ… æˆåŠŸä½¿ç”¨AudioLDM2å†…ç½®HiFiGANï¼")
        if snr > 0:
            print("ğŸ‰ é‡å»ºè´¨é‡è‰¯å¥½")
        elif snr > -5:
            print("âš ï¸ é‡å»ºè´¨é‡ä¸€èˆ¬ï¼Œä½†å¯è¯†åˆ«")
        else:
            print("âŒ é‡å»ºè´¨é‡è¾ƒå·®")
    else:
        print(f"âš ï¸ ä½¿ç”¨äº†é™çº§æ–¹æ³•: {vocoder_method}")
    
    return {
        'original_path': original_path,
        'reconstructed_path': reconstructed_path,
        'mse': mse,
        'snr': snr,
        'correlation': correlation,
        'encode_time': encode_time,
        'decode_time': decode_time,
        'compression_ratio': compression_ratio,
        'vocoder_method': vocoder_method
    }

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python vae_hifigan_fixed.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„> [æœ€å¤§é•¿åº¦ç§’æ•°]")
        
        # æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
        audio_files = []
        for ext in ['.wav', '.mp3', '.flac']:
            audio_files.extend(Path('.').glob(f'*{ext}'))
        
        if audio_files:
            print(f"\næ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files, 1):
                print(f"{i}. {file}")
            
            try:
                choice = input("è¯·é€‰æ‹©æ–‡ä»¶åºå·: ").strip()
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(audio_files):
                    audio_path = str(audio_files[file_idx])
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")
                    return
            except (ValueError, KeyboardInterrupt):
                print("âŒ å–æ¶ˆæ“ä½œ")
                return
        else:
            print("âŒ å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    # è·å–æœ€å¤§é•¿åº¦
    max_length = 5  # é»˜è®¤5ç§’
    if len(sys.argv) >= 3:
        try:
            max_length = float(sys.argv[2])
        except ValueError:
            print(f"âš ï¸ æ— æ•ˆé•¿åº¦å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {max_length} ç§’")
    
    # æ‰§è¡Œæµ‹è¯•
    print(f"ğŸš€ å¼€å§‹æµ‹è¯•: {audio_path}")
    print(f"â±ï¸ æœ€å¤§é•¿åº¦: {max_length} ç§’")
    
    try:
        result = load_and_test_vae_fixed(audio_path, max_length=max_length)
        if result:
            print("\nâœ… æµ‹è¯•å®Œæˆï¼")
            print("ğŸ§ è¯·æ’­æ”¾åŸå§‹éŸ³é¢‘å’Œé‡å»ºéŸ³é¢‘æ¥æ¯”è¾ƒè´¨é‡")
            if result['vocoder_method'] == "AudioLDM2_HiFiGAN":
                print("ğŸ‰ æˆåŠŸçªç ´äº†Griffin-Limç“¶é¢ˆï¼Œä½¿ç”¨äº†ç¥ç»vocoderï¼")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
