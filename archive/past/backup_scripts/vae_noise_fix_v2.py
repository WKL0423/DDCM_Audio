#!/usr/bin/env python3
"""
AudioLDM2 VAE é‡å»ºæµ‹è¯• - å™ªéŸ³ä¿®å¤ç‰ˆæœ¬ v2
ä¸»è¦è§£å†³HiFiGANç”Ÿæˆçš„"å’”å“’å’”å“’"å™ªéŸ³é—®é¢˜

é‡ç‚¹ä¿®å¤:
1. VAEè§£ç ç»“æœçš„æ•°å€¼ç¨³å®šæ€§
2. HiFiGANè¾“å…¥å½’ä¸€åŒ–ä¼˜åŒ–
3. åå¤„ç†é™å™ªç­–ç•¥
"""

import torch
import torchaudio
import numpy as np
import librosa
import time
import os
import sys
from pathlib import Path
from diffusers import AudioLDM2Pipeline
import scipy.signal
from scipy.ndimage import gaussian_filter1d


def safe_normalize_mel(mel_tensor, target_mean=-5.0, target_std=5.0):
    """å®‰å…¨çš„mel-spectrogramå½’ä¸€åŒ–ï¼Œé¿å…æ•°å€¼é—®é¢˜"""
    if isinstance(mel_tensor, torch.Tensor):
        mel_np = mel_tensor.detach().cpu().numpy().astype(np.float64)  # ä½¿ç”¨float64é¿å…æº¢å‡º
    else:
        mel_np = mel_tensor.astype(np.float64)
    
    # æ£€æŸ¥å¹¶ä¿®å¤æ— æ•ˆå€¼
    if not np.isfinite(mel_np).all():
        print(f"   âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆå€¼ï¼Œè¿›è¡Œä¿®å¤...")
        mel_np = np.nan_to_num(mel_np, nan=target_mean, posinf=0.0, neginf=-50.0)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    mean_val = np.mean(mel_np)
    std_val = np.std(mel_np)
    
    print(f"   å½’ä¸€åŒ–å‰: mean={mean_val:.3f}, std={std_val:.3f}")
    
    # å®‰å…¨çš„æ ‡å‡†åŒ–
    if np.isfinite(std_val) and std_val > 1e-6:
        # æ ‡å‡†æ ‡å‡†åŒ–
        normalized = (mel_np - mean_val) / std_val
        normalized = normalized * target_std + target_mean
    else:
        # ç®€å•çš„çº¿æ€§æ˜ å°„
        min_val = np.min(mel_np)
        max_val = np.max(mel_np)
        if max_val > min_val:
            normalized = (mel_np - min_val) / (max_val - min_val)
            normalized = normalized * (2 * target_std) + (target_mean - target_std)
        else:
            normalized = np.full_like(mel_np, target_mean)
    
    # è£å‰ªåˆ°åˆç†èŒƒå›´
    normalized = np.clip(normalized, target_mean - 3*target_std, target_mean + 3*target_std)
    
    # æœ€ç»ˆæ£€æŸ¥
    normalized = np.nan_to_num(normalized, nan=target_mean, posinf=target_mean + target_std, neginf=target_mean - target_std)
    
    final_mean = np.mean(normalized)
    final_std = np.std(normalized)
    print(f"   å½’ä¸€åŒ–å: mean={final_mean:.3f}, std={final_std:.3f}")
    
    # è½¬æ¢å›é€‚å½“çš„ç²¾åº¦
    normalized = normalized.astype(np.float32)
    
    if isinstance(mel_tensor, torch.Tensor):
        return torch.from_numpy(normalized).to(mel_tensor.device).to(torch.float32)  # å¼ºåˆ¶ä½¿ç”¨float32
    else:
        return normalized


def apply_audio_postprocessing(audio, sr=16000, fade_samples=1024):
    """åº”ç”¨éŸ³é¢‘åå¤„ç†ä»¥å‡å°‘å™ªéŸ³"""
    if len(audio) == 0:
        return audio
    
    # 1. ç§»é™¤ç›´æµåç½®
    audio = audio - np.mean(audio)
    
    # 2. æ¸å˜æ·¡å…¥æ·¡å‡ºï¼ˆå‡å°‘ç‚¹å‡»ï¼‰
    if len(audio) > 2 * fade_samples:
        # æ·¡å…¥
        fade_in = np.linspace(0, 1, fade_samples)
        audio[:fade_samples] *= fade_in
        
        # æ·¡å‡º
        fade_out = np.linspace(1, 0, fade_samples)
        audio[-fade_samples:] *= fade_out
    
    # 3. è½»å¾®å¹³æ»‘æ»¤æ³¢ï¼ˆå‡å°‘é«˜é¢‘å™ªéŸ³ï¼‰
    audio = gaussian_filter1d(audio, sigma=0.8)
    
    # 4. å¸¦é€šæ»¤æ³¢ï¼ˆç§»é™¤æä½å’Œæé«˜é¢‘ç‡ï¼‰
    try:
        nyquist = sr / 2
        low = 80 / nyquist    # 80Hzé«˜é€š
        high = 7500 / nyquist # 7.5kHzä½é€š
        b, a = scipy.signal.butter(3, [low, high], btype='band')
        audio = scipy.signal.filtfilt(b, a, audio)
    except:
        print(f"   âš ï¸ æ»¤æ³¢å™¨åº”ç”¨å¤±è´¥ï¼Œè·³è¿‡...")
    
    # 5. è½»å¾®å‹ç¼©ï¼ˆå‡å°‘å³°å€¼ï¼‰
    threshold = 0.8
    audio = np.where(np.abs(audio) > threshold, 
                     np.sign(audio) * (threshold + (np.abs(audio) - threshold) * 0.5),
                     audio)
    
    return audio


def load_and_test_vae_v2(audio_path, max_length=10):
    """VAEé‡å»ºæµ‹è¯• - ç¨³å®šç‰ˆæœ¬"""
    print(f"\nğŸµ å¼€å§‹VAEé‡å»ºæµ‹è¯• v2")
    print(f"éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    print(f"æœ€å¤§é•¿åº¦: {max_length} ç§’")
    
    # è®¾å¤‡è®¾ç½®
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½ AudioLDM2 æ¨¡å‹...")
    try:
        pipeline = AudioLDM2Pipeline.from_pretrained(
            "cvssp/audioldm2",
            torch_dtype=torch.float16 if device == "cuda" else torch.float32
        ).to(device)
        
        vae = pipeline.vae
        vocoder = pipeline.vocoder
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: VAE={type(vae).__name__}, Vocoder={type(vocoder).__name__}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None
    
    # åŠ è½½éŸ³é¢‘
    print(f"ğŸ¶ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
    try:
        sample_rate = 16000
        audio, orig_sr = torchaudio.load(audio_path)
        
        if orig_sr != sample_rate:
            audio = torchaudio.functional.resample(audio, orig_sr, sample_rate)
        
        if audio.shape[0] > 1:
            audio = torch.mean(audio, dim=0, keepdim=True)
        
        audio = audio.squeeze().numpy()
        
        # æˆªå–
        max_samples = int(max_length * sample_rate)
        if len(audio) > max_samples:
            audio = audio[:max_samples]
        
        # é¢„å¤„ç†
        audio = audio - np.mean(audio)  # ç§»é™¤ç›´æµåç½®
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.95  # è½»å¾®é™ä½å¹…åº¦
        
        print(f"   éŸ³é¢‘é•¿åº¦: {len(audio)} æ ·æœ¬ ({len(audio)/sample_rate:.2f} ç§’)")
        
    except Exception as e:
        print(f"âŒ éŸ³é¢‘åŠ è½½å¤±è´¥: {e}")
        return None
    
    # ç”Ÿæˆmel-spectrogram
    print("ğŸ”„ ç”Ÿæˆ mel-spectrogram...")
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sample_rate,
        n_fft=1024,
        hop_length=160,
        n_mels=64,
        fmin=0,
        fmax=8000
    )
    
    # è½¬æ¢ä¸ºdB
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max, top_db=80)
    mel_spec_db = np.clip(mel_spec_db, -80, 0)
    
    print(f"   Melå½¢çŠ¶: {mel_spec_db.shape}")
    print(f"   MelèŒƒå›´: [{np.min(mel_spec_db):.2f}, {np.max(mel_spec_db):.2f}] dB")
    
    # VAEå¤„ç†
    with torch.no_grad():
        # å‡†å¤‡VAEè¾“å…¥
        mel_tensor = torch.from_numpy(mel_spec_db).to(device).to(vae.dtype)
        mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, time]
        
        # å¡«å……åˆ°8çš„å€æ•°
        time_dim = mel_input.shape[-1]
        if time_dim % 8 != 0:
            pad_width = 8 - (time_dim % 8)
            mel_input = torch.nn.functional.pad(mel_input, (0, pad_width), mode='constant', value=-80)
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        mel_min, mel_max = -80.0, 0.0
        mel_normalized = 2.0 * (mel_input - mel_min) / (mel_max - mel_min) - 1.0
        mel_normalized = torch.clamp(mel_normalized, -1, 1)
        
        print(f"   VAEè¾“å…¥å½¢çŠ¶: {mel_normalized.shape}")
        
        # VAEç¼–ç 
        print("ğŸ”§ VAE ç¼–ç ...")
        start_time = time.time()
        
        latents = vae.encode(mel_normalized).latent_dist.sample()
        latents = latents * vae.config.scaling_factor
        
        encode_time = time.time() - start_time
        print(f"   ç¼–ç å®Œæˆ: {encode_time:.2f}ç§’, æ½œåœ¨å½¢çŠ¶: {latents.shape}")
        
        # VAEè§£ç 
        print("ğŸ”§ VAE è§£ç ...")
        decode_start = time.time()
        
        latents_scaled = latents / vae.config.scaling_factor
        reconstructed_mel = vae.decode(latents_scaled).sample
        
        decode_time = time.time() - decode_start
        print(f"   è§£ç å®Œæˆ: {decode_time:.2f}ç§’, é‡å»ºmelå½¢çŠ¶: {reconstructed_mel.shape}")
        
        # æ£€æŸ¥è§£ç ç»“æœ
        print(f"   é‡å»ºmelèŒƒå›´: [{reconstructed_mel.min().item():.3f}, {reconstructed_mel.max().item():.3f}]")
    
    # éŸ³é¢‘é‡å»º
    print("ğŸ”Š éŸ³é¢‘é‡å»º (æ”¹è¿›ç‰ˆæœ¬)...")
    
    try:
        with torch.no_grad():
            # åå½’ä¸€åŒ–mel
            mel_denorm = (reconstructed_mel + 1) / 2 * (mel_max - mel_min) + mel_min
            mel_denorm = torch.clamp(mel_denorm, mel_min, mel_max)
            
            # æ£€æŸ¥æœ‰æ•ˆæ€§
            if not torch.isfinite(mel_denorm).all():
                print(f"   âš ï¸ ä¿®å¤è§£ç è¾“å‡ºçš„æ— æ•ˆå€¼...")
                mel_denorm = torch.nan_to_num(mel_denorm, nan=-40.0, posinf=-20.0, neginf=-80.0)
            
            # å‡†å¤‡vocoderè¾“å…¥
            vocoder_input = mel_denorm.squeeze(0).squeeze(0)  # [64, time]
            vocoder_input = vocoder_input.transpose(-2, -1)   # [time, 64]
            vocoder_input = vocoder_input.unsqueeze(0)        # [1, time, 64]
            
            print(f"   Vocoderè¾“å…¥å½¢çŠ¶: {vocoder_input.shape}")
            print(f"   Vocoderè¾“å…¥èŒƒå›´: [{vocoder_input.min().item():.3f}, {vocoder_input.max().item():.3f}]")
            
            # å®‰å…¨å½’ä¸€åŒ–
            vocoder_input_norm = safe_normalize_mel(vocoder_input, target_mean=-5.0, target_std=4.0)
            
            # ä½¿ç”¨vocoder
            reconstructed_audio_tensor = vocoder(vocoder_input_norm)
            reconstructed_audio = reconstructed_audio_tensor.squeeze().cpu().numpy()
            
            print(f"   âœ… VocoderæˆåŠŸ: è¾“å‡º{len(reconstructed_audio)}æ ·æœ¬")
            
            # åº”ç”¨åå¤„ç†
            print("ğŸ› ï¸ åº”ç”¨åå¤„ç†é™å™ª...")
            reconstructed_audio = apply_audio_postprocessing(reconstructed_audio, sr=sample_rate)
            
            vocoder_method = "AudioLDM2_HiFiGAN_v2_NoiseFixed"
            
    except Exception as e:
        print(f"   âŒ HiFiGANå¤±è´¥: {e}")
        print("   ğŸ“Š é™çº§åˆ°Griffin-Lim...")
        
        # Griffin-Limé™çº§
        try:
            recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
            recon_mel_denorm = (recon_mel_np + 1) / 2 * (mel_max - mel_min) + mel_min
            recon_mel_denorm = np.clip(recon_mel_denorm, -80.0, 0.0)
            
            recon_mel_power = librosa.db_to_power(recon_mel_denorm)
            recon_mel_power = np.clip(recon_mel_power, 1e-10, None)
            
            reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                recon_mel_power,
                sr=sample_rate,
                hop_length=160,
                n_fft=1024,
                fmin=0,
                fmax=8000
            )
            
            reconstructed_audio = apply_audio_postprocessing(reconstructed_audio, sr=sample_rate)
            vocoder_method = "Griffin_Lim_PostProcessed"
            
        except Exception as griffin_e:
            print(f"   âŒ Griffin-Limä¹Ÿå¤±è´¥: {griffin_e}")
            reconstructed_audio = np.random.randn(len(audio)) * 0.01
            vocoder_method = "Fallback"
    
    # é•¿åº¦å¯¹é½
    if len(reconstructed_audio) > len(audio):
        reconstructed_audio = reconstructed_audio[:len(audio)]
    elif len(reconstructed_audio) < len(audio):
        reconstructed_audio = np.pad(reconstructed_audio, (0, len(audio) - len(reconstructed_audio)))
    
    # ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    output_dir = "vae_noise_fix_v2_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    reconstructed_path = os.path.join(output_dir, f"{input_name}_reconstructed_v2_{timestamp}.wav")
    
    # å½’ä¸€åŒ–ä¿å­˜
    audio_norm = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio
    recon_norm = reconstructed_audio / np.max(np.abs(reconstructed_audio)) if np.max(np.abs(reconstructed_audio)) > 0 else reconstructed_audio
    
    torchaudio.save(original_path, torch.from_numpy(audio_norm).unsqueeze(0), sample_rate)
    torchaudio.save(reconstructed_path, torch.from_numpy(recon_norm).unsqueeze(0), sample_rate)
    
    # è®¡ç®—æŒ‡æ ‡
    print("ğŸ“Š è®¡ç®—è´¨é‡æŒ‡æ ‡...")
    min_len = min(len(audio), len(reconstructed_audio))
    orig_segment = audio[:min_len]
    recon_segment = reconstructed_audio[:min_len]
    
    mse = np.mean((orig_segment - recon_segment) ** 2)
    correlation = np.corrcoef(orig_segment, recon_segment)[0, 1] if len(orig_segment) > 1 else 0
    
    signal_power = np.mean(orig_segment ** 2)
    noise_power = np.mean((orig_segment - recon_segment) ** 2)
    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
    
    original_size = mel_normalized.numel()
    compressed_size = latents.numel()
    compression_ratio = original_size / compressed_size
    
    # æ‰“å°ç»“æœ
    print(f"\n{'='*60}")
    print(f"VAE é‡å»ºæµ‹è¯•ç»“æœ - å™ªéŸ³ä¿®å¤ v2")
    print(f"{'='*60}")
    print(f"åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ç¼–ç æ—¶é—´: {encode_time:.2f}ç§’")
    print(f"è§£ç æ—¶é—´: {decode_time:.2f}ç§’")
    print(f"æ€»æ—¶é—´: {encode_time + decode_time:.2f}ç§’")
    print(f"MSE: {mse:.6f}")
    print(f"SNR: {snr:.2f} dB")
    print(f"ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
    print(f"é‡å»ºæ–¹æ³•: {vocoder_method}")
    print(f"âœ… æ”¹è¿›é¡¹ç›®: æ•°å€¼ç¨³å®šæ€§ã€åå¤„ç†é™å™ªã€æ¸å˜å¤„ç†")
    
    return {
        'original_path': original_path,
        'reconstructed_path': reconstructed_path,
        'mse': mse,
        'snr': snr,
        'correlation': correlation,
        'encode_time': encode_time,
        'decode_time': decode_time,
        'compression_ratio': compression_ratio,
        'vocoder_method': vocoder_method
    }


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python vae_noise_fix_v2.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„> [æœ€å¤§é•¿åº¦ç§’æ•°]")
        
        audio_files = []
        for ext in ['.wav', '.mp3', '.flac']:
            audio_files.extend(Path('.').glob(f'*{ext}'))
        
        if audio_files:
            print(f"\næ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files, 1):
                print(f"{i}. {file}")
            
            try:
                choice = input("è¯·é€‰æ‹©æ–‡ä»¶åºå·: ").strip()
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(audio_files):
                    audio_path = str(audio_files[file_idx])
                else:
                    print("æ— æ•ˆé€‰æ‹©")
                    return
            except (ValueError, KeyboardInterrupt):
                print("å–æ¶ˆæ“ä½œ")
                return
        else:
            print("å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    max_length = 10
    if len(sys.argv) >= 3:
        try:
            max_length = float(sys.argv[2])
        except ValueError:
            print(f"æ— æ•ˆçš„é•¿åº¦å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {max_length} ç§’")
    
    print(f"ğŸš€ å¼€å§‹å™ªéŸ³ä¿®å¤æµ‹è¯• v2")
    
    try:
        result = load_and_test_vae_v2(audio_path, max_length=max_length)
        if result:
            print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
            print(f"è¯·æ¯”è¾ƒéŸ³é¢‘è´¨é‡ï¼Œæ£€æŸ¥å™ªéŸ³æ˜¯å¦å‡å°‘ã€‚")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
