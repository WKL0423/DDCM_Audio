"""
AudioLDM2 VAE ç»ˆææ”¹è¿›ç‰ˆæœ¬
å°è¯•æ‰¾åˆ°AudioLDM2çš„æœ€ä½³é‡å»ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨å†…ç½®vocoder

è¿™ä¸ªç‰ˆæœ¬ä¸“æ³¨äºï¼š
1. ç†è§£AudioLDM2çš„çœŸå®å·¥ä½œæµç¨‹
2. å°è¯•é€†å‘å·¥ç¨‹AudioLDM2çš„melå¤„ç†
3. ä½¿ç”¨æœ€æ¥è¿‘è®­ç»ƒæ—¶çš„å‚æ•°
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F

from diffusers import AudioLDM2Pipeline


def ultimate_vae_test(audio_path, model_id="cvssp/audioldm2-music", max_length=10):
    """
    ç»ˆæVAEæµ‹è¯• - å°½å¯èƒ½æ¥è¿‘AudioLDM2çš„åŸå§‹å·¥ä½œæµç¨‹
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    if not os.path.exists(audio_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ {audio_path}")
        return
    
    print(f"æ­£åœ¨åŠ è½½ AudioLDM2 æ¨¡å‹: {model_id}")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    # æ¢ç´¢vocoderçš„å±æ€§
    print(f"\\nğŸ“Š Vocoder ä¿¡æ¯:")
    print(f"  ç±»å‹: {type(vocoder).__name__}")
    print(f"  æ˜¯å¦æœ‰decodeæ–¹æ³•: {hasattr(vocoder, 'decode')}")
    print(f"  æ˜¯å¦å¯è°ƒç”¨: {hasattr(vocoder, '__call__')}")
    if hasattr(vocoder, 'config'):
        print(f"  Vocoderé…ç½®: {vocoder.config}")
    
    # æ¢ç´¢VAEçš„å±æ€§
    print(f"\\nğŸ”§ VAE ä¿¡æ¯:")
    print(f"  ç±»å‹: {type(vae).__name__}")
    if hasattr(vae, 'config'):
        config = vae.config
        print(f"  è¾“å…¥channels: {getattr(config, 'in_channels', 'unknown')}")
        print(f"  è¾“å‡ºchannels: {getattr(config, 'out_channels', 'unknown')}")
        print(f"  æ½œåœ¨ç»´åº¦: {getattr(config, 'latent_channels', 'unknown')}")
        print(f"  ç¼©æ”¾å› å­: {getattr(config, 'scaling_factor', 'unknown')}")
    
    print(f"\\næ­£åœ¨åŠ è½½éŸ³é¢‘: {audio_path}")
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    
    if len(audio) > max_length * sample_rate:
        audio = audio[:int(max_length * sample_rate)]
        print(f"éŸ³é¢‘å·²è£å‰ªåˆ° {max_length} ç§’")
    
    print(f"éŸ³é¢‘ä¿¡æ¯: é•¿åº¦={len(audio)/sample_rate:.2f}ç§’, æ ·æœ¬æ•°={len(audio)}")
    
    # æ–¹æ³•1: ç ”ç©¶AudioLDM2çš„æºç æ‰¾åˆ°æ­£ç¡®çš„melå‚æ•°
    print("\\n=== æ–¹æ³•1: ç ”ç©¶AudioLDM2å†…éƒ¨melå‚æ•° ===")
    result1 = test_with_audioldm2_internals(audio, pipeline, device, sample_rate)
    
    # æ–¹æ³•2: æ¨¡æ‹ŸAudioLDM2çš„è®­ç»ƒé¢„å¤„ç†
    print("\\n=== æ–¹æ³•2: æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„é¢„å¤„ç† ===")
    result2 = test_with_training_simulation(audio, vae, vocoder, device, sample_rate)
    
    # æ–¹æ³•3: å°è¯•æ›´å¥½çš„åå¤„ç†
    print("\\n=== æ–¹æ³•3: æ”¹è¿›çš„åå¤„ç† ===")
    result3 = test_with_improved_postprocessing(audio, vae, device, sample_rate)
    
    # ä¿å­˜ç»“æœ
    output_dir = "vae_ultimate_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    audio_save = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio
    torchaudio.save(original_path, torch.from_numpy(audio_save).unsqueeze(0), sample_rate)
    
    # åˆ†æå’Œä¿å­˜ç»“æœ
    results = {}
    method_names = ["AudioLDM2å†…éƒ¨", "è®­ç»ƒæ¨¡æ‹Ÿ", "æ”¹è¿›åå¤„ç†"]
    
    for i, (method_name, result) in enumerate(zip(method_names, [result1, result2, result3]), 1):
        if result is not None and result.get('audio') is not None:
            recon_path = os.path.join(output_dir, f"{input_name}_ultimate_method{i}_{method_name}_{timestamp}.wav")
            
            recon_audio = result['audio']
            if len(recon_audio) > 0 and np.max(np.abs(recon_audio)) > 0:
                recon_audio = recon_audio / np.max(np.abs(recon_audio))
            
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            if len(recon_audio) > len(audio):
                recon_audio = recon_audio[:len(audio)]
            elif len(recon_audio) < len(audio):
                recon_audio = np.pad(recon_audio, (0, len(audio) - len(recon_audio)))
            
            torchaudio.save(recon_path, torch.from_numpy(recon_audio).unsqueeze(0), sample_rate)
            
            # è®¡ç®—è¯¦ç»†è´¨é‡æŒ‡æ ‡
            metrics = calculate_comprehensive_metrics(audio, recon_audio, sample_rate)
            
            results[f"æ–¹æ³•{i}_{method_name}"] = {
                'path': recon_path,
                'metrics': metrics,
                'processing_time': result.get('processing_time', 0),
                'compression_info': result.get('compression_info', {})
            }
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print_comprehensive_results(original_path, results)
    
    return results


def test_with_audioldm2_internals(audio, pipeline, device, sample_rate):
    """å°è¯•æ‰¾åˆ°AudioLDM2å†…éƒ¨çš„melå¤„ç†æ–¹æ³•"""
    try:
        print("æ¢ç´¢AudioLDM2çš„å†…éƒ¨melå¤„ç†...")
        
        vae = pipeline.vae
        
        # å°è¯•æ‰¾åˆ°æ­£ç¡®çš„melå‚æ•°
        # è¿™äº›å‚æ•°åŸºäºAudioLDM2è®ºæ–‡å’Œå¸¸è§é…ç½®
        mel_params = {
            'n_fft': 1024,
            'hop_length': 160,
            'win_length': 1024,
            'n_mels': 64,
            'fmin': 0,
            'fmax': 8000,
            'power': 1.0,  # ä½¿ç”¨å¹…åº¦è€Œä¸æ˜¯åŠŸç‡
            'norm': 'slaney',
            'htk': False
        }
        
        start_time = time.time()
        
        # è®¡ç®—melè°±
        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, **mel_params)
        
        # è½¬æ¢ä¸ºå¯¹æ•°å°ºåº¦ - ä½¿ç”¨AudioLDM2é£æ ¼
        mel_spec_log = np.log(np.clip(mel_spec, a_min=1e-5, a_max=None))
        
        # ä½¿ç”¨æ›´æ¥è¿‘AudioLDM2è®­ç»ƒçš„å½’ä¸€åŒ–
        # åŸºäºç ”ç©¶ï¼ŒAudioLDM2å¯èƒ½ä½¿ç”¨è¿™ç§å½’ä¸€åŒ–æ–¹å¼
        mel_mean = -4.0  # å‡è®¾çš„å‡å€¼
        mel_std = 4.0    # å‡è®¾çš„æ ‡å‡†å·®
        mel_spec_normalized = (mel_spec_log - mel_mean) / mel_std
        mel_spec_normalized = np.clip(mel_spec_normalized, -3, 3)
        
        # è½¬æ¢ä¸ºå¼ é‡
        mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.to(torch.float16)
        
        mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)
        
        # ç¡®ä¿å°ºå¯¸å…¼å®¹
        if mel_input.shape[-1] % 8 != 0:
            pad_width = 8 - (mel_input.shape[-1] % 8)
            mel_input = F.pad(mel_input, (0, pad_width))
        
        with torch.no_grad():
            # VAEå¤„ç†
            latents = vae.encode(mel_input).latent_dist.sample()
            latents = latents * vae.config.scaling_factor
            latents = latents / vae.config.scaling_factor
            reconstructed_mel = vae.decode(latents).sample
            
            # åå½’ä¸€åŒ–
            recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
            recon_mel_denorm = recon_mel_np * mel_std + mel_mean
            
            # è½¬å›çº¿æ€§å°ºåº¦
            recon_mel_linear = np.exp(recon_mel_denorm)
            
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„Griffin-Limå‚æ•°
            reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                recon_mel_linear,
                sr=sample_rate,
                n_fft=mel_params['n_fft'],
                hop_length=mel_params['hop_length'],
                win_length=mel_params['win_length'],
                fmin=mel_params['fmin'],
                fmax=mel_params['fmax'],
                n_iter=100,  # æ›´å¤šè¿­ä»£
                window='hann',
                length=len(audio)
            )
        
        processing_time = time.time() - start_time
        
        return {
            'audio': reconstructed_audio,
            'processing_time': processing_time,
            'compression_info': {
                'original_size': mel_input.numel(),
                'compressed_size': latents.numel(),
                'compression_ratio': mel_input.numel() / latents.numel()
            }
        }
        
    except Exception as e:
        print(f"AudioLDM2å†…éƒ¨æ–¹æ³•å¤±è´¥: {e}")
        return None


def test_with_training_simulation(audio, vae, vocoder, device, sample_rate):
    """æ¨¡æ‹ŸAudioLDM2è®­ç»ƒæ—¶çš„æ•°æ®å¤„ç†"""
    try:
        print("æ¨¡æ‹ŸAudioLDM2è®­ç»ƒæ—¶çš„æ•°æ®é¢„å¤„ç†...")
        
        start_time = time.time()
        
        # ä½¿ç”¨HiFi-GANé£æ ¼çš„melå‚æ•°ï¼ˆAudioLDM2å¯èƒ½åŸºäºæ­¤ï¼‰
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sample_rate,
            n_fft=1024,
            hop_length=256,  # å°è¯•ä¸åŒçš„hop length
            win_length=1024,
            n_mels=80,
            fmin=0,
            fmax=8000,
            power=1.0
        )
        
        # è½¬æ¢ä¸ºåˆ†è´å¹¶ä½¿ç”¨HiFi-GANé£æ ¼çš„å½’ä¸€åŒ–
        mel_spec_db = 20 * np.log10(np.maximum(mel_spec, 1e-5))
        mel_spec_normalized = (mel_spec_db + 100) / 100  # [0, 1] èŒƒå›´
        mel_spec_normalized = mel_spec_normalized * 2 - 1  # [-1, 1] èŒƒå›´
        
        # è°ƒæ•´åˆ°64 mels
        if mel_spec_normalized.shape[0] != 64:
            from scipy.interpolate import interp1d
            old_indices = np.linspace(0, 1, mel_spec_normalized.shape[0])
            new_indices = np.linspace(0, 1, 64)
            interpolator = interp1d(old_indices, mel_spec_normalized, axis=0, kind='linear')
            mel_spec_normalized = interpolator(new_indices)
        
        # è°ƒæ•´æ—¶é—´ç»´åº¦åˆ°160 hop lengthçš„ç­‰æ•ˆ
        target_time_frames = len(audio) // 160
        if mel_spec_normalized.shape[1] != target_time_frames:
            from scipy.interpolate import interp1d
            old_indices = np.linspace(0, 1, mel_spec_normalized.shape[1])
            new_indices = np.linspace(0, 1, target_time_frames)
            interpolator = interp1d(old_indices, mel_spec_normalized, axis=1, kind='linear')
            mel_spec_normalized = interpolator(new_indices)
        
        # VAEå¤„ç†
        mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.to(torch.float16)
        
        mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)
        
        # å¡«å……åˆ°8çš„å€æ•°
        if mel_input.shape[-1] % 8 != 0:
            pad_width = 8 - (mel_input.shape[-1] % 8)
            mel_input = F.pad(mel_input, (0, pad_width))
        
        with torch.no_grad():
            latents = vae.encode(mel_input).latent_dist.sample()
            latents = latents * vae.config.scaling_factor
            latents = latents / vae.config.scaling_factor
            reconstructed_mel = vae.decode(latents).sample
            
            # åå½’ä¸€åŒ–
            recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
            recon_mel_db = (recon_mel_np + 1) / 2 * 100 - 100
            recon_mel_linear = np.power(10, recon_mel_db / 20)
            
            # Griffin-Limé‡å»º
            reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                recon_mel_linear,
                sr=sample_rate,
                n_fft=1024,
                hop_length=160,
                win_length=1024,
                fmin=0,
                fmax=8000,
                n_iter=200,  # æ›´å¤šè¿­ä»£ä»¥è·å¾—æ›´å¥½è´¨é‡
                momentum=0.99,
                length=len(audio)
            )
        
        processing_time = time.time() - start_time
        
        return {
            'audio': reconstructed_audio,
            'processing_time': processing_time,
            'compression_info': {
                'original_size': mel_input.numel(),
                'compressed_size': latents.numel(),
                'compression_ratio': mel_input.numel() / latents.numel()
            }
        }
        
    except Exception as e:
        print(f"è®­ç»ƒæ¨¡æ‹Ÿæ–¹æ³•å¤±è´¥: {e}")
        return None


def test_with_improved_postprocessing(audio, vae, device, sample_rate):
    """ä½¿ç”¨æ”¹è¿›çš„åå¤„ç†æ–¹æ³•"""
    try:
        print("ä½¿ç”¨æ”¹è¿›çš„åå¤„ç†é‡å»ºæ–¹æ³•...")
        
        start_time = time.time()
        
        # ä½¿ç”¨æ ‡å‡†melå‚æ•°
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sample_rate,
            n_fft=1024,
            hop_length=160,
            n_mels=64,
            fmin=0,
            fmax=8000
        )
        
        # æ›´ç¨³å®šçš„å¯¹æ•°è½¬æ¢
        mel_spec_log = np.log(mel_spec + 1e-8)
        
        # ä½¿ç”¨ç™¾åˆ†ä½æ•°å½’ä¸€åŒ– - æ›´ç¨³å¥
        p1, p99 = np.percentile(mel_spec_log, [1, 99])
        mel_spec_normalized = 2 * (mel_spec_log - p1) / (p99 - p1) - 1
        mel_spec_normalized = np.clip(mel_spec_normalized, -1, 1)
        
        # VAEå¤„ç†
        mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.to(torch.float16)
        
        mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)
        
        if mel_input.shape[-1] % 8 != 0:
            pad_width = 8 - (mel_input.shape[-1] % 8)
            mel_input = F.pad(mel_input, (0, pad_width))
        
        with torch.no_grad():
            latents = vae.encode(mel_input).latent_dist.sample()
            latents = latents * vae.config.scaling_factor
            latents = latents / vae.config.scaling_factor
            reconstructed_mel = vae.decode(latents).sample
            
            # æ”¹è¿›çš„åå½’ä¸€åŒ–
            recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
            recon_mel_denorm = (recon_mel_np + 1) / 2 * (p99 - p1) + p1
            recon_mel_linear = np.exp(recon_mel_denorm)
            
            # å¤šæ­¥éª¤éŸ³é¢‘é‡å»º
            # 1. æ ‡å‡†Griffin-Lim
            audio_gl = librosa.feature.inverse.mel_to_audio(
                recon_mel_linear,
                sr=sample_rate,
                hop_length=160,
                n_fft=1024,
                n_iter=32
            )
            
            # 2. è¿›ä¸€æ­¥ä½¿ç”¨ISTFTä¼˜åŒ–
            # è®¡ç®—STFT
            D = librosa.stft(audio_gl, hop_length=160, n_fft=1024)
            
            # é‡å»ºmelå¹¶æ¯”è¾ƒ
            reconstructed_audio = librosa.istft(D, hop_length=160, length=len(audio))
            
            # 3. ç®€å•çš„åå¤„ç†æ»¤æ³¢
            from scipy.signal import savgol_filter
            if len(reconstructed_audio) > 51:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç‚¹è¿›è¡Œæ»¤æ³¢
                reconstructed_audio = savgol_filter(reconstructed_audio, 51, 3)
        
        processing_time = time.time() - start_time
        
        return {
            'audio': reconstructed_audio,
            'processing_time': processing_time,
            'compression_info': {
                'original_size': mel_input.numel(),
                'compressed_size': latents.numel(),
                'compression_ratio': mel_input.numel() / latents.numel()
            }
        }
        
    except Exception as e:
        print(f"æ”¹è¿›åå¤„ç†æ–¹æ³•å¤±è´¥: {e}")
        return None


def calculate_comprehensive_metrics(original, reconstructed, sample_rate):
    """è®¡ç®—å…¨é¢çš„è´¨é‡æŒ‡æ ‡"""
    min_len = min(len(original), len(reconstructed))
    orig = original[:min_len]
    recon = reconstructed[:min_len]
    
    # åŸºæœ¬æŒ‡æ ‡
    mse = np.mean((orig - recon) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(orig - recon))
    
    # ä¿¡å™ªæ¯”
    signal_power = np.mean(orig ** 2)
    noise_power = np.mean((orig - recon) ** 2)
    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
    
    # ç›¸å…³æ€§
    correlation = np.corrcoef(orig, recon)[0, 1] if len(orig) > 1 else 0
    
    # é¢‘è°±æŒ‡æ ‡
    orig_fft = np.abs(np.fft.fft(orig))
    recon_fft = np.abs(np.fft.fft(recon))
    spectral_correlation = np.corrcoef(orig_fft, recon_fft)[0, 1] if len(orig_fft) > 1 else 0
    
    # æ„ŸçŸ¥æŒ‡æ ‡
    try:
        orig_mfcc = librosa.feature.mfcc(y=orig, sr=sample_rate, n_mfcc=13)
        recon_mfcc = librosa.feature.mfcc(y=recon, sr=sample_rate, n_mfcc=13)
        mfcc_correlation = np.corrcoef(orig_mfcc.flatten(), recon_mfcc.flatten())[0, 1]
    except:
        mfcc_correlation = 0
    
    # é›¶äº¤å‰ç‡æ¯”è¾ƒ
    orig_zcr = librosa.feature.zero_crossing_rate(orig)[0].mean()
    recon_zcr = librosa.feature.zero_crossing_rate(recon)[0].mean()
    zcr_diff = abs(orig_zcr - recon_zcr) / (orig_zcr + 1e-8)
    
    return {
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'snr': snr,
        'correlation': correlation,
        'spectral_correlation': spectral_correlation,
        'mfcc_correlation': mfcc_correlation,
        'zcr_difference': zcr_diff
    }


def print_comprehensive_results(original_path, results):
    """æ‰“å°è¯¦ç»†çš„æµ‹è¯•ç»“æœ"""
    print(f"\\n{'='*70}")
    print(f"ç»ˆæVAEé‡å»ºæµ‹è¯•ç»“æœ")
    print(f"{'='*70}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print()
    
    if not results:
        print("âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†")
        return
    
    # æŒ‰SNRæ’åº
    sorted_results = sorted(results.items(), key=lambda x: x[1]['metrics']['snr'], reverse=True)
    
    print("ğŸ“Š å„æ–¹æ³•è¯¦ç»†å¯¹æ¯”:")
    print("-" * 70)
    
    for method_name, data in sorted_results:
        metrics = data['metrics']
        comp_info = data['compression_info']
        
        print(f"ğŸ”¬ {method_name}:")
        print(f"   ğŸ“„ æ–‡ä»¶: {os.path.basename(data['path'])}")
        print(f"   âš¡ å¤„ç†æ—¶é—´: {data['processing_time']:.2f}ç§’")
        print(f"   ğŸ“ å‹ç¼©æ¯”: {comp_info.get('compression_ratio', 0):.1f}:1")
        print(f"   ğŸ“ˆ è´¨é‡æŒ‡æ ‡:")
        print(f"      SNR: {metrics['snr']:.2f} dB")
        print(f"      ç›¸å…³ç³»æ•°: {metrics['correlation']:.4f}")
        print(f"      é¢‘è°±ç›¸å…³æ€§: {metrics['spectral_correlation']:.4f}")
        print(f"      MFCCç›¸å…³æ€§: {metrics['mfcc_correlation']:.4f}")
        print(f"      RMSE: {metrics['rmse']:.6f}")
        print(f"      é›¶äº¤å‰ç‡å·®å¼‚: {metrics['zcr_difference']:.4f}")
        print()
    
    # æ¨èæœ€ä½³æ–¹æ³•
    best_method = sorted_results[0]
    print(f"ğŸ† æ¨èæ–¹æ³•: {best_method[0]}")
    print(f"   ç»¼åˆå¾—åˆ†æœ€é«˜ (SNR: {best_method[1]['metrics']['snr']:.2f} dB)")
    
    print("\\nğŸ’¡ è´¨é‡æ”¹è¿›å»ºè®®:")
    best_snr = best_method[1]['metrics']['snr']
    if best_snr < 0:
        print("   - å½“å‰é‡å»ºè´¨é‡è¾ƒä½ï¼Œå»ºè®®å°è¯•:")
        print("     1. è°ƒæ•´mel-spectrogramå‚æ•°")
        print("     2. ä½¿ç”¨ä¸“é—¨çš„vocoderæ¨¡å‹")
        print("     3. è€ƒè™‘ç«¯åˆ°ç«¯çš„éŸ³é¢‘å‹ç¼©æ–¹æ³•")
    elif best_snr < 10:
        print("   - è´¨é‡ä¸­ç­‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–:")
        print("     1. ä¼˜åŒ–å½’ä¸€åŒ–æ–¹æ³•")
        print("     2. å¢åŠ Griffin-Limè¿­ä»£æ¬¡æ•°")
        print("     3. æ·»åŠ åå¤„ç†æ»¤æ³¢")
    else:
        print("   - é‡å»ºè´¨é‡è‰¯å¥½ï¼")
    
    print(f"\\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° vae_ultimate_test/ ç›®å½•")


def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python ultimate_vae_test.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„> [æœ€å¤§é•¿åº¦ç§’æ•°]")
        
        audio_files = []
        for ext in ['.wav', '.mp3', '.flac']:
            audio_files.extend(Path('.').glob(f'*{ext}'))
        
        if audio_files:
            print(f"\\næ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files, 1):
                print(f"{i}. {file}")
            
            try:
                choice = input("è¯·é€‰æ‹©æ–‡ä»¶åºå·: ").strip()
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(audio_files):
                    audio_path = str(audio_files[file_idx])
                else:
                    print("æ— æ•ˆé€‰æ‹©")
                    return
            except (ValueError, KeyboardInterrupt):
                print("å–æ¶ˆæ“ä½œ")
                return
        else:
            print("å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    max_length = 10
    if len(sys.argv) >= 3:
        try:
            max_length = float(sys.argv[2])
        except ValueError:
            print(f"æ— æ•ˆçš„é•¿åº¦å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {max_length} ç§’")
    
    print(f"ğŸš€ å¼€å§‹ç»ˆæVAEæµ‹è¯•: {audio_path}")
    print(f"â±ï¸ æœ€å¤§é•¿åº¦: {max_length} ç§’")
    
    try:
        results = ultimate_vae_test(audio_path, max_length=max_length)
        if results:
            print("\\nâœ… ç»ˆææµ‹è¯•å®Œæˆï¼")
            print("ğŸ§ è¯·æ’­æ”¾ä¸åŒæ–¹æ³•çš„é‡å»ºéŸ³é¢‘æ¥è¯„ä¼°è´¨é‡ã€‚")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
