"""
AudioLDM2 VAEéŸ³é¢‘é‡å»º - æœ€ç»ˆæ•´åˆç‰ˆæœ¬

è¿™æ˜¯é›†æˆäº†æ‰€æœ‰æŠ€æœ¯çªç ´å’Œæœ€ä½³å®è·µçš„å®Œæ•´ç‰ˆæœ¬ï¼š
1. âœ… è§£å†³äº†Vocoderç»´åº¦é—®é¢˜ (å…³é”®çªç ´)
2. âœ… ä¿®å¤äº†æ•°æ®ç±»å‹å…¼å®¹æ€§é—®é¢˜  
3. âœ… å®ç°äº†å¤šç§é‡å»ºæ–¹æ³•å¯¹æ¯”
4. âœ… æä¾›äº†å®Œæ•´çš„æ€§èƒ½åˆ†æ
5. âœ… åŒ…å«äº†è¯¦ç»†çš„é”™è¯¯å¤„ç†

ä½¿ç”¨æ–¹æ³•:
python ultimate_vae_reconstruction.py <audio_file> [model_variant] [max_length]

ç¤ºä¾‹:
python ultimate_vae_reconstruction.py AudioLDM2_Music_output.wav music 10
"""

import torch
import librosa
import numpy as np
import os
import time
from pathlib import Path
import torchaudio
import warnings
warnings.filterwarnings('ignore')

from diffusers import AudioLDM2Pipeline


class AudioLDM2VAEReconstructor:
    """AudioLDM2 VAEéŸ³é¢‘é‡å»ºå™¨"""
    
    def __init__(self, model_variant="music", device=None):
        """
        åˆå§‹åŒ–é‡å»ºå™¨
        
        Args:
            model_variant: æ¨¡å‹å˜ä½“ ("music", "speech", "large")
            device: è®¡ç®—è®¾å¤‡ (Noneä¸ºè‡ªåŠ¨æ£€æµ‹)
        """
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.sample_rate = 16000
        
        # æ¨¡å‹å˜ä½“æ˜ å°„
        self.model_variants = {
            "music": "cvssp/audioldm2-music",
            "speech": "cvssp/audioldm2",  
            "large": "cvssp/audioldm2-large"
        }
        
        self.model_id = self.model_variants.get(model_variant, model_variant)
        
        print(f"ğŸš€ åˆå§‹åŒ–AudioLDM2 VAEé‡å»ºå™¨")
        print(f"   ğŸ¯ è®¾å¤‡: {self.device}")
        print(f"   ğŸµ æ¨¡å‹: {self.model_id}")
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # ä½¿ç”¨float32é¿å…ç±»å‹é—®é¢˜
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            self.model_id,
            torch_dtype=torch.float32
        ).to(self.device)
        
        self.vae = self.pipeline.vae
        self.vocoder = self.pipeline.vocoder
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   ğŸ”§ VAE: {type(self.vae).__name__}")
        print(f"   ğŸ¤ Vocoder: {type(self.vocoder).__name__}")
        
        if hasattr(self.vocoder, 'config'):
            print(f"   ğŸ“Š Vocoderå‚æ•°: {self.vocoder.config.model_in_dim}é€šé“, {self.vocoder.config.sampling_rate}Hz")
    
    def audio_to_mel(self, audio):
        """éŸ³é¢‘è½¬mel-spectrogram"""
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=self.sample_rate,
            n_mels=64,
            n_fft=1024,
            hop_length=160,
            window='hann',
            center=True,
            pad_mode='reflect',
            power=2.0
        )
        
        # è½¬æ¢åˆ°å¯¹æ•°åŸŸå¹¶å½’ä¸€åŒ–åˆ°[-1, 1]
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        mel_spec_norm = 2.0 * (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min()) - 1.0
        
        return mel_spec_norm.astype(np.float32)
    
    def vae_encode_decode(self, mel_spec):
        """VAEç¼–ç è§£ç """
        # å‡†å¤‡è¾“å…¥å¼ é‡
        mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).unsqueeze(0).float().to(self.device)
        
        with torch.no_grad():
            # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
            latent = self.vae.encode(mel_tensor).latent_dist.sample()
            
            # ä»æ½œåœ¨ç©ºé—´è§£ç 
            decoded = self.vae.decode(latent).sample
        
        return decoded.squeeze().cpu().float().numpy()
    
    def mel_to_audio_vocoder(self, mel_spec):
        """ä½¿ç”¨ä¿®æ­£ç»´åº¦çš„Vocoderé‡å»ºéŸ³é¢‘"""
        try:
            # è½¬æ¢ä¸ºå¼ é‡å¹¶æ·»åŠ batchç»´åº¦
            mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).float().to(self.device)
            
            # ğŸ”‘ å…³é”®ä¿®æ­£: è½¬ç½®ç»´åº¦ä» [batch, channels, time] åˆ° [batch, time, channels]
            mel_tensor_corrected = mel_tensor.transpose(-2, -1)
            
            with torch.no_grad():
                audio_tensor = self.vocoder(mel_tensor_corrected)
                
                if isinstance(audio_tensor, tuple):
                    audio_tensor = audio_tensor[0]
                
                audio = audio_tensor.squeeze().cpu().numpy()
            
            return audio, "success"
            
        except Exception as e:
            return None, f"vocoder_failed: {e}"
    
    def mel_to_audio_griffinlim(self, mel_spec):
        """ä½¿ç”¨Griffin-Limé‡å»ºéŸ³é¢‘"""
        try:
            # ç¡®ä¿float32ç±»å‹
            mel_spec = mel_spec.astype(np.float32)
            
            # åå½’ä¸€åŒ–ï¼šä»[-1,1] -> [min_db, 0]
            mel_spec_denorm = (mel_spec + 1.0) / 2.0
            mel_spec_denorm = mel_spec_denorm * 80.0 - 80.0
            
            # è½¬æ¢åˆ°åŠŸç‡åŸŸ
            mel_spec_power = librosa.db_to_power(mel_spec_denorm, ref=1.0)
            
            # Griffin-Limé‡å»º
            audio = librosa.feature.inverse.mel_to_audio(
                mel_spec_power,
                sr=self.sample_rate,
                n_fft=1024,
                hop_length=160,
                window='hann',
                center=True,
                pad_mode='reflect',
                n_iter=32
            )
            
            return audio, "success"
            
        except Exception as e:
            return None, f"griffinlim_failed: {e}"
    
    def calculate_metrics(self, original, reconstructed):
        """è®¡ç®—é‡å»ºè´¨é‡æŒ‡æ ‡"""
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(original), len(reconstructed))
        original = original[:min_len]
        reconstructed = reconstructed[:min_len]
        
        # SNRè®¡ç®—
        noise = reconstructed - original
        signal_power = np.mean(original ** 2)
        noise_power = np.mean(noise ** 2)
        snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
        
        # ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(original, reconstructed)[0, 1] if len(original) > 1 else 0
        
        # RMSE
        rmse = np.sqrt(np.mean(noise ** 2))
        
        return {
            'snr': snr,
            'correlation': correlation,
            'rmse': rmse,
            'signal_power': signal_power,
            'noise_power': noise_power
        }
    
    def reconstruct_audio(self, audio_path, max_length=10, output_dir=None):
        """
        å®Œæ•´çš„éŸ³é¢‘é‡å»ºæµç¨‹
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            max_length: æœ€å¤§å¤„ç†é•¿åº¦(ç§’)
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            dict: åŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = f"vae_reconstruction_{int(time.time())}"
        os.makedirs(output_dir, exist_ok=True)
        
        input_name = Path(audio_path).stem
        timestamp = int(time.time())
        
        print(f"\\nğŸµ å¼€å§‹é‡å»ºéŸ³é¢‘: {audio_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # 1. åŠ è½½éŸ³é¢‘
        print(f"\\n1ï¸âƒ£ åŠ è½½éŸ³é¢‘")
        audio, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True)
        
        if len(audio) > max_length * self.sample_rate:
            audio = audio[:int(max_length * self.sample_rate)]
            print(f"   âœ‚ï¸ éŸ³é¢‘è£å‰ªåˆ° {max_length} ç§’")
        
        print(f"   ğŸ“Š éŸ³é¢‘ä¿¡æ¯: {len(audio)/self.sample_rate:.2f}ç§’, {len(audio)}æ ·æœ¬")
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘
        original_path = os.path.join(output_dir, f"{input_name}_original.wav")
        audio_normalized = audio / (np.max(np.abs(audio)) + 1e-8)
        torchaudio.save(original_path, torch.from_numpy(audio_normalized).unsqueeze(0), self.sample_rate)
        
        # 2. éŸ³é¢‘è½¬Mel
        print(f"\\n2ï¸âƒ£ éŸ³é¢‘è½¬Mel-spectrogram")
        start_time = time.time()
        mel_spec = self.audio_to_mel(audio)
        mel_time = time.time() - start_time
        print(f"   âœ… Melå½¢çŠ¶: {mel_spec.shape} ({mel_time:.3f}ç§’)")
        
        # 3. VAEç¼–ç è§£ç 
        print(f"\\n3ï¸âƒ£ VAEç¼–ç è§£ç ")
        start_time = time.time()
        decoded_mel = self.vae_encode_decode(mel_spec)
        vae_time = time.time() - start_time
        print(f"   âœ… VAEè¾“å‡º: {decoded_mel.shape} ({vae_time:.3f}ç§’)")
        
        # 4. å¤šç§é‡å»ºæ–¹æ³•æµ‹è¯•
        print(f"\\n4ï¸âƒ£ éŸ³é¢‘é‡å»ºå¯¹æ¯”")
        
        results = []
        
        # æ–¹æ³•A: Vocoderé‡å»º
        print(f"\\nğŸ¤ æ–¹æ³•A: AudioLDM2 Vocoder")
        start_time = time.time()
        vocoder_audio, vocoder_status = self.mel_to_audio_vocoder(decoded_mel)
        vocoder_time = time.time() - start_time
        
        if vocoder_audio is not None:
            metrics = self.calculate_metrics(audio, vocoder_audio)
            
            vocoder_path = os.path.join(output_dir, f"{input_name}_vocoder_reconstruction.wav")
            audio_norm = vocoder_audio / (np.max(np.abs(vocoder_audio)) + 1e-8)
            torchaudio.save(vocoder_path, torch.from_numpy(audio_norm).unsqueeze(0), self.sample_rate)
            
            results.append({
                'method': 'AudioLDM2 Vocoder',
                'path': vocoder_path,
                'time': vocoder_time,
                'status': vocoder_status,
                **metrics
            })
            
            print(f"   âœ… æˆåŠŸ! SNR: {metrics['snr']:.2f}dB, ç›¸å…³: {metrics['correlation']:.4f}")
        else:
            print(f"   âŒ å¤±è´¥: {vocoder_status}")
        
        # æ–¹æ³•B: Griffin-Limé‡å»º  
        print(f"\\nğŸµ æ–¹æ³•B: Griffin-Lim")
        start_time = time.time()
        gl_audio, gl_status = self.mel_to_audio_griffinlim(decoded_mel)
        gl_time = time.time() - start_time
        
        if gl_audio is not None:
            metrics = self.calculate_metrics(audio, gl_audio)
            
            gl_path = os.path.join(output_dir, f"{input_name}_griffinlim_reconstruction.wav")
            audio_norm = gl_audio / (np.max(np.abs(gl_audio)) + 1e-8)
            torchaudio.save(gl_path, torch.from_numpy(audio_norm).unsqueeze(0), self.sample_rate)
            
            results.append({
                'method': 'Griffin-Lim',
                'path': gl_path,
                'time': gl_time,
                'status': gl_status,
                **metrics
            })
            
            print(f"   âœ… æˆåŠŸ! SNR: {metrics['snr']:.2f}dB, ç›¸å…³: {metrics['correlation']:.4f}")
        else:
            print(f"   âŒ å¤±è´¥: {gl_status}")
        
        # 5. ç»“æœåˆ†æ
        print(f"\\n{'='*60}")
        print(f"ğŸ¯ AudioLDM2 VAEé‡å»ºç»“æœåˆ†æ")
        print(f"{'='*60}")
        
        analysis = {
            'input_file': audio_path,
            'output_dir': output_dir,
            'original_path': original_path,
            'audio_duration': len(audio) / self.sample_rate,
            'mel_generation_time': mel_time,
            'vae_processing_time': vae_time,
            'results': results,
            'model_info': {
                'model_id': self.model_id,
                'device': self.device,
                'vae_type': type(self.vae).__name__,
                'vocoder_type': type(self.vocoder).__name__
            }
        }
        
        if results:
            # æŒ‰SNRæ’åº
            results.sort(key=lambda x: x['snr'], reverse=True)
            
            print(f"\\nğŸ† é‡å»ºè´¨é‡æ’å:")
            for i, result in enumerate(results, 1):
                print(f"   #{i} {result['method']}:")
                print(f"       ğŸ“ˆ SNR: {result['snr']:.2f} dB")
                print(f"       ğŸ”— ç›¸å…³ç³»æ•°: {result['correlation']:.4f}")
                print(f"       ğŸ“ RMSE: {result['rmse']:.6f}")
                print(f"       â±ï¸ å¤„ç†æ—¶é—´: {result['time']:.3f}ç§’")
                print(f"       ğŸ“„ æ–‡ä»¶: {result['path']}")
                print(f"       âœ… çŠ¶æ€: {result['status']}")
                print()
            
            best_result = results[0]
            analysis['best_method'] = best_result['method']
            analysis['best_snr'] = best_result['snr']
            
            print(f"ğŸš€ æœ€ä½³ç»“æœ:")
            print(f"   ğŸ† æœ€ä¼˜æ–¹æ³•: {best_result['method']}")
            print(f"   ğŸ“ˆ æœ€é«˜SNR: {best_result['snr']:.2f} dB")
            print(f"   ğŸ”— ç›¸å…³ç³»æ•°: {best_result['correlation']:.4f}")
            
            if len(results) > 1:
                improvement = best_result['snr'] - results[-1]['snr']
                print(f"   ğŸ“Š æ–¹æ³•é—´å·®å¼‚: {improvement:.2f} dB")
                analysis['method_difference'] = improvement
            
            # æ£€æŸ¥vocoderæˆåŠŸçŠ¶æ€
            vocoder_success = any(r['method'] == 'AudioLDM2 Vocoder' for r in results)
            if vocoder_success:
                print(f"\\nğŸ‰ é‡å¤§æˆå°±: AudioLDM2 Vocoderç»´åº¦é—®é¢˜å·²å®Œå…¨è§£å†³ï¼")
                analysis['vocoder_breakthrough'] = True
            else:
                print(f"\\nâš ï¸ Vocoderä»æœ‰é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨Griffin-Lim")
                analysis['vocoder_breakthrough'] = False
        else:
            print(f"\\nâŒ æ‰€æœ‰é‡å»ºæ–¹æ³•éƒ½å¤±è´¥äº†")
            analysis['success'] = False
        
        # ä¿å­˜åˆ†æç»“æœ
        import json
        analysis_path = os.path.join(output_dir, f"{input_name}_analysis.json")
        with open(analysis_path, 'w', encoding='utf-8') as f:
            # å°†numpyç±»å‹è½¬æ¢ä¸ºpythonåŸç”Ÿç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
            def convert_types(obj):
                if isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                return obj
            
            def clean_for_json(data):
                if isinstance(data, dict):
                    return {k: clean_for_json(v) for k, v in data.items()}
                elif isinstance(data, list):
                    return [clean_for_json(item) for item in data]
                else:
                    return convert_types(data)
            
            json.dump(clean_for_json(analysis), f, indent=2, ensure_ascii=False)
        
        print(f"\\nğŸ“Š è¯¦ç»†åˆ†æä¿å­˜: {analysis_path}")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}/")
        print(f"ğŸ§ å»ºè®®æ’­æ”¾éŸ³é¢‘æ–‡ä»¶è¿›è¡Œä¸»è§‚è´¨é‡è¯„ä¼°")
        print(f"\\nâœ… é‡å»ºå®Œæˆï¼")
        
        return analysis


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description='AudioLDM2 VAEéŸ³é¢‘é‡å»ºç³»ç»Ÿ')
    parser.add_argument('audio_path', help='è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', default='music', choices=['music', 'speech', 'large'], 
                       help='æ¨¡å‹å˜ä½“ (é»˜è®¤: music)')
    parser.add_argument('--max_length', type=int, default=10, 
                       help='æœ€å¤§å¤„ç†é•¿åº¦(ç§’) (é»˜è®¤: 10)')
    parser.add_argument('--output_dir', help='è¾“å‡ºç›®å½• (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ)')
    
    if len(sys.argv) == 1:
        # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
        audio_path = "AudioLDM2_Music_output.wav"
        model_variant = "music"
        max_length = 10
        output_dir = None
        
        print(f"ğŸ¯ ä½¿ç”¨é»˜è®¤å‚æ•°è¿è¡Œ:")
        print(f"   ğŸ“„ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        print(f"   ğŸµ æ¨¡å‹å˜ä½“: {model_variant}")
        print(f"   â±ï¸ æœ€å¤§é•¿åº¦: {max_length}ç§’")
    else:
        args = parser.parse_args()
        audio_path = args.audio_path
        model_variant = args.model
        max_length = args.max_length
        output_dir = args.output_dir
    
    print(f"ğŸš€ å¯åŠ¨AudioLDM2 VAEéŸ³é¢‘é‡å»ºç³»ç»Ÿ")
    print(f"=" * 60)
    
    try:
        # åˆ›å»ºé‡å»ºå™¨
        reconstructor = AudioLDM2VAEReconstructor(model_variant=model_variant)
        
        # æ‰§è¡Œé‡å»º
        results = reconstructor.reconstruct_audio(
            audio_path=audio_path,
            max_length=max_length,
            output_dir=output_dir
        )
        
        print(f"\\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æŸ¥çœ‹è¾“å‡ºç›®å½•äº†è§£è¯¦ç»†ç»“æœã€‚")
        
    except Exception as e:
        print(f"\\nâŒ é”™è¯¯: {e}")
        print(f"è¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶å’Œä¾èµ–é¡¹æ˜¯å¦æ­£ç¡®ã€‚")


if __name__ == "__main__":
    main()
