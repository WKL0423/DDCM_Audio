"""
AudioLDM2 VAEé«˜è´¨é‡éŸ³é¢‘é‡å»º - ä¿®å¤ç‰ˆæœ¬
============================================

åŸºäºå‰æœŸè¯Šæ–­ç»“æœçš„é’ˆå¯¹æ€§è§£å†³æ–¹æ¡ˆ:
- Griffin-Limå¯¼è‡´92%ä¿¡æ¯æŸå¤± â†’ ä½¿ç”¨ç¥ç»vocoder
- VAEå‹ç¼©15%æŸå¤± â†’ ä¼˜åŒ–å‚æ•°é…ç½®
- Melå˜æ¢14%æŸå¤± â†’ å¤šåˆ†è¾¨ç‡å¯¹æ¯”æµ‹è¯•

ç›®æ ‡: æ˜¾è‘—æå‡é‡å»ºè´¨é‡ï¼Œè§£å†³"èƒ½å¬å‡ºè”ç³»ä½†è´¨é‡ä¸€èˆ¬"é—®é¢˜
"""

import torch
import torchaudio
import librosa
import numpy as np
import os
import time
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from diffusers import AudioLDM2Pipeline
from transformers import SpeechT5HifiGan
import scipy.io.wavfile
from datetime import datetime

warnings.filterwarnings("ignore")

class VAEReconstructionFixer:
    """VAEé‡å»ºè´¨é‡ä¿®å¤å™¨ - ä¸“é—¨è§£å†³è´¨é‡ç“¶é¢ˆ"""
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ VAEé‡å»ºè´¨é‡ä¿®å¤å™¨å¯åŠ¨")
        print(f"ğŸ“± è®¾å¤‡: {self.device}")
        
        # åŠ è½½AudioLDM2
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipe = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float32 if self.device.type == "cpu" else torch.float16
        ).to(self.device)
        
        self.vae = self.pipe.vae
        
        # å°è¯•åŠ è½½é«˜è´¨é‡vocoder
        self.hifigan_vocoder = None
        try:
            print("ğŸ¤ åŠ è½½HiFiGAN vocoder...")
            self.hifigan_vocoder = SpeechT5HifiGan.from_pretrained(
                "microsoft/speecht5_hifigan"
            ).to(self.device)
            print("âœ… HiFiGANåŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ HiFiGANåŠ è½½å¤±è´¥: {e}")
        
        print("âœ… ä¿®å¤å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_and_preprocess_audio(self, audio_path: str, max_duration: float = 5.0) -> Tuple[np.ndarray, int]:
        """åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘"""
        audio, sr = librosa.load(audio_path, sr=16000, duration=max_duration)
        print(f"ğŸ“Š éŸ³é¢‘: {len(audio)/sr:.2f}ç§’, {len(audio)}æ ·æœ¬")
        return audio, sr
    
    def create_mel_spectrogram(self, audio: np.ndarray, sr: int, 
                              n_mels: int = 64, enhanced: bool = False) -> torch.Tensor:
        """åˆ›å»ºmelé¢‘è°±å›¾"""
        if enhanced:
            # é«˜è´¨é‡é…ç½®
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=n_mels, n_fft=2048, 
                hop_length=256, win_length=1024, fmin=0, fmax=8000
            )
        else:
            # æ ‡å‡†é…ç½®
            mel_spec = librosa.feature.melspectrogram(
                y=audio, sr=sr, n_mels=n_mels, n_fft=1024,
                hop_length=256, win_length=1024, fmin=0, fmax=8000
            )
        
        # è½¬æ¢ä¸ºå¯¹æ•°å°ºåº¦å¹¶å½’ä¸€åŒ–
        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        mel_spec = 2.0 * (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min()) - 1.0
        
        mel_tensor = torch.from_numpy(mel_spec).float().unsqueeze(0).unsqueeze(0)
        print(f"ğŸ¼ Melé¢‘è°±: {mel_tensor.shape}")
        return mel_tensor.to(self.device)
    
    def vae_reconstruct(self, mel_tensor: torch.Tensor) -> torch.Tensor:
        """VAEé‡å»ºè¿‡ç¨‹"""
        with torch.no_grad():
            # æ•°æ®ç±»å‹åŒ¹é…
            if next(self.vae.parameters()).dtype == torch.float16:
                mel_tensor = mel_tensor.half()
            
            # ç¡®ä¿å°ºå¯¸åŒ¹é…VAEè¦æ±‚
            if mel_tensor.shape[-1] % 4 != 0:
                pad_length = 4 - (mel_tensor.shape[-1] % 4)
                mel_tensor = torch.nn.functional.pad(mel_tensor, (0, pad_length))
            
            # VAEç¼–ç è§£ç 
            start_time = time.time()
            latent = self.vae.encode(mel_tensor).latent_dist.sample()
            reconstructed = self.vae.decode(latent).sample
            vae_time = time.time() - start_time
            
            print(f"ğŸ”„ VAEé‡å»º: {vae_time:.3f}s, æ½œåœ¨ç©ºé—´: {latent.shape}")
            return reconstructed.float()
    
    def griffin_lim_improved(self, mel_tensor: torch.Tensor, sr: int = 16000) -> np.ndarray:
        """æ”¹è¿›çš„Griffin-Limç®—æ³•"""
        mel_np = mel_tensor.squeeze().cpu().numpy()
        
        # åå½’ä¸€åŒ–
        mel_np = (mel_np + 1.0) / 2.0
        mel_min, mel_max = -80, 0
        mel_np = mel_np * (mel_max - mel_min) + mel_min
        mel_linear = librosa.db_to_power(mel_np)
        
        # æ”¹è¿›çš„Griffin-Limå‚æ•°
        audio = librosa.feature.inverse.mel_to_audio(
            mel_linear, sr=sr, n_iter=64, hop_length=256,
            win_length=1024, n_fft=1024, fmin=0, fmax=8000
        )
        
        return audio
    
    def hifigan_reconstruct(self, mel_tensor: torch.Tensor) -> Optional[np.ndarray]:
        """ä½¿ç”¨HiFiGANé‡å»ºéŸ³é¢‘"""
        if self.hifigan_vocoder is None:
            return None
        
        try:
            with torch.no_grad():
                mel_input = mel_tensor.squeeze()
                if mel_input.dim() == 3:
                    mel_input = mel_input.squeeze(0)
                
                # è½¬æ¢åˆ°80ç»´ (HiFiGANæ ‡å‡†)
                if mel_input.shape[0] != 80:
                    mel_input = torch.nn.functional.interpolate(
                        mel_input.unsqueeze(0).unsqueeze(0),
                        size=(80, mel_input.shape[1]),
                        mode='bilinear', align_corners=False
                    ).squeeze(0).squeeze(0)
                
                # HiFiGANæœŸæœ›çš„è¾“å…¥æ ¼å¼: [batch, mel_bins, time]
                mel_input = mel_input.unsqueeze(0)
                
                audio_tensor = self.hifigan_vocoder(mel_input)
                audio = audio_tensor.squeeze().cpu().numpy()
                
                return audio
                
        except Exception as e:
            print(f"âŒ HiFiGANå¤±è´¥: {e}")
            return None
    
    def calculate_quality_metrics(self, original: np.ndarray, reconstructed: np.ndarray) -> Dict:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        min_len = min(len(original), len(reconstructed))
        orig = original[:min_len]
        recon = reconstructed[:min_len]
        
        mse = np.mean((orig - recon) ** 2)
        snr = 10 * np.log10(np.mean(orig ** 2) / (mse + 1e-8))
        correlation = np.corrcoef(orig, recon)[0, 1]
        
        return {
            'mse': float(mse),
            'snr': float(snr),
            'correlation': float(correlation if not np.isnan(correlation) else 0)
        }
    
    def comprehensive_test(self, audio_path: str) -> Dict:
        """å…¨é¢æµ‹è¯•ä¸åŒé‡å»ºç­–ç•¥"""
        print("ğŸ”§ å¼€å§‹VAEé‡å»ºè´¨é‡ä¿®å¤æµ‹è¯•")
        print("="*50)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("vae_quality_fix_test")
        output_dir.mkdir(exist_ok=True)
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = self.load_and_preprocess_audio(audio_path)
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘
        audio_name = Path(audio_path).stem
        timestamp = int(time.time())
        original_path = output_dir / f"{audio_name}_original_{timestamp}.wav"
        scipy.io.wavfile.write(original_path, sr, (audio * 32767).astype(np.int16))
        
        results = {}
        
        # æµ‹è¯•ç­–ç•¥é…ç½®
        test_configs = [
            {"name": "baseline_64mel_griffin", "n_mels": 64, "enhanced": False, "vocoder": "griffin_lim"},
            {"name": "enhanced_64mel_griffin", "n_mels": 64, "enhanced": True, "vocoder": "griffin_lim"},
            {"name": "baseline_80mel_griffin", "n_mels": 80, "enhanced": False, "vocoder": "griffin_lim"},
            {"name": "enhanced_80mel_griffin", "n_mels": 80, "enhanced": True, "vocoder": "griffin_lim"},
        ]
        
        # å¦‚æœHiFiGANå¯ç”¨ï¼Œæ·»åŠ ç¥ç»vocoderæµ‹è¯•
        if self.hifigan_vocoder:
            test_configs.extend([
                {"name": "baseline_80mel_hifigan", "n_mels": 80, "enhanced": False, "vocoder": "hifigan"},
                {"name": "enhanced_80mel_hifigan", "n_mels": 80, "enhanced": True, "vocoder": "hifigan"},
            ])
        
        print(f"ğŸ§ª æµ‹è¯• {len(test_configs)} ç§é…ç½®...")
        
        for i, config in enumerate(test_configs):
            print(f"\nğŸ“Š é…ç½® {i+1}/{len(test_configs)}: {config['name']}")
            
            try:
                # åˆ›å»ºmelé¢‘è°±
                mel_tensor = self.create_mel_spectrogram(
                    audio, sr, config['n_mels'], config['enhanced']
                )
                
                # VAEé‡å»º
                reconstructed_mel = self.vae_reconstruct(mel_tensor)
                
                # ä½¿ç”¨æŒ‡å®šçš„vocoder
                if config['vocoder'] == 'griffin_lim':
                    final_audio = self.griffin_lim_improved(reconstructed_mel, sr)
                elif config['vocoder'] == 'hifigan':
                    final_audio = self.hifigan_reconstruct(reconstructed_mel)
                    if final_audio is None:
                        print("   âŒ HiFiGANå¤±è´¥ï¼Œè·³è¿‡")
                        continue
                
                # åå¤„ç†: å½’ä¸€åŒ–
                final_audio = final_audio / (np.max(np.abs(final_audio)) + 1e-8)
                
                # è®¡ç®—æŒ‡æ ‡
                metrics = self.calculate_quality_metrics(audio, final_audio)
                
                # ä¿å­˜ç»“æœ
                output_file = output_dir / f"{audio_name}_{config['name']}_{timestamp}.wav"
                scipy.io.wavfile.write(output_file, sr, (final_audio * 32767).astype(np.int16))
                
                results[config['name']] = {
                    'metrics': metrics,
                    'file': str(output_file),
                    'config': config
                }
                
                print(f"   âœ… SNR: {metrics['snr']:.2f}dB, ç›¸å…³æ€§: {metrics['correlation']:.4f}")
                
            except Exception as e:
                print(f"   âŒ é…ç½®å¤±è´¥: {e}")
                continue
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self._generate_analysis_report(results, audio_path, output_dir)
        
        return results
    
    def _generate_analysis_report(self, results: Dict, audio_path: str, output_dir: Path):
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*50)
        print("ğŸ”§ VAEé‡å»ºè´¨é‡ä¿®å¤æµ‹è¯•ç»“æœ")
        print("="*50)
        
        if not results:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
            return
        
        # æŒ‰SNRæ’åº
        sorted_results = sorted(
            results.items(),
            key=lambda x: x[1]['metrics']['snr'],
            reverse=True
        )
        
        print(f"\nğŸ† è´¨é‡æ’å:")
        baseline_snr = None
        for i, (config_name, data) in enumerate(sorted_results):
            metrics = data['metrics']
            config = data['config']
            
            if i == 0:
                baseline_snr = metrics['snr']
            
            improvement = metrics['snr'] - baseline_snr if baseline_snr else 0
            
            print(f"   #{i+1} {config_name}")
            print(f"       ğŸ“ˆ SNR: {metrics['snr']:.2f}dB ({improvement:+.2f})")
            print(f"       ğŸ”— ç›¸å…³æ€§: {metrics['correlation']:.4f}")
            print(f"       ğŸ¤ Vocoder: {config['vocoder']}")
            print(f"       ğŸ“ æ–‡ä»¶: {Path(data['file']).name}")
        
        # æŒ‰vocoderç±»å‹åˆ†æ
        vocoder_analysis = {}
        for config_name, data in results.items():
            vocoder = data['config']['vocoder']
            if vocoder not in vocoder_analysis:
                vocoder_analysis[vocoder] = []
            vocoder_analysis[vocoder].append(data['metrics']['snr'])
        
        print(f"\nğŸ“Š Vocoderæ•ˆæœå¯¹æ¯”:")
        for vocoder, snrs in vocoder_analysis.items():
            avg_snr = np.mean(snrs)
            max_snr = np.max(snrs)
            print(f"   ğŸ¤ {vocoder}: å¹³å‡{avg_snr:.2f}dB, æœ€ä½³{max_snr:.2f}dB")
        
        # æ”¹è¿›å»ºè®®
        best_config = sorted_results[0][1]['config']
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        if best_config['vocoder'] == 'hifigan':
            print("   âœ… ç¥ç»vocoderæ˜¾è‘—æ”¹å–„è´¨é‡")
        else:
            print("   âš ï¸ å»ºè®®å°è¯•æ›´å¤šç¥ç»vocoderé€‰é¡¹")
        
        if best_config['enhanced']:
            print("   âœ… é«˜è´¨é‡melé…ç½®æœ‰æ•ˆ")
        else:
            print("   âš ï¸ å¯å°è¯•æ›´é«˜åˆ†è¾¨ç‡melé…ç½®")
        
        print(f"\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")
        print("ğŸ§ å»ºè®®ä¸»è§‚è¯„ä¼°æœ€ä½³ç»“æœ")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python vae_quality_fixer.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„>")
        
        # åˆ—å‡ºå¯ç”¨éŸ³é¢‘æ–‡ä»¶
        audio_files = list(Path('.').glob('*.wav')) + list(Path('.').glob('*.mp3'))
        
        if audio_files:
            print("æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files[:5], 1):
                print(f"{i}. {file.name}")
            
            try:
                choice = int(input("é€‰æ‹©æ–‡ä»¶åºå·: "))
                audio_path = str(audio_files[choice-1])
            except (ValueError, IndexError):
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    # è¿è¡Œä¿®å¤æµ‹è¯•
    fixer = VAEReconstructionFixer()
    results = fixer.comprehensive_test(audio_path)
    
    print("\nâœ… VAEé‡å»ºè´¨é‡ä¿®å¤æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()
