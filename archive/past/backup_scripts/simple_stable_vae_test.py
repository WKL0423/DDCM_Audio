"""
ç®€åŒ–ä¸”ç¨³å®šçš„VAEæµ‹è¯•ç‰ˆæœ¬
ä¸“æ³¨äºŽè§£å†³æ•°æ®ç±»åž‹å’Œç»´åº¦é—®é¢˜ï¼Œå®žçŽ°åŸºæœ¬çš„VAEé‡å»ºåŠŸèƒ½

ä¸»è¦æ”¹è¿›ï¼š
1. ç»Ÿä¸€ä½¿ç”¨float32é¿å…æ•°æ®ç±»åž‹é—®é¢˜
2. ç®€åŒ–vocoderè°ƒç”¨é€»è¾‘
3. æ”¹è¿›é”™è¯¯å¤„ç†
4. æä¾›æ¸…æ™°çš„ç»“æžœå¯¹æ¯”
"""

import torch
import librosa
import numpy as np
import os
import time
from pathlib import Path
import torchaudio

from diffusers import AudioLDM2Pipeline


def calculate_metrics(original, reconstructed):
    """è®¡ç®—é‡å»ºè´¨é‡æŒ‡æ ‡"""
    if len(original) != len(reconstructed):
        min_len = min(len(original), len(reconstructed))
        original = original[:min_len]
        reconstructed = reconstructed[:min_len]
    
    # SNRè®¡ç®—
    noise = reconstructed - original
    signal_power = np.mean(original ** 2)
    noise_power = np.mean(noise ** 2)
    snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
    
    # ç›¸å…³ç³»æ•°
    correlation = np.corrcoef(original, reconstructed)[0, 1] if len(original) > 1 else 0
    
    return snr, correlation


def audio_to_mel_simple(audio, sample_rate=16000):
    """
    ç®€åŒ–çš„éŸ³é¢‘è½¬mel-spectrogram
    """
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sample_rate,
        n_mels=64,
        n_fft=1024,
        hop_length=160,
        window='hann',
        center=True,
        pad_mode='reflect',
        power=2.0
    )
    
    # è½¬æ¢åˆ°å¯¹æ•°åŸŸå¹¶å½’ä¸€åŒ–
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_spec_norm = 2.0 * (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min()) - 1.0
    
    return mel_spec_norm.astype(np.float32)


def mel_to_audio_simple(mel_spec, sample_rate=16000):
    """
    ç®€åŒ–çš„melåˆ°éŸ³é¢‘è½¬æ¢ï¼ˆGriffin-Limï¼‰
    """
    # ç¡®ä¿æ˜¯float32
    mel_spec = mel_spec.astype(np.float32)
    
    # åå½’ä¸€åŒ–
    mel_spec_denorm = (mel_spec + 1.0) / 2.0
    mel_spec_denorm = mel_spec_denorm * 80.0 - 80.0
    
    # è½¬æ¢å›žåŠŸçŽ‡åŸŸ
    mel_spec_power = librosa.db_to_power(mel_spec_denorm, ref=1.0)
    
    # Griffin-Limé‡å»º
    audio = librosa.feature.inverse.mel_to_audio(
        mel_spec_power,
        sr=sample_rate,
        n_fft=1024,
        hop_length=160,
        window='hann',
        center=True,
        pad_mode='reflect',
        n_iter=32
    )
    
    return audio


def try_vocoder_reconstruction(mel_spec, vocoder, device):
    """
    å°è¯•ä½¿ç”¨vocoderé‡å»ºéŸ³é¢‘ï¼ŒåŒ…å«å¤šç§æ–¹æ³•
    """
    results = []
    
    # ç¡®ä¿è¾“å…¥æ˜¯float32
    mel_spec = mel_spec.astype(np.float32)
    
    # æ–¹æ³•1: ç›´æŽ¥ä½¿ç”¨mel_spec
    try:
        print(f"   å°è¯•æ–¹æ³•1: ç›´æŽ¥vocoder...")
        mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).float().to(device)
        
        # æ£€æŸ¥vocoderæœŸæœ›çš„æ•°æ®ç±»åž‹
        vocoder_dtype = next(vocoder.parameters()).dtype
        if vocoder_dtype == torch.float16:
            mel_tensor = mel_tensor.half()
        
        print(f"   è¾“å…¥å½¢çŠ¶: {mel_tensor.shape}, ç±»åž‹: {mel_tensor.dtype}")
        
        with torch.no_grad():
            audio_tensor = vocoder(mel_tensor)
            if isinstance(audio_tensor, tuple):
                audio_tensor = audio_tensor[0]
            audio = audio_tensor.squeeze().cpu().float().numpy()
            
        results.append(("ç›´æŽ¥vocoder", audio, None))
        print(f"   âœ… æ–¹æ³•1æˆåŠŸï¼")
        
    except Exception as e:
        print(f"   âŒ æ–¹æ³•1å¤±è´¥: {e}")
    
    # æ–¹æ³•2: è°ƒæ•´mel_specèŒƒå›´
    try:
        print(f"   å°è¯•æ–¹æ³•2: è°ƒæ•´èŒƒå›´...")
        # å°†[-1,1]èŒƒå›´æ˜ å°„åˆ°[0,1]
        mel_adjusted = (mel_spec + 1.0) / 2.0
        mel_tensor = torch.from_numpy(mel_adjusted).unsqueeze(0).float().to(device)
        
        # æ£€æŸ¥vocoderæœŸæœ›çš„æ•°æ®ç±»åž‹
        vocoder_dtype = next(vocoder.parameters()).dtype
        if vocoder_dtype == torch.float16:
            mel_tensor = mel_tensor.half()
        
        with torch.no_grad():
            audio_tensor = vocoder(mel_tensor)
            if isinstance(audio_tensor, tuple):
                audio_tensor = audio_tensor[0]
            audio = audio_tensor.squeeze().cpu().float().numpy()
            
        results.append(("è°ƒæ•´èŒƒå›´vocoder", audio, None))
        print(f"   âœ… æ–¹æ³•2æˆåŠŸï¼")
        
    except Exception as e:
        print(f"   âŒ æ–¹æ³•2å¤±è´¥: {e}")
    
    # æ–¹æ³•3: ä½¿ç”¨Griffin-Limä½œä¸ºå¤‡é€‰
    try:
        print(f"   å¤‡é€‰: Griffin-Lim...")
        audio = mel_to_audio_simple(mel_spec)
        results.append(("Griffin-Limå¤‡é€‰", audio, None))
        print(f"   âœ… Griffin-LimæˆåŠŸï¼")
        
    except Exception as e:
        print(f"   âŒ Griffin-Limå¤±è´¥: {e}")
    
    return results


def simple_vae_test(audio_path, model_id="cvssp/audioldm2-music", max_length=5):
    """
    ç®€åŒ–çš„VAEé‡å»ºæµ‹è¯•
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ðŸŽ¯ è®¾å¤‡: {device}")
    
    if not os.path.exists(audio_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {audio_path}")
        return
    
    print(f"ðŸ”„ åŠ è½½æ¨¡åž‹: {model_id}")
    
    # å¼ºåˆ¶ä½¿ç”¨float32ä»¥é¿å…æ•°æ®ç±»åž‹é—®é¢˜
    pipeline = AudioLDM2Pipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    print(f"ðŸ”§ Vocoder: {type(vocoder).__name__}")
    
    # åŠ è½½éŸ³é¢‘
    print(f"ðŸ“ åŠ è½½éŸ³é¢‘: {audio_path}")
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    
    if len(audio) > max_length * sample_rate:
        audio = audio[:int(max_length * sample_rate)]
        print(f"âœ‚ï¸ è£å‰ªåˆ° {max_length} ç§’")
    
    print(f"ðŸ“Š éŸ³é¢‘: {len(audio)/sample_rate:.2f}ç§’, {len(audio)}æ ·æœ¬")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "vae_simple_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŽŸå§‹éŸ³é¢‘
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    audio_save = audio / (np.max(np.abs(audio)) + 1e-8)
    torchaudio.save(original_path, torch.from_numpy(audio_save).unsqueeze(0), sample_rate)
    
    print(f"\\nðŸ”¬ å¼€å§‹VAEé‡å»ºæµç¨‹...")
    
    # æ­¥éª¤1: éŸ³é¢‘ -> Mel
    print(f"\\n1ï¸âƒ£ éŸ³é¢‘è½¬Mel-spectrogram")
    start_time = time.time()
    mel_spec = audio_to_mel_simple(audio, sample_rate)
    print(f"   âœ… Melå½¢çŠ¶: {mel_spec.shape} ({time.time()-start_time:.2f}ç§’)")
    
    # æ­¥éª¤2: VAEç¼–ç /è§£ç 
    print(f"\\n2ï¸âƒ£ VAEç¼–ç /è§£ç ")
    start_time = time.time()
    
    # å‡†å¤‡VAEè¾“å…¥ï¼ˆç¡®ä¿float32ï¼‰
    mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).unsqueeze(0).float().to(device)
    print(f"   VAEè¾“å…¥: {mel_tensor.shape}")
    
    with torch.no_grad():
        # ç¼–ç 
        latent = vae.encode(mel_tensor).latent_dist.sample()
        print(f"   æ½œåœ¨ç©ºé—´: {latent.shape}")
        
        # è§£ç 
        decoded = vae.decode(latent).sample
        print(f"   VAEè¾“å‡º: {decoded.shape}")
    
    # è½¬æ¢ä¸ºnumpyï¼ˆç¡®ä¿float32ï¼‰
    decoded_mel = decoded.squeeze().cpu().float().numpy()
    print(f"   âœ… VAEå®Œæˆ ({time.time()-start_time:.2f}ç§’)")
    
    # æ­¥éª¤3: å¤šç§é‡å»ºæ–¹æ³•
    print(f"\\n3ï¸âƒ£ éŸ³é¢‘é‡å»ºæµ‹è¯•")
    
    # æµ‹è¯•vocoderæ–¹æ³•
    print(f"\\nðŸŽ¤ æµ‹è¯•Vocoderæ–¹æ³•:")
    vocoder_results = try_vocoder_reconstruction(decoded_mel, vocoder, device)
    
    # æµ‹è¯•Griffin-Lim
    print(f"\\nðŸŽµ æµ‹è¯•Griffin-Lim:")
    try:
        gl_audio = mel_to_audio_simple(decoded_mel, sample_rate)
        gl_results = [("Griffin-Lim", gl_audio, None)]
        print(f"   âœ… Griffin-LimæˆåŠŸ")
    except Exception as e:
        print(f"   âŒ Griffin-Limå¤±è´¥: {e}")
        gl_results = []
    
    # åˆå¹¶æ‰€æœ‰ç»“æžœ
    all_results = vocoder_results + gl_results
    
    # ä¿å­˜å’Œè¯„ä¼°ç»“æžœ
    print(f"\\nðŸ“Š ç»“æžœè¯„ä¼°:")
    final_results = []
    
    for i, (method_name, recon_audio, error) in enumerate(all_results):
        if recon_audio is None:
            continue
            
        try:
            # è®¡ç®—æŒ‡æ ‡
            snr, corr = calculate_metrics(audio, recon_audio)
            
            # ä¿å­˜éŸ³é¢‘
            save_path = os.path.join(output_dir, f"{input_name}_{method_name.replace(' ', '_')}_{timestamp}.wav")
            audio_norm = recon_audio / (np.max(np.abs(recon_audio)) + 1e-8)
            torchaudio.save(save_path, torch.from_numpy(audio_norm).unsqueeze(0), sample_rate)
            
            final_results.append({
                'method': method_name,
                'path': save_path,
                'snr': snr,
                'correlation': corr
            })
            
            print(f"   âœ… {method_name}: SNR={snr:.2f}dB, ç›¸å…³={corr:.4f}")
            
        except Exception as e:
            print(f"   âŒ {method_name}ä¿å­˜å¤±è´¥: {e}")
    
    # æ‰“å°æœ€ç»ˆæ€»ç»“
    print(f"\\n{'='*60}")
    print(f"ðŸŽ¯ VAEé‡å»ºæµ‹è¯•æ€»ç»“")
    print(f"{'='*60}")
    print(f"ðŸ“ åŽŸå§‹éŸ³é¢‘: {original_path}")
    
    if final_results:
        # æŒ‰SNRæŽ’åº
        final_results.sort(key=lambda x: x['snr'], reverse=True)
        
        print(f"\\nðŸ† ç»“æžœæŽ’å (æŒ‰SNR):")
        for i, result in enumerate(final_results, 1):
            print(f"   #{i} {result['method']}: {result['snr']:.2f}dB (ç›¸å…³:{result['correlation']:.4f})")
            print(f"       æ–‡ä»¶: {result['path']}")
        
        best_snr = final_results[0]['snr']
        worst_snr = final_results[-1]['snr'] if len(final_results) > 1 else best_snr
        improvement = best_snr - worst_snr
        
        print(f"\\nðŸš€ æ€§èƒ½åˆ†æž:")
        print(f"   ðŸ“ˆ æœ€ä½³SNR: {best_snr:.2f}dB ({final_results[0]['method']})")
        print(f"   ðŸ“‰ æœ€å·®SNR: {worst_snr:.2f}dB")
        print(f"   ðŸ“Š æ–¹æ³•å·®å¼‚: {improvement:.2f}dB")
    else:
        print(f"\\nâŒ æ²¡æœ‰æˆåŠŸçš„é‡å»ºç»“æžœ")
    
    print(f"\\nðŸ“ ç»“æžœä¿å­˜åœ¨: {output_dir}/")
    print(f"ðŸŽ§ è¯·æ’­æ”¾éŸ³é¢‘æ–‡ä»¶è¿›è¡Œä¸»è§‚è¯„ä¼°")
    print(f"\\nâœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        audio_path = sys.argv[1]
    else:
        audio_path = "AudioLDM2_Music_output.wav"
    
    print(f"ðŸš€ å¼€å§‹ç®€åŒ–VAEæµ‹è¯•: {audio_path}")
    simple_vae_test(audio_path)
