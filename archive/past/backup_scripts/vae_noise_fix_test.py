#!/usr/bin/env python3
"""
AudioLDM2 VAE é‡å»ºæµ‹è¯• - å™ªéŸ³ä¿®å¤ç‰ˆæœ¬
ä¸»è¦è§£å†³HiFiGANç”Ÿæˆçš„"å’”å“’å’”å“’"å™ªéŸ³é—®é¢˜

å™ªéŸ³ä¿®å¤ç­–ç•¥:
1. è¾“å…¥å½’ä¸€åŒ–ä¼˜åŒ–
2. è¾¹ç•Œå¹³æ»‘å¤„ç†
3. çª—å£å‡½æ•°åº”ç”¨
4. åå¤„ç†æ»¤æ³¢
5. æ¸å˜æ·¡å…¥æ·¡å‡º
"""

import torch
import torchaudio
import numpy as np
import librosa
import time
import os
import sys
from pathlib import Path
from diffusers import AudioLDM2Pipeline
import scipy.signal
from scipy.ndimage import gaussian_filter1d


def apply_fade_in_out(audio, fade_samples=1024):
    """åº”ç”¨æ¸å˜æ·¡å…¥æ·¡å‡ºæ¥å‡å°‘è¾¹ç•Œç‚¹å‡»"""
    if len(audio) <= 2 * fade_samples:
        fade_samples = len(audio) // 4
    
    # æ·¡å…¥
    fade_in = np.linspace(0, 1, fade_samples)
    audio[:fade_samples] *= fade_in
    
    # æ·¡å‡º
    fade_out = np.linspace(1, 0, fade_samples)
    audio[-fade_samples:] *= fade_out
    
    return audio


def smooth_spectrogram_boundaries(mel_spec, boundary_frames=4):
    """å¹³æ»‘mel-spectrogramçš„è¾¹ç•Œä»¥å‡å°‘ä¸è¿ç»­æ€§"""
    if mel_spec.shape[-1] <= 2 * boundary_frames:
        return mel_spec
    
    # å¯¹å¼€å§‹å’Œç»“æŸå¸§åº”ç”¨å¹³æ»‘
    # ä½¿ç”¨é«˜æ–¯æ»¤æ³¢å™¨å¹³æ»‘è¾¹ç•Œ
    smoothed = mel_spec.clone()
    
    # å¹³æ»‘å¼€å§‹éƒ¨åˆ†
    for i in range(boundary_frames):
        weight = i / boundary_frames
        smoothed[..., i] = mel_spec[..., i] * weight + mel_spec[..., boundary_frames] * (1 - weight)
    
    # å¹³æ»‘ç»“æŸéƒ¨åˆ†
    for i in range(boundary_frames):
        idx = -(i+1)
        weight = i / boundary_frames
        smoothed[..., idx] = mel_spec[..., idx] * weight + mel_spec[..., -(boundary_frames+1)] * (1 - weight)
    
    return smoothed


def normalize_mel_for_hifigan(mel_spec, target_mean=-4.0, target_std=4.0):
    """
    ä¸“é—¨ä¸ºHiFiGANä¼˜åŒ–çš„mel-spectrogramå½’ä¸€åŒ–
    åŸºäºAudioLDM2è®­ç»ƒæ—¶ä½¿ç”¨çš„ç»Ÿè®¡ä¿¡æ¯
    """
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œè®¡ç®—
    if isinstance(mel_spec, torch.Tensor):
        mel_np = mel_spec.cpu().numpy()
    else:
        mel_np = mel_spec
    
    # æ£€æŸ¥è¾“å…¥çš„æœ‰æ•ˆæ€§
    if not np.isfinite(mel_np).all():
        print(f"   âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆå€¼ï¼Œåº”ç”¨ä¿®å¤...")
        mel_np = np.nan_to_num(mel_np, nan=target_mean, posinf=target_mean + 2*target_std, neginf=target_mean - 2*target_std)
    
    # è®¡ç®—å½“å‰ç»Ÿè®¡ä¿¡æ¯
    current_mean = np.mean(mel_np)
    current_std = np.std(mel_np)
    
    print(f"   Melå½’ä¸€åŒ–å‰: mean={current_mean:.3f}, std={current_std:.3f}")
    
    # æ£€æŸ¥æ ‡å‡†å·®æ˜¯å¦æœ‰æ•ˆ
    if not np.isfinite(current_std) or current_std < 1e-6:
        print(f"   âš ï¸ æ ‡å‡†å·®å¼‚å¸¸ï¼Œä½¿ç”¨ç®€å•ç¼©æ”¾...")
        # å¦‚æœæ ‡å‡†å·®å¼‚å¸¸ï¼Œä½¿ç”¨ç®€å•çš„çº¿æ€§æ˜ å°„
        min_val = np.min(mel_np)
        max_val = np.max(mel_np)
        if max_val > min_val:
            normalized = (mel_np - min_val) / (max_val - min_val)
            normalized = normalized * (2 * target_std) + (target_mean - target_std)
        else:
            normalized = np.full_like(mel_np, target_mean)
    else:
        # æ ‡å‡†åŒ–å¹¶è°ƒæ•´åˆ°ç›®æ ‡åˆ†å¸ƒ
        normalized = (mel_np - current_mean) / current_std
        normalized = normalized * target_std + target_mean
    
    # åº”ç”¨åˆç†çš„è£å‰ª
    normalized = np.clip(normalized, target_mean - 3*target_std, target_mean + 3*target_std)
    
    # ç¡®ä¿æ²¡æœ‰æ— æ•ˆå€¼
    normalized = np.nan_to_num(normalized, nan=target_mean, posinf=target_mean + 2*target_std, neginf=target_mean - 2*target_std)
    
    final_mean = np.mean(normalized)
    final_std = np.std(normalized)
    print(f"   Melå½’ä¸€åŒ–å: mean={final_mean:.3f}, std={final_std:.3f}")
    
    if isinstance(mel_spec, torch.Tensor):
        return torch.from_numpy(normalized).to(mel_spec.device).to(mel_spec.dtype)
    else:
        return normalized


def apply_post_filter(audio, sr=16000, cutoff_low=50, cutoff_high=7500):
    """åº”ç”¨åå¤„ç†æ»¤æ³¢å™¨ç§»é™¤ä¸éœ€è¦çš„é¢‘ç‡æˆåˆ†"""
    # è®¾è®¡å¸¦é€šæ»¤æ³¢å™¨
    nyquist = sr / 2
    low = cutoff_low / nyquist
    high = cutoff_high / nyquist
    
    # åº”ç”¨å·´ç‰¹æ²ƒæ–¯æ»¤æ³¢å™¨
    b, a = scipy.signal.butter(4, [low, high], btype='band')
    filtered_audio = scipy.signal.filtfilt(b, a, audio)
    
    return filtered_audio


def remove_dc_bias(audio):
    """ç§»é™¤ç›´æµåç½®"""
    return audio - np.mean(audio)


def load_and_test_vae_noise_fixed(audio_path, max_length=10):
    """
    åŠ è½½éŸ³é¢‘å¹¶æ‰§è¡ŒVAEé‡å»ºæµ‹è¯• - å™ªéŸ³ä¿®å¤ç‰ˆæœ¬
    """
    print(f"\nğŸµ åŠ è½½å’Œæµ‹è¯•éŸ³é¢‘: {audio_path}")
    print(f"â±ï¸ æœ€å¤§é•¿åº¦: {max_length} ç§’")
    
    # æ£€æŸ¥CUDA
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½ AudioLDM2 æ¨¡å‹
    print("ğŸ“¦ åŠ è½½ AudioLDM2 æ¨¡å‹...")
    try:
        pipeline = AudioLDM2Pipeline.from_pretrained(
            "cvssp/audioldm2",
            torch_dtype=torch.float16 if device == "cuda" else torch.float32
        ).to(device)
        
        vae = pipeline.vae
        vocoder = pipeline.vocoder
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   VAE: {type(vae).__name__}")
        print(f"   Vocoder: {type(vocoder).__name__}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None
    
    # åŠ è½½éŸ³é¢‘æ–‡ä»¶
    print(f"ğŸ¶ åŠ è½½éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    try:
        # å¼ºåˆ¶é‡‡æ ·ç‡ä¸º16kHz (AudioLDM2é»˜è®¤)
        sample_rate = 16000
        audio, orig_sr = torchaudio.load(audio_path)
        
        if orig_sr != sample_rate:
            print(f"   é‡é‡‡æ ·: {orig_sr}Hz -> {sample_rate}Hz")
            audio = torchaudio.functional.resample(audio, orig_sr, sample_rate)
        
        # è½¬æ¢ä¸ºå•å£°é“
        if audio.shape[0] > 1:
            audio = torch.mean(audio, dim=0, keepdim=True)
        
        audio = audio.squeeze().numpy()
        
        # æˆªå–æŒ‡å®šé•¿åº¦
        max_samples = int(max_length * sample_rate)
        if len(audio) > max_samples:
            audio = audio[:max_samples]
            print(f"   æˆªå–åˆ° {max_length} ç§’")
        
        print(f"   éŸ³é¢‘é•¿åº¦: {len(audio)} æ ·æœ¬ ({len(audio)/sample_rate:.2f} ç§’)")
        
    except Exception as e:
        print(f"âŒ éŸ³é¢‘åŠ è½½å¤±è´¥: {e}")
        return None
    
    # é¢„å¤„ç†ï¼šç§»é™¤ç›´æµåç½®å’Œå½’ä¸€åŒ–
    audio = remove_dc_bias(audio)
    if np.max(np.abs(audio)) > 0:
        audio = audio / np.max(np.abs(audio)) * 0.95  # è½»å¾®é™ä½å¹…åº¦ä»¥é¿å…å‰Šæ³¢
    
    # ç”Ÿæˆ mel-spectrogram
    print("ğŸ”„ ç”Ÿæˆ mel-spectrogram...")
    
    # ä½¿ç”¨ä¸AudioLDM2ä¸€è‡´çš„å‚æ•°
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sample_rate,
        n_fft=1024,        # AudioLDM2é»˜è®¤
        hop_length=160,    # AudioLDM2é»˜è®¤
        n_mels=64,         # AudioLDM2é»˜è®¤
        fmin=0,
        fmax=8000
    )
    
    # è½¬æ¢ä¸ºdBå¹¶åº”ç”¨åˆç†èŒƒå›´
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max, top_db=80)
    mel_spec_db = np.clip(mel_spec_db, -80, 0)
    
    print(f"   åŸå§‹ mel å½¢çŠ¶: {mel_spec_db.shape}")
    print(f"   åŸå§‹ mel èŒƒå›´: [{np.min(mel_spec_db):.2f}, {np.max(mel_spec_db):.2f}] dB")
    
    # ä¸ºVAEå‡†å¤‡è¾“å…¥
    with torch.no_grad():
        # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        mel_tensor = torch.from_numpy(mel_spec_db).to(device).to(vae.dtype)
        
        # è°ƒæ•´å½¢çŠ¶ä¸º [batch, channels, height, width]
        mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, time]
        
        # ç¡®ä¿æ—¶é—´ç»´åº¦æ˜¯8çš„å€æ•°ï¼ˆVAEè¦æ±‚ï¼‰
        time_dim = mel_input.shape[-1]
        if time_dim % 8 != 0:
            pad_width = 8 - (time_dim % 8)
            mel_input = torch.nn.functional.pad(mel_input, (0, pad_width), mode='constant', value=-80)
            print(f"   å¡«å…… mel åˆ°: {mel_input.shape}")
        
        # è¾¹ç•Œå¹³æ»‘å¤„ç†
        mel_input_smoothed = smooth_spectrogram_boundaries(mel_input, boundary_frames=4)
        
        # å½’ä¸€åŒ–ä¸ºVAEæœŸæœ›çš„èŒƒå›´ [-1, 1]
        mel_min, mel_max = -80.0, 0.0
        mel_normalized = 2.0 * (mel_input_smoothed - mel_min) / (mel_max - mel_min) - 1.0
        mel_normalized = torch.clamp(mel_normalized, -1, 1)
        
        print(f"   VAEè¾“å…¥å½¢çŠ¶: {mel_normalized.shape}")
        print(f"   VAEè¾“å…¥èŒƒå›´: [{mel_normalized.min().item():.3f}, {mel_normalized.max().item():.3f}]")
        
        # VAE ç¼–ç 
        print("ğŸ”§ VAE ç¼–ç ...")
        start_time = time.time()
        
        try:
            latents = vae.encode(mel_normalized).latent_dist.sample()
            latents = latents * vae.config.scaling_factor
            
            encode_time = time.time() - start_time
            print(f"   ç¼–ç å®Œæˆ: {encode_time:.2f}ç§’")
            print(f"   æ½œåœ¨å½¢çŠ¶: {latents.shape}")
            print(f"   æ½œåœ¨èŒƒå›´: [{latents.min().item():.3f}, {latents.max().item():.3f}]")
            
        except Exception as e:
            print(f"âŒ VAEç¼–ç å¤±è´¥: {e}")
            return None
        
        # VAE è§£ç 
        print("ğŸ”§ VAE è§£ç ...")
        decode_start = time.time()
        
        try:
            latents_scaled = latents / vae.config.scaling_factor
            reconstructed_mel = vae.decode(latents_scaled).sample
            
            decode_time = time.time() - decode_start
            print(f"   è§£ç å®Œæˆ: {decode_time:.2f}ç§’")
            print(f"   é‡å»º mel å½¢çŠ¶: {reconstructed_mel.shape}")
            print(f"   é‡å»º mel èŒƒå›´: [{reconstructed_mel.min().item():.3f}, {reconstructed_mel.max().item():.3f}]")
            
        except Exception as e:
            print(f"âŒ VAEè§£ç å¤±è´¥: {e}")
            return None
    
    # éŸ³é¢‘é‡å»º
    print("ğŸ”Š éŸ³é¢‘é‡å»º...")
      with torch.no_grad():
        # æ–¹æ³•1: ä½¿ç”¨AudioLDM2å†…ç½®çš„HiFiGAN (å™ªéŸ³ä¿®å¤ç‰ˆæœ¬)
        print("ğŸ¤ ä½¿ç”¨AudioLDM2å†…ç½®HiFiGAN (å™ªéŸ³ä¿®å¤ç‰ˆæœ¬)...")
        
        try:
            # åå½’ä¸€åŒ– - ç¡®ä¿èŒƒå›´åˆç†
            mel_denorm = (reconstructed_mel + 1) / 2 * (mel_max - mel_min) + mel_min
            mel_denorm = torch.clamp(mel_denorm, mel_min, mel_max)
            
            # æ£€æŸ¥è§£ç åçš„æœ‰æ•ˆæ€§
            if not torch.isfinite(mel_denorm).all():
                print(f"   âš ï¸ æ£€æµ‹åˆ°VAEè§£ç è¾“å‡ºçš„æ— æ•ˆå€¼ï¼Œåº”ç”¨ä¿®å¤...")
                mel_denorm = torch.nan_to_num(mel_denorm, nan=-40.0, posinf=-20.0, neginf=-80.0)
                mel_denorm = torch.clamp(mel_denorm, mel_min, mel_max)
            
            # å‡†å¤‡HiFiGANè¾“å…¥: [batch, time, mel_dim]
            vocoder_input = mel_denorm.squeeze(0).squeeze(0)  # [64, time]
            vocoder_input = vocoder_input.transpose(-2, -1)   # [time, 64]
            vocoder_input = vocoder_input.unsqueeze(0)        # [1, time, 64]
            
            print(f"   HiFiGANè¾“å…¥å½¢çŠ¶ (é¢„å¤„ç†): {vocoder_input.shape}")
            print(f"   HiFiGANè¾“å…¥èŒƒå›´ (é¢„å¤„ç†): [{vocoder_input.min().item():.3f}, {vocoder_input.max().item():.3f}]")
            
            # ä¸“é—¨ä¸ºHiFiGANä¼˜åŒ–å½’ä¸€åŒ–
            vocoder_input_norm = normalize_mel_for_hifigan(vocoder_input)
            
            print(f"   HiFiGANè¾“å…¥å½¢çŠ¶ (æœ€ç»ˆ): {vocoder_input_norm.shape}")
            print(f"   HiFiGANè¾“å…¥èŒƒå›´ (æœ€ç»ˆ): [{vocoder_input_norm.min().item():.3f}, {vocoder_input_norm.max().item():.3f}]")
            
            # ä½¿ç”¨AudioLDM2å†…ç½®HiFiGAN
            reconstructed_audio_tensor = vocoder(vocoder_input_norm)
            reconstructed_audio = reconstructed_audio_tensor.squeeze().cpu().numpy()
            
            print(f"   âœ… HiFiGANæˆåŠŸ: è¾“å‡º{len(reconstructed_audio)}æ ·æœ¬")
            
            # åå¤„ç†æ­¥éª¤
            print("ğŸ› ï¸ åº”ç”¨åå¤„ç†...")
            
            # 1. ç§»é™¤ç›´æµåç½®
            reconstructed_audio = remove_dc_bias(reconstructed_audio)
            
            # 2. åº”ç”¨æ¸å˜æ·¡å…¥æ·¡å‡º
            reconstructed_audio = apply_fade_in_out(reconstructed_audio, fade_samples=512)
            
            # 3. åå¤„ç†æ»¤æ³¢
            reconstructed_audio = apply_post_filter(reconstructed_audio, sr=sample_rate)
            
            # 4. è½»å¾®å¹³æ»‘
            reconstructed_audio = gaussian_filter1d(reconstructed_audio, sigma=0.5)
            
            # 5. æœ€ç»ˆå½’ä¸€åŒ–
            if np.max(np.abs(reconstructed_audio)) > 0:
                reconstructed_audio = reconstructed_audio / np.max(np.abs(reconstructed_audio)) * 0.9
            
            vocoder_method = "AudioLDM2_HiFiGAN_NoiseFixed"
            print(f"   âœ… åå¤„ç†å®Œæˆ")
            
        except Exception as e:
            print(f"   âŒ HiFiGANå¤±è´¥: {e}")
            print("ğŸ“Š é™çº§åˆ°Griffin-Limç®—æ³•...")
            
            # é™çº§æ–¹æ¡ˆ: Griffin-Lim
            try:
                # åå½’ä¸€åŒ– mel-spectrogram
                recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
                recon_mel_denorm = (recon_mel_np + 1) / 2 * (mel_max - mel_min) + mel_min
                recon_mel_denorm = np.clip(recon_mel_denorm, -80.0, 0.0)
                
                # è½¬æ¢å›åŠŸç‡è°±
                recon_mel_power = librosa.db_to_power(recon_mel_denorm)
                recon_mel_power = np.clip(recon_mel_power, 1e-10, None)
                
                # Griffin-Limç®—æ³•
                reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                    recon_mel_power,
                    sr=sample_rate,
                    hop_length=160,
                    n_fft=1024,
                    fmin=0,
                    fmax=8000
                )
                
                # åå¤„ç†
                reconstructed_audio = remove_dc_bias(reconstructed_audio)
                reconstructed_audio = apply_fade_in_out(reconstructed_audio, fade_samples=512)
                
                vocoder_method = "Griffin_Lim_PostProcessed"
                print(f"   âœ… Griffin-LimæˆåŠŸ: è¾“å‡º{len(reconstructed_audio)}æ ·æœ¬")
                
            except Exception as griffin_e:
                print(f"   âŒ Griffin-Limä¹Ÿå¤±è´¥: {griffin_e}")
                reconstructed_audio = np.random.randn(len(audio)) * 0.01
                vocoder_method = "Fallback_Noise"
    
    # ç¡®ä¿éŸ³é¢‘é•¿åº¦ä¸€è‡´
    if len(reconstructed_audio) > len(audio):
        reconstructed_audio = reconstructed_audio[:len(audio)]
    elif len(reconstructed_audio) < len(audio):
        reconstructed_audio = np.pad(reconstructed_audio, (0, len(audio) - len(reconstructed_audio)))
    
    # ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    output_dir = "vae_noise_fix_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    audio_norm = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio
    torchaudio.save(original_path, torch.from_numpy(audio_norm).unsqueeze(0), sample_rate)
    
    # ä¿å­˜é‡å»ºéŸ³é¢‘
    reconstructed_path = os.path.join(output_dir, f"{input_name}_reconstructed_noisefixed_{timestamp}.wav")
    recon_norm = reconstructed_audio / np.max(np.abs(reconstructed_audio)) if np.max(np.abs(reconstructed_audio)) > 0 else reconstructed_audio
    torchaudio.save(reconstructed_path, torch.from_numpy(recon_norm).unsqueeze(0), sample_rate)
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    print("ğŸ“Š è®¡ç®—è´¨é‡æŒ‡æ ‡...")
    min_len = min(len(audio), len(reconstructed_audio))
    orig_segment = audio[:min_len]
    recon_segment = reconstructed_audio[:min_len]
    
    # MSE å’Œç›¸å…³ç³»æ•°
    mse = np.mean((orig_segment - recon_segment) ** 2)
    correlation = np.corrcoef(orig_segment, recon_segment)[0, 1] if len(orig_segment) > 1 else 0
    
    # SNR
    signal_power = np.mean(orig_segment ** 2)
    noise_power = np.mean((orig_segment - recon_segment) ** 2)
    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
    
    # å‹ç¼©æ¯”
    original_size = mel_normalized.numel()
    compressed_size = latents.numel()
    compression_ratio = original_size / compressed_size
    
    # æ‰“å°ç»“æœ
    print(f"\n{'='*60}")
    print(f"VAE é‡å»ºæµ‹è¯•ç»“æœ - å™ªéŸ³ä¿®å¤ç‰ˆæœ¬")
    print(f"{'='*60}")
    print(f"åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ç¼–ç æ—¶é—´: {encode_time:.2f}ç§’")
    print(f"è§£ç æ—¶é—´: {decode_time:.2f}ç§’")
    print(f"æ€»æ—¶é—´: {encode_time + decode_time:.2f}ç§’")
    print(f"MSE: {mse:.6f}")
    print(f"SNR: {snr:.2f} dB")
    print(f"ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"Mel-spectrogram å½¢çŠ¶: {mel_normalized.shape}")
    print(f"æ½œåœ¨è¡¨ç¤ºå½¢çŠ¶: {latents.shape}")
    print(f"å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
    print(f"é‡å»ºæ–¹æ³•: {vocoder_method}")
    print(f"å™ªéŸ³ä¿®å¤: âœ… åº”ç”¨äº†è¾¹ç•Œå¹³æ»‘ã€æ¸å˜æ·¡å…¥æ·¡å‡ºã€åå¤„ç†æ»¤æ³¢")
    
    return {
        'original_path': original_path,
        'reconstructed_path': reconstructed_path,
        'mse': mse,
        'snr': snr,
        'correlation': correlation,
        'encode_time': encode_time,
        'decode_time': decode_time,
        'compression_ratio': compression_ratio,
        'vocoder_method': vocoder_method
    }


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python vae_noise_fix_test.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„> [æœ€å¤§é•¿åº¦ç§’æ•°]")
        
        # æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„éŸ³é¢‘æ–‡ä»¶
        audio_files = []
        for ext in ['.wav', '.mp3', '.flac']:
            audio_files.extend(Path('.').glob(f'*{ext}'))
        
        if audio_files:
            print(f"\næ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files, 1):
                print(f"{i}. {file}")
            
            try:
                choice = input("è¯·é€‰æ‹©æ–‡ä»¶åºå·: ").strip()
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(audio_files):
                    audio_path = str(audio_files[file_idx])
                else:
                    print("æ— æ•ˆé€‰æ‹©")
                    return
            except (ValueError, KeyboardInterrupt):
                print("å–æ¶ˆæ“ä½œ")
                return
        else:
            print("å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    # è·å–æœ€å¤§é•¿åº¦å‚æ•°
    max_length = 10  # é»˜è®¤10ç§’
    if len(sys.argv) >= 3:
        try:
            max_length = float(sys.argv[2])
        except ValueError:
            print(f"æ— æ•ˆçš„é•¿åº¦å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {max_length} ç§’")
    
    # æ‰§è¡Œæµ‹è¯•
    print(f"ğŸš€ å¼€å§‹å™ªéŸ³ä¿®å¤æµ‹è¯•")
    print(f"éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    print(f"æœ€å¤§é•¿åº¦: {max_length} ç§’")
    
    try:
        result = load_and_test_vae_noise_fixed(audio_path, max_length=max_length)
        if result:
            print("\nâœ… å™ªéŸ³ä¿®å¤æµ‹è¯•å®Œæˆï¼")
            print("è¯·æ’­æ”¾é‡å»ºéŸ³é¢‘æ£€æŸ¥å™ªéŸ³æ˜¯å¦å‡å°‘ã€‚")
            print("\nğŸ” å»ºè®®æ¯”è¾ƒ:")
            print("1. åŸå§‹éŸ³é¢‘ vs é‡å»ºéŸ³é¢‘")
            print("2. ä¸ä¹‹å‰ç‰ˆæœ¬çš„è¾“å‡ºè¿›è¡Œå¯¹æ¯”")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
