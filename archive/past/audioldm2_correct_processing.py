#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
AudioLDM2 æ­£ç¡®çš„éŸ³é¢‘å¤„ç†æ–¹å¼
=========================

ä½¿ç”¨ClapFeatureExtractorï¼Œå®Œå…¨æ¨¡æ‹ŸAudioLDM2çš„å†…éƒ¨å¤„ç†
"""

import torch
import numpy as np
import librosa
import soundfile as sf
from pathlib import Path
from diffusers import AudioLDM2Pipeline
import time
import sys


def save_audio_compatible(audio_data, filepath, sample_rate=16000):
    """ä¿å­˜å…¼å®¹çš„éŸ³é¢‘æ–‡ä»¶"""
    if isinstance(audio_data, torch.Tensor):
        audio_data = audio_data.detach().cpu().numpy()
    
    if len(audio_data.shape) > 1:
        audio_data = audio_data.flatten()
    
    audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=0.0, neginf=0.0)
    
    if np.max(np.abs(audio_data)) > 0:
        audio_data = audio_data / np.max(np.abs(audio_data))
    
    try:
        sf.write(filepath, audio_data, sample_rate, subtype='PCM_16')
        print(f"   âœ… ä¿å­˜: {Path(filepath).name}")
        return True
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
        return False


def test_audioldm2_correct_processing(audio_path, max_length=5):
    """
    ä½¿ç”¨AudioLDM2çš„æ­£ç¡®å¤„ç†æ–¹å¼
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ¯ AudioLDM2 æ­£ç¡®å¤„ç†æ–¹å¼æµ‹è¯•")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # åŠ è½½AudioLDM2 pipeline
    print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    feature_extractor = pipeline.feature_extractor
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   VAE scaling_factor: {pipeline.vae.config.scaling_factor}")
    print(f"   FeatureExtractor: {type(feature_extractor).__name__}")
    print(f"   æœŸæœ›é‡‡æ ·ç‡: {feature_extractor.sampling_rate} Hz")
    
    # åŠ è½½éŸ³é¢‘ - å…³é”®ï¼šä½¿ç”¨48kHzé‡‡æ ·ç‡
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘: {Path(audio_path).name}")
    # åŒæ—¶åŠ è½½48kHzå’Œ16kHzç‰ˆæœ¬ç”¨äºå¯¹æ¯”
    audio_48k, sr_48k = librosa.load(audio_path, sr=feature_extractor.sampling_rate, duration=max_length)
    audio_16k, sr_16k = librosa.load(audio_path, sr=16000, duration=max_length)
    
    print(f"   48kHzéŸ³é¢‘: {len(audio_48k)/sr_48k:.2f}ç§’, èŒƒå›´[{audio_48k.min():.3f}, {audio_48k.max():.3f}]")
    print(f"   16kHzéŸ³é¢‘: {len(audio_16k)/sr_16k:.2f}ç§’, èŒƒå›´[{audio_16k.min():.3f}, {audio_16k.max():.3f}]")
    
    # ä½¿ç”¨ClapFeatureExtractor - å…³é”®ä¿®å¤
    print(f"\nğŸµ ä½¿ç”¨ClapFeatureExtractorå¤„ç†...")
    try:
        # ä½¿ç”¨AudioLDM2çš„æ­£ç¡®ç‰¹å¾æå–æ–¹å¼
        features = feature_extractor(
            audio_48k, 
            return_tensors="pt", 
            sampling_rate=feature_extractor.sampling_rate
        )
        
        mel_input = features.input_features.to(device)
        if device == "cuda":
            mel_input = mel_input.half()
        
        print(f"   ç‰¹å¾æå–æˆåŠŸ")
        print(f"   è¾“å…¥: {mel_input.shape} (æ³¨æ„ï¼štimeåœ¨å‰ï¼Œfeatureåœ¨å)")
        print(f"   èŒƒå›´: [{mel_input.min():.3f}, {mel_input.max():.3f}]")
        
        # æ£€æŸ¥ç»´åº¦ - AudioLDM2æœŸæœ›[batch, channel, time, feature]
        if mel_input.dim() == 4 and mel_input.shape[2] > mel_input.shape[3]:
            # å½“å‰æ˜¯[1, 1, time, 64]ï¼Œéœ€è¦è½¬æ¢ä¸º[1, 1, 64, time]
            print(f"   ç»´åº¦è½¬æ¢å‰: {mel_input.shape}")
            # ä¸éœ€è¦è½¬æ¢ï¼AudioLDM2çš„VAEæœŸæœ›[batch, channel, time, feature]æ ¼å¼
            vae_input = mel_input
        else:
            vae_input = mel_input
            
        print(f"   VAEè¾“å…¥: {vae_input.shape}")
        
    except Exception as e:
        print(f"   âŒ ClapFeatureExtractorå¤±è´¥: {e}")
        print(f"   ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿmelå¤„ç†...")
        
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨ä¼ ç»Ÿmelå¤„ç†ä½†æ¨¡æ‹ŸClapFeatureExtractorçš„å‚æ•°
        mel_spec = librosa.feature.melspectrogram(
            y=audio_48k, 
            sr=48000, 
            n_mels=64,
            hop_length=480,  # ClapFeatureExtractorçš„hop_length
            n_fft=1024,
            fmin=50,         # ClapFeatureExtractorçš„é¢‘ç‡èŒƒå›´
            fmax=14000,
            window='hann',
            center=True,
            pad_mode='reflect'
        )
        
        mel_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        # è½¬æ¢ä¸ºAudioLDM2æœŸæœ›çš„æ ¼å¼
        mel_tensor = torch.from_numpy(mel_db).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.half()
        
        # è½¬æ¢ç»´åº¦ï¼š[64, time] -> [1, 1, time, 64]
        vae_input = mel_tensor.transpose(0, 1).unsqueeze(0).unsqueeze(0)
        
        print(f"   å›é€€å¤„ç†æˆåŠŸ")
        print(f"   VAEè¾“å…¥: {vae_input.shape}")
        print(f"   èŒƒå›´: [{vae_input.min():.3f}, {vae_input.max():.3f}]")
    
    # VAEå¤„ç†
    print(f"\nğŸ§  VAEç¼–ç è§£ç ...")
    with torch.no_grad():
        # ç¼–ç 
        latent_dist = pipeline.vae.encode(vae_input)
        if hasattr(latent_dist, 'latent_dist'):
            latent = latent_dist.latent_dist.sample()
        else:
            latent = latent_dist.sample()
        
        # åº”ç”¨scaling_factor
        latent = latent * pipeline.vae.config.scaling_factor
        print(f"   ç¼–ç latent: {latent.shape}")
        print(f"   LatentèŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
        
        # è§£ç 
        latent_for_decode = latent / pipeline.vae.config.scaling_factor
        reconstructed_mel = pipeline.vae.decode(latent_for_decode).sample
        
        print(f"   è§£ç è¾“å‡º: {reconstructed_mel.shape}")
        print(f"   è¾“å‡ºèŒƒå›´: [{reconstructed_mel.min():.3f}, {reconstructed_mel.max():.3f}]")
    
    # HiFiGANå¤„ç† - ä½¿ç”¨pipelineçš„æ ‡å‡†æ–¹æ³•
    print(f"\nğŸ¤ HiFiGAN vocoder...")
    try:
        print(f"   ğŸš€ ä½¿ç”¨pipeline.mel_spectrogram_to_waveform...")
        print(f"   è¾“å…¥åˆ°vocoder: {reconstructed_mel.shape}")
          # å…³é”®ï¼šç›´æ¥ä½¿ç”¨pipelineçš„æ–¹æ³•ï¼Œå®ƒçŸ¥é“å¦‚ä½•å¤„ç†ç»´åº¦
        waveform = pipeline.mel_spectrogram_to_waveform(reconstructed_mel)
        reconstructed_audio = waveform.squeeze().detach().cpu().float().numpy()
        vocoder_method = "AudioLDM2_ClapFeatureExtractor"
        
        print(f"   âœ… æˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
        print(f"   è¾“å‡ºèŒƒå›´: [{reconstructed_audio.min():.3f}, {reconstructed_audio.max():.3f}]")
        
        # è°ƒæ•´é‡‡æ ·ç‡åˆ°16kHzç”¨äºä¿å­˜
        if len(reconstructed_audio) > 0:
            reconstructed_audio_16k = librosa.resample(
                reconstructed_audio, 
                orig_sr=48000, 
                target_sr=16000
            )
        else:
            reconstructed_audio_16k = reconstructed_audio
            
    except Exception as e:
        print(f"   âŒ Pipelineæ–¹æ³•å¤±è´¥: {e}")
        print(f"   å…·ä½“é”™è¯¯: {str(e)}")
        
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨å¤„ç†ç»´åº¦
        try:
            print(f"   ğŸ”„ å°è¯•æ‰‹åŠ¨å¤„ç†ç»´åº¦...")
            
            # æ‰‹åŠ¨å¤„ç†ç»´åº¦åŒ¹é…
            if reconstructed_mel.dim() == 4:
                # ä»[1, 1, time, 64]è½¬æ¢ä¸º[1, time, 64]
                vocoder_input = reconstructed_mel.squeeze(1)
            else:
                vocoder_input = reconstructed_mel
                
            print(f"   æ‰‹åŠ¨å¤„ç†å: {vocoder_input.shape}")
            
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            vocoder_dtype = next(pipeline.vocoder.parameters()).dtype
            vocoder_input = vocoder_input.to(vocoder_dtype)
              # ç›´æ¥è°ƒç”¨vocoder
            waveform = pipeline.vocoder(vocoder_input)
            reconstructed_audio = waveform.squeeze().detach().cpu().float().numpy()
            
            # è°ƒæ•´é‡‡æ ·ç‡
            reconstructed_audio_16k = librosa.resample(
                reconstructed_audio, 
                orig_sr=48000, 
                target_sr=16000
            )
            
            vocoder_method = "AudioLDM2_Manual_Dimension_Fix"
            print(f"   âœ… æ‰‹åŠ¨å¤„ç†æˆåŠŸï¼")
            
        except Exception as e2:
            print(f"   âŒ æ‰‹åŠ¨å¤„ç†ä¹Ÿå¤±è´¥: {e2}")
            return None
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    print(f"\nğŸ“Š è´¨é‡è¯„ä¼°...")
    
    # ä½¿ç”¨16kHzç‰ˆæœ¬è¿›è¡Œå¯¹æ¯”
    min_len = min(len(audio_16k), len(reconstructed_audio_16k))
    audio_aligned = audio_16k[:min_len]
    recon_aligned = reconstructed_audio_16k[:min_len]
    
    # è®¡ç®—æŒ‡æ ‡
    mse = np.mean((audio_aligned - recon_aligned) ** 2)
    correlation = np.corrcoef(audio_aligned, recon_aligned)[0, 1]
    signal_power = np.mean(audio_aligned ** 2)
    noise_power = np.mean((audio_aligned - recon_aligned) ** 2)
    snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
    
    print(f"   MSE: {mse:.6f}")
    print(f"   ç›¸å…³æ€§: {correlation:.4f}")
    print(f"   SNR: {snr:.2f} dB")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("audioldm2_correct_processing")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
    reconstructed_path = output_dir / f"{input_name}_{vocoder_method}_{timestamp}.wav"
    
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœ...")
    save_audio_compatible(audio_aligned, original_path)
    save_audio_compatible(reconstructed_audio_16k, reconstructed_path)
    
    # ç»“æœæŠ¥å‘Š
    print(f"\n{'='*60}")
    print(f"ğŸ¯ AudioLDM2 æ­£ç¡®å¤„ç†æ–¹å¼ç»“æœ")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ğŸ“Š è´¨é‡æŒ‡æ ‡:")
    print(f"   MSE: {mse:.6f}")
    print(f"   SNR: {snr:.2f} dB")
    print(f"   ç›¸å…³æ€§: {correlation:.4f}")
    print(f"ğŸ¤ æ–¹æ³•: {vocoder_method}")
    
    # å…³é”®å‘ç°
    print(f"\nğŸ” å…³é”®æŠ€æœ¯å‘ç°:")
    print(f"âœ… ä½¿ç”¨ClapFeatureExtractor (48kHz)")
    print(f"âœ… æ­£ç¡®çš„ç»´åº¦æ ¼å¼: [batch, channel, time, feature]")
    print(f"âœ… æ­£ç¡®çš„VAE scaling_factoråº”ç”¨")
    print(f"âœ… ä½¿ç”¨pipeline.mel_spectrogram_to_waveform")
    
    # è´¨é‡è¯„ä¼°
    if snr > 5:
        print(f"\nğŸ† ä¼˜ç§€ï¼é‡å»ºè´¨é‡å¾ˆé«˜ï¼Œå£°éŸ³æ›´é¥±æ»¡")
    elif snr > 0:
        print(f"\nâœ… è‰¯å¥½ï¼é‡å»ºè´¨é‡æ˜¾è‘—æ”¹å–„")
    elif snr > -5:
        print(f"\nğŸ‘ å¯æ¥å—ï¼æ¯”ä¹‹å‰çš„æ–¹æ³•æœ‰æ”¹å–„")
    else:
        print(f"\nâš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    print(f"\nğŸŠ è¿™æ˜¯AudioLDM2çš„æ­£ç¡®ä½¿ç”¨æ–¹å¼ï¼")
    
    return {
        'mse': mse,
        'snr': snr,
        'correlation': correlation,
        'vocoder_method': vocoder_method,
        'original_path': original_path,
        'reconstructed_path': reconstructed_path
    }


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        audio_files = list(Path('.').glob('*.wav'))
        if audio_files:
            print("æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files[:3], 1):
                print(f"{i}. {file.name}")
            
            try:
                choice = int(input("é€‰æ‹©æ–‡ä»¶: "))
                audio_path = str(audio_files[choice-1])
            except:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    print(f"ğŸš€ å¼€å§‹AudioLDM2æ­£ç¡®å¤„ç†æ–¹å¼æµ‹è¯•")
    
    try:
        result = test_audioldm2_correct_processing(audio_path)
        
        if result:
            print(f"\nğŸ“‹ æœ€ç»ˆæ€»ç»“:")
            print(f"   æ–¹æ³•: {result['vocoder_method']}")
            print(f"   è´¨é‡: SNR={result['snr']:.2f}dB, MSE={result['mse']:.6f}")
            print(f"   ç›¸å…³æ€§: {result['correlation']:.4f}")
            
            if result['snr'] > 0:
                print(f"\nğŸ‰ æˆåŠŸï¼ä½¿ç”¨AudioLDM2çš„æ­£ç¡®å¤„ç†æ–¹å¼")
                print(f"ğŸ”‘ å…³é”®ï¼šClapFeatureExtractor + 48kHz + æ­£ç¡®ç»´åº¦")
                print(f"ğŸ“ˆ å£°éŸ³åº”è¯¥æ›´é¥±æ»¡ï¼Œç»†èŠ‚æ›´ä¸°å¯Œ")
            else:
                print(f"\nğŸ” ç»§ç»­ä¼˜åŒ–ä¸­...")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
