#!/usr/bin/env python3
"""
AudioLDM2 DDCM ç®€åŒ–å®ç°
é‡ç‚¹æ¼”ç¤ºDDCMçš„æ ¸å¿ƒæ¦‚å¿µï¼šä½¿ç”¨ç æœ¬å™ªå£°è¿›è¡Œå‹ç¼©å’Œç”Ÿæˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
from diffusers import AudioLDM2Pipeline
import torchaudio
from pathlib import Path
import json
import time
import soundfile as sf
import warnings
warnings.filterwarnings("ignore")

class SimpleNoiseCodebook:
    """
    ç®€åŒ–çš„å™ªå£°ç æœ¬
    DDCMçš„æ ¸å¿ƒï¼šé¢„å®šä¹‰çš„å™ªå£°å‘é‡é›†åˆ
    """
    
    def __init__(self, codebook_size: int = 256, latent_shape: Tuple[int, int, int] = (8, 250, 16), device: str = "cpu"):
        """
        åˆå§‹åŒ–å™ªå£°ç æœ¬
        
        Args:
            codebook_size: ç æœ¬å¤§å°
            latent_shape: latentç»´åº¦ (C, H, W)
            device: è®¾å¤‡
        """
        self.codebook_size = codebook_size
        self.latent_shape = latent_shape
        self.device = device
          # åˆ›å»ºé«˜æ–¯å™ªå£°ç æœ¬
        self.noise_vectors = torch.randn(codebook_size, *latent_shape).to(device)
        self.usage_count = torch.zeros(codebook_size).to(device)
        
        print(f"âœ… å™ªå£°ç æœ¬åˆå§‹åŒ–: {codebook_size} å‘é‡, å½¢çŠ¶ {latent_shape}, è®¾å¤‡ {device}")
    
    def find_best_noise_index(self, target_latent: torch.Tensor) -> int:
        """
        ä¸ºç›®æ ‡latentæ‰¾åˆ°æœ€ä½³çš„å™ªå£°å‘é‡ç´¢å¼•
        DDCMæ ¸å¿ƒï¼šé€‰æ‹©æœ€é€‚åˆçš„å™ªå£°è€Œééšæœºå™ªå£°
        
        Args:
            target_latent: ç›®æ ‡latent [C, H, W]
            
        Returns:
            best_index: æœ€ä½³å™ªå£°ç´¢å¼•
        """
        target = target_latent.unsqueeze(0).to(self.device)  # [1, C, H, W]
        
        # è®¡ç®—ç›®æ ‡ä¸æ‰€æœ‰å™ªå£°å‘é‡çš„ç›¸ä¼¼åº¦
        similarities = []
        for i, noise in enumerate(self.noise_vectors):
            # ç®€å•çš„è´Ÿæ¬§æ°è·ç¦»ä½œä¸ºç›¸ä¼¼åº¦
            dist = -F.mse_loss(target, noise.unsqueeze(0), reduction='mean')
            similarities.append(dist.item())
        
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ï¼ˆè·ç¦»æœ€å°çš„ï¼‰
        best_index = int(np.argmax(similarities))
        self.usage_count[best_index] += 1
        
        return best_index
    
    def get_noise_by_index(self, index: int) -> torch.Tensor:
        """æ ¹æ®ç´¢å¼•è·å–å™ªå£°å‘é‡"""
        return self.noise_vectors[index].clone()
    
    def get_random_index(self) -> int:
        """è·å–éšæœºç´¢å¼•ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
        index = np.random.randint(0, self.codebook_size)
        self.usage_count[index] += 1
        return index
    
    def get_usage_stats(self) -> Dict:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        used = (self.usage_count > 0).sum().item()
        return {
            "used_vectors": used,
            "total_vectors": self.codebook_size,
            "usage_rate": used / self.codebook_size,
            "total_usage": self.usage_count.sum().item()
        }

class AudioLDM2_DDCM_Simple:
    """
    ç®€åŒ–çš„AudioLDM2 DDCMå®ç°
    æ¼”ç¤ºDDCMçš„åŸºæœ¬æ¦‚å¿µå’Œå‹ç¼©èƒ½åŠ›
    """
    
    def __init__(self, codebook_size: int = 256):
        """åˆå§‹åŒ–ç®€åŒ–DDCM"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸµ åˆå§‹åŒ–ç®€åŒ–ç‰ˆAudioLDM2 DDCM...")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        
        # åŠ è½½AudioLDM2
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            "cvssp/audioldm2-music",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
          # åˆ›å»ºå™ªå£°ç æœ¬
        self.codebook = SimpleNoiseCodebook(codebook_size, device=self.device)
        
        print(f"âœ… ç®€åŒ–DDCMåˆå§‹åŒ–å®Œæˆ")
    
    def encode_audio_to_latent(self, audio_path: str) -> torch.Tensor:
        """ç¼–ç éŸ³é¢‘ä¸ºlatentè¡¨ç¤º"""
        # åŠ è½½éŸ³é¢‘
        audio, sr = torchaudio.load(audio_path)
        print(f"   ğŸ“Š åŠ è½½éŸ³é¢‘: {audio.shape}, {sr}Hz")
        
        # é¢„å¤„ç†
        if sr != 48000:
            resampler = torchaudio.transforms.Resample(sr, 48000)
            audio = resampler(audio)
        
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # é™åˆ¶é•¿åº¦
        max_length = 48000 * 10  # 10ç§’
        if audio.shape[-1] > max_length:
            audio = audio[..., :max_length]
        
        with torch.no_grad():
            audio_numpy = audio.squeeze(0).cpu().numpy()
            
            # ç‰¹å¾æå–
            inputs = self.pipeline.feature_extractor(
                audio_numpy,
                sampling_rate=48000,
                return_tensors="pt"
            )
            
            mel_features = inputs["input_features"].to(self.device)
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            # VAEç¼–ç 
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode() if hasattr(latent_dist.latent_dist, 'mode') else latent_dist.latent_dist.sample()
            
            return latent.squeeze(0)  # ç§»é™¤batchç»´åº¦
    
    def compress_audio_ddcm(self, audio_path: str) -> Dict:
        """
        DDCMå‹ç¼©ï¼šå°†éŸ³é¢‘å‹ç¼©ä¸ºå™ªå£°ç æœ¬ç´¢å¼•
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            compression_data: å‹ç¼©æ•°æ®
        """
        print(f"ğŸ—œï¸ DDCMå‹ç¼©: {Path(audio_path).name}")
        
        # ç¼–ç ä¸ºlatent
        target_latent = self.encode_audio_to_latent(audio_path)
        print(f"   ğŸ“Š Target latent: {target_latent.shape}")
        
        # æ‰¾åˆ°æœ€ä½³å™ªå£°ç´¢å¼•
        best_index = self.codebook.find_best_noise_index(target_latent)
        
        # è®¡ç®—å‹ç¼©æ•ˆæœ
        original_size = target_latent.numel() * 4  # float32å­—èŠ‚
        compressed_size = 4  # ä¸€ä¸ªint32ç´¢å¼•
        compression_ratio = original_size / compressed_size
        
        # éªŒè¯é‡å»ºè´¨é‡
        best_noise = self.codebook.get_noise_by_index(best_index)
        reconstruction_error = F.mse_loss(target_latent, best_noise).item()
        
        result = {
            "input_file": audio_path,
            "best_noise_index": best_index,
            "reconstruction_error": reconstruction_error,
            "compression_ratio": compression_ratio,
            "original_size_bytes": original_size,
            "compressed_size_bytes": compressed_size,
            "target_latent_shape": list(target_latent.shape)
        }
        
        print(f"   âœ… DDCMå‹ç¼©å®Œæˆ")
        print(f"   ğŸ“Š æœ€ä½³å™ªå£°ç´¢å¼•: {best_index}")
        print(f"   ğŸ“Š é‡å»ºè¯¯å·®: {reconstruction_error:.6f}")
        print(f"   ğŸ“Š å‹ç¼©æ¯”: {compression_ratio:.0f}:1")
        
        return result
    
    def generate_from_ddcm(self, 
                          compressed_data: Dict,
                          prompt: str = "high quality music") -> torch.Tensor:
        """
        ä»DDCMå‹ç¼©æ•°æ®ç”ŸæˆéŸ³é¢‘
        
        Args:
            compressed_data: å‹ç¼©æ•°æ®
            prompt: ç”Ÿæˆæç¤º
            
        Returns:
            generated_audio: ç”Ÿæˆçš„éŸ³é¢‘
        """
        print(f"ğŸµ ä»DDCMæ•°æ®ç”ŸæˆéŸ³é¢‘")
        print(f"   ğŸ“ æç¤º: {prompt}")
        print(f"   ğŸ“š ä½¿ç”¨å™ªå£°ç´¢å¼•: {compressed_data['best_noise_index']}")
        
        # ä»ç æœ¬è·å–å™ªå£°
        noise_index = compressed_data["best_noise_index"]
        # selected_noise = self.codebook.get_noise_by_index(noise_index)
        
        # ç›®å‰ä½¿ç”¨æ ‡å‡†pipelineç”Ÿæˆï¼ˆæ¼”ç¤ºDDCMæ¦‚å¿µï¼‰
        # å®é™…DDCMéœ€è¦åœ¨diffusionè¿‡ç¨‹ä¸­ä½¿ç”¨é€‰å®šçš„å™ªå£°
        with torch.no_grad():
            result = self.pipeline(
                prompt=prompt,
                num_inference_steps=20,
                guidance_scale=7.5,
                audio_length_in_s=10.0
            )
            
            audio = torch.tensor(result.audios[0])
            print(f"   âœ… ç”Ÿæˆå®Œæˆ: {audio.shape}")
            
            return audio
    
    def compare_compression_methods(self, audio_path: str) -> Dict:
        """
        å¯¹æ¯”ä¸åŒçš„å‹ç¼©æ–¹æ³•
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            
        Returns:
            comparison_results: å¯¹æ¯”ç»“æœ
        """
        print(f"ğŸ” å‹ç¼©æ–¹æ³•å¯¹æ¯”")
        print("=" * 40)
        
        # DDCMå‹ç¼©
        ddcm_data = self.compress_audio_ddcm(audio_path)
        
        # éšæœºé€‰æ‹©å¯¹æ¯”
        target_latent = self.encode_audio_to_latent(audio_path)
        random_index = self.codebook.get_random_index()
        random_noise = self.codebook.get_noise_by_index(random_index)
        random_error = F.mse_loss(target_latent, random_noise).item()
        
        print(f"\nğŸ“Š å‹ç¼©å¯¹æ¯”ç»“æœ:")
        print(f"   DDCMæœ€ä½³ç´¢å¼•: {ddcm_data['best_noise_index']}, è¯¯å·®: {ddcm_data['reconstruction_error']:.6f}")
        print(f"   éšæœºç´¢å¼•: {random_index}, è¯¯å·®: {random_error:.6f}")
        print(f"   DDCMæ”¹è¿›: {(random_error - ddcm_data['reconstruction_error']) / random_error * 100:.1f}%")
        
        # ç”ŸæˆéŸ³é¢‘å¯¹æ¯”
        print(f"\nğŸµ ç”ŸæˆéŸ³é¢‘å¯¹æ¯”...")
        
        # DDCMç”Ÿæˆ
        ddcm_audio = self.generate_from_ddcm(ddcm_data, "beautiful orchestral music")
        
        # æ ‡å‡†ç”Ÿæˆï¼ˆéšæœºå™ªå£°ï¼‰
        standard_audio = self.generate_from_ddcm(
            {"best_noise_index": random_index}, 
            "beautiful orchestral music"
        )
        
        # ä¿å­˜ç»“æœ
        output_dir = Path("ddcm_comparison_simple")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = int(time.time())
        ddcm_path = output_dir / f"ddcm_compressed_{timestamp}.wav"
        standard_path = output_dir / f"standard_random_{timestamp}.wav"
        original_path = output_dir / f"original_{timestamp}.wav"
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘
        original_audio, sr = torchaudio.load(audio_path)
        if original_audio.shape[0] > 1:
            original_audio = original_audio.mean(dim=0, keepdim=True)
        
        # é™åˆ¶åˆ°10ç§’å¹¶é‡é‡‡æ ·åˆ°16kHzç”¨äºå¯¹æ¯”
        max_samples = 16000 * 10
        if original_audio.shape[-1] > max_samples:
            original_audio = original_audio[..., :max_samples]
        
        if sr != 16000:
            resampler = torchaudio.transforms.Resample(sr, 16000)
            original_audio = resampler(original_audio)
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        sf.write(ddcm_path, ddcm_audio.numpy(), 16000)
        sf.write(standard_path, standard_audio.numpy(), 16000)
        sf.write(original_path, original_audio.squeeze().numpy(), 16000)
        
        # è®¡ç®—éŸ³é¢‘ç›¸ä¼¼åº¦
        ddcm_spec = torch.stft(ddcm_audio, n_fft=1024, return_complex=True).abs()
        standard_spec = torch.stft(standard_audio, n_fft=1024, return_complex=True).abs()
        
        spectral_similarity = F.cosine_similarity(
            ddcm_spec.flatten(), 
            standard_spec.flatten(), 
            dim=0
        ).item()
        
        results = {
            "ddcm_compression": ddcm_data,
            "random_error": random_error,
            "improvement_percent": (random_error - ddcm_data['reconstruction_error']) / random_error * 100,
            "output_files": {
                "ddcm": str(ddcm_path),
                "standard": str(standard_path),
                "original": str(original_path)
            },
            "spectral_similarity": spectral_similarity,
            "codebook_stats": self.codebook.get_usage_stats()
        }
        
        print(f"\nâœ… å¯¹æ¯”å®Œæˆ")
        print(f"   ğŸ“ DDCMè¾“å‡º: {ddcm_path}")
        print(f"   ğŸ“ æ ‡å‡†è¾“å‡º: {standard_path}")
        print(f"   ğŸ“ åŸå§‹æ–‡ä»¶: {original_path}")
        print(f"   ğŸ“Š é¢‘è°±ç›¸ä¼¼åº¦: {spectral_similarity:.4f}")
        print(f"   ğŸ“Š ç æœ¬ä½¿ç”¨ç‡: {results['codebook_stats']['usage_rate']*100:.1f}%")
        
        return results

def demo_simple_ddcm():
    """ç®€åŒ–DDCMæ¼”ç¤º"""
    print("ğŸ¯ AudioLDM2 ç®€åŒ–DDCMæ¼”ç¤º")
    print("=" * 50)
    print("ğŸ“ æ¼”ç¤ºå†…å®¹:")
    print("   1. DDCMéŸ³é¢‘å‹ç¼©ï¼ˆlatent -> å™ªå£°ç´¢å¼•ï¼‰")
    print("   2. å¯¹æ¯”DDCM vs éšæœºå™ªå£°é€‰æ‹©")
    print("   3. å‹ç¼©æ•ˆæœåˆ†æ")
    print("=" * 50)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        print("è¯·ç¡®ä¿æœ‰æµ‹è¯•éŸ³é¢‘æ–‡ä»¶")
        return
    
    # åˆå§‹åŒ–ç®€åŒ–DDCM
    ddcm = AudioLDM2_DDCM_Simple(codebook_size=128)  # å°ç æœ¬ç”¨äºæ¼”ç¤º
    
    # è¿è¡Œå¯¹æ¯”
    results = ddcm.compare_compression_methods(input_file)
    
    print(f"\nğŸ‰ DDCMæ¼”ç¤ºå®Œæˆï¼")
    print(f"\nğŸ“Š å…³é”®ç»“æœ:")
    print(f"   ğŸ—œï¸ å‹ç¼©æ¯”: {results['ddcm_compression']['compression_ratio']:.0f}:1")
    print(f"   ğŸ“ˆ ç›¸æ¯”éšæœºå™ªå£°æ”¹è¿›: {results['improvement_percent']:.1f}%")
    print(f"   ğŸµ ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜åˆ° ddcm_comparison_simple/ ç›®å½•")
    
    print(f"\nğŸ’¡ DDCMæ ¸å¿ƒæ€æƒ³:")
    print(f"   â€¢ ä½¿ç”¨é¢„å®šä¹‰å™ªå£°ç æœ¬æ›¿ä»£éšæœºå™ªå£°")
    print(f"   â€¢ é€‰æ‹©æœ€é€‚åˆçš„å™ªå£°å‘é‡è¿›è¡Œå‹ç¼©")
    print(f"   â€¢ å®ç°é«˜å‹ç¼©æ¯”åŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡")
    
    return results

if __name__ == "__main__":
    demo_simple_ddcm()
