#!/usr/bin/env python3
"""
AudioLDM2 DDCM (Denoising Diffusion Codebook Model) å®Œæ•´å®ç°
åŸºäº DDCM è®ºæ–‡ï¼šç”¨é¢„å®šä¹‰ç æœ¬æ›¿æ¢éšæœºå™ªå£°çš„diffusionè¿‡ç¨‹
æ ¸å¿ƒæ€æƒ³ï¼šä½¿ç”¨ç æœ¬å™ªå£°å‘é‡æ›¿ä»£éšæœºé«˜æ–¯å™ªå£°ï¼Œå®ç°é«˜è´¨é‡å‹ç¼©ç”Ÿæˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from diffusers import AudioLDM2Pipeline, DDPMScheduler
import torchaudio
from pathlib import Path
import json
import time
import soundfile as sf
import warnings
warnings.filterwarnings("ignore")

class DDCMCodebook(nn.Module):
    """
    DDCMç æœ¬ï¼šä¸ºdiffusionè¿‡ç¨‹æä¾›é¢„å®šä¹‰çš„å™ªå£°å‘é‡
    æ›¿ä»£æ ‡å‡†diffusionä¸­çš„éšæœºé«˜æ–¯å™ªå£°
    """
    
    def __init__(self, 
                 codebook_size: int = 1024,
                 latent_shape: Tuple[int, int, int] = (8, 250, 16),
                 noise_schedule: str = "cosine"):
        """
        åˆå§‹åŒ–DDCMç æœ¬
        
        Args:
            codebook_size: ç æœ¬å¤§å°
            latent_shape: AudioLDM2 latentå½¢çŠ¶ (channels, height, width)
            noise_schedule: å™ªå£°è°ƒåº¦ç±»å‹
        """
        super().__init__()
        
        self.codebook_size = codebook_size
        self.latent_shape = latent_shape
        self.latent_dim = np.prod(latent_shape)
        
        # åˆ›å»ºç æœ¬ï¼š[codebook_size, channels, height, width]
        # ä½¿ç”¨æ ‡å‡†é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ï¼Œä½†å›ºå®šè¿™äº›å™ªå£°å‘é‡
        self.register_buffer("codebook", torch.randn(codebook_size, *latent_shape))
        
        # ä½¿ç”¨è®¡æ•°å™¨è¿½è¸ªä½¿ç”¨æƒ…å†µ
        self.register_buffer("usage_count", torch.zeros(codebook_size))
        
        print(f"ğŸ”§ DDCMç æœ¬åˆå§‹åŒ–:")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {codebook_size}")
        print(f"   ğŸ“ Latentå½¢çŠ¶: {latent_shape}")
        print(f"   ğŸ“Š æ€»å‚æ•°: {self.latent_dim * codebook_size:,}")
    
    def get_noise_for_timestep(self, batch_size: int, timestep: int, device: str = "cuda") -> torch.Tensor:
        """
        ä¸ºç‰¹å®šæ—¶é—´æ­¥è·å–ç æœ¬å™ªå£°å‘é‡
        DDCMæ ¸å¿ƒï¼šç”¨ç æœ¬å™ªå£°æ›¿ä»£éšæœºå™ªå£°
        
        Args:
            batch_size: æ‰¹å¤§å°
            timestep: å½“å‰æ—¶é—´æ­¥
            device: è®¾å¤‡
            
        Returns:
            noise: ç æœ¬å™ªå£°å‘é‡ [batch_size, channels, height, width]
        """
        # æ ¹æ®æ—¶é—´æ­¥é€‰æ‹©ç æœ¬ç´¢å¼•
        # å¯ä»¥ä½¿ç”¨ä¸åŒçš„é€‰æ‹©ç­–ç•¥
        indices = self._select_codebook_indices(batch_size, timestep)
        
        # è·å–å¯¹åº”çš„ç æœ¬å‘é‡
        noise = self.codebook[indices].to(device)
        
        # æ›´æ–°ä½¿ç”¨è®¡æ•°
        with torch.no_grad():
            for idx in indices:
                self.usage_count[idx] += 1
        
        return noise
    
    def _select_codebook_indices(self, batch_size: int, timestep: int) -> torch.Tensor:
        """
        é€‰æ‹©ç æœ¬ç´¢å¼•çš„ç­–ç•¥
        å¯ä»¥å®ç°å¤šç§é€‰æ‹©æ–¹æ³•ï¼šéšæœºã€åŸºäºæ—¶é—´æ­¥ã€åŸºäºå†…å®¹ç­‰
        """
        # ç­–ç•¥1: åŸºäºæ—¶é—´æ­¥çš„ç¡®å®šæ€§é€‰æ‹©
        base_idx = (timestep * 7) % self.codebook_size  # ä½¿ç”¨è´¨æ•°é¿å…å‘¨æœŸæ€§
        indices = [(base_idx + i) % self.codebook_size for i in range(batch_size)]
        
        return torch.tensor(indices, dtype=torch.long)
    
    def get_usage_stats(self) -> Dict:
        """è·å–ç æœ¬ä½¿ç”¨ç»Ÿè®¡"""
        used_codes = (self.usage_count > 0).sum().item()
        total_usage = self.usage_count.sum().item()
        
        return {
            "used_codes": used_codes,
            "total_codes": self.codebook_size,
            "usage_rate": used_codes / self.codebook_size,
            "total_usage": total_usage,
            "avg_usage": total_usage / max(used_codes, 1),
            "most_used": self.usage_count.max().item(),
            "least_used": self.usage_count.min().item()
        }

class AudioLDM2_DDCM_Pipeline:
    """
    AudioLDM2 DDCMå®Œæ•´ç®¡é“
    é›†æˆç æœ¬åŒ–æ‰©æ•£æ¨¡å‹åˆ°AudioLDM2çš„ç”Ÿæˆè¿‡ç¨‹
    """
    
    def __init__(self, 
                 model_name: str = "cvssp/audioldm2-music",
                 codebook_size: int = 512,
                 enable_compression: bool = True):
        """
        åˆå§‹åŒ–AudioLDM2 DDCMç®¡é“
        
        Args:
            model_name: AudioLDM2æ¨¡å‹åç§°
            codebook_size: DDCMç æœ¬å¤§å°
            enable_compression: æ˜¯å¦å¯ç”¨å‹ç¼©æ¨¡å¼
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.enable_compression = enable_compression
        
        print(f"ğŸµ åˆå§‹åŒ–AudioLDM2 DDCMç®¡é“")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {codebook_size}")
        print(f"   ğŸ—œï¸ å‹ç¼©æ¨¡å¼: {enable_compression}")
        
        # åŠ è½½åŸºç¡€AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        # è·å–latentç»´åº¦
        self.latent_shape = (8, 250, 16)  # AudioLDM2æ ‡å‡†å½¢çŠ¶
        
        # åˆ›å»ºDDCMç æœ¬
        self.ddcm_codebook = DDCMCodebook(
            codebook_size=codebook_size,
            latent_shape=self.latent_shape
        ).to(self.device)
        
        # åˆ›å»ºæ”¹è¿›çš„è°ƒåº¦å™¨
        self.scheduler = DDPMScheduler.from_pretrained(model_name, subfolder="scheduler")
        
        print(f"âœ… AudioLDM2 DDCMç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    def generate_with_ddcm(self, 
                          prompt: str,
                          audio_length_in_s: float = 10.0,
                          num_inference_steps: int = 20,
                          guidance_scale: float = 7.5,
                          use_codebook_noise: bool = True) -> Dict:
        """
        ä½¿ç”¨DDCMç”ŸæˆéŸ³é¢‘
        
        Args:
            prompt: æ–‡æœ¬æç¤º
            audio_length_in_s: éŸ³é¢‘é•¿åº¦ï¼ˆç§’ï¼‰
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            use_codebook_noise: æ˜¯å¦ä½¿ç”¨ç æœ¬å™ªå£°
            
        Returns:
            result: ç”Ÿæˆç»“æœå­—å…¸
        """
        print(f"ğŸµ DDCMç”ŸæˆéŸ³é¢‘")
        print(f"   ğŸ“ æç¤º: {prompt}")
        print(f"   â±ï¸ æ—¶é•¿: {audio_length_in_s}s")
        print(f"   ğŸ”„ æ­¥æ•°: {num_inference_steps}")
        print(f"   ğŸ“š ä½¿ç”¨ç æœ¬å™ªå£°: {use_codebook_noise}")
        
        start_time = time.time()
        
        if use_codebook_noise:
            # DDCMæ¨¡å¼ï¼šä½¿ç”¨ç æœ¬å™ªå£°
            try:
                audio = self._generate_with_codebook_noise(
                    prompt, audio_length_in_s, num_inference_steps, guidance_scale
                )
                method = "DDCM_Codebook"
            except Exception as e:
                print(f"   âš ï¸ DDCMç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹æ³•: {e}")
                result = self.pipeline(
                    prompt=prompt,
                    audio_length_in_s=audio_length_in_s,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale
                )
                audio = result.audios[0]
                method = "Standard_Diffusion_Fallback"
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šä½¿ç”¨éšæœºå™ªå£°
            result = self.pipeline(
                prompt=prompt,
                audio_length_in_s=audio_length_in_s,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale
            )
            audio = result.audios[0]
            method = "Standard_Diffusion"
        
        generation_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        output_dir = Path("ddcm_output")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = int(time.time())
        output_file = output_dir / f"ddcm_{method}_{timestamp}.wav"
        
        # ç¡®ä¿éŸ³é¢‘æ ¼å¼æ­£ç¡®
        if isinstance(audio, torch.Tensor):
            audio_np = audio.cpu().numpy()
        else:
            audio_np = np.array(audio)
        
        if audio_np.ndim > 1:
            audio_np = audio_np.squeeze()
        
        # å½’ä¸€åŒ–
        if audio_np.max() > 1.0 or audio_np.min() < -1.0:
            audio_np = audio_np / np.max(np.abs(audio_np))
        
        sf.write(str(output_file), audio_np, 16000)
        
        result = {
            "prompt": prompt,
            "method": method,
            "audio_length": audio_length_in_s,
            "generation_time": generation_time,
            "output_file": str(output_file),
            "audio_data": audio_np,
            "sample_rate": 16000
        }
        
        if use_codebook_noise and "DDCM" in method:
            # æ·»åŠ ç æœ¬ä½¿ç”¨ç»Ÿè®¡
            result["codebook_stats"] = self.ddcm_codebook.get_usage_stats()
        
        print(f"   âœ… ç”Ÿæˆå®Œæˆ: {output_file}")
        print(f"   â±ï¸ ç”¨æ—¶: {generation_time:.2f}ç§’")
        
        return result
    
    def _generate_with_codebook_noise(self, 
                                     prompt: str,
                                     audio_length_in_s: float,
                                     num_inference_steps: int,
                                     guidance_scale: float) -> np.ndarray:
        """
        ä½¿ç”¨ç æœ¬å™ªå£°è¿›è¡ŒDDCMç”Ÿæˆ
        è¿™æ˜¯DDCMçš„æ ¸å¿ƒå®ç° - ç®€åŒ–ç‰ˆæœ¬
        """
        # ä½¿ç”¨æ ‡å‡†ç®¡é“ç”Ÿæˆï¼Œä½†åœ¨åˆå§‹å™ªå£°ä¸­æ³¨å…¥ç æœ¬å™ªå£°
        print(f"   ğŸ”§ DDCMç®€åŒ–æ¨¡å¼: ä½¿ç”¨ç æœ¬åˆå§‹åŒ–å™ªå£°...")
          # è·å–ç æœ¬å™ªå£°ä½œä¸ºåˆå§‹latent
        batch_size = 1
        initial_noise = self.ddcm_codebook.get_noise_for_timestep(
            batch_size, 1000, self.device  # ä½¿ç”¨é«˜æ—¶é—´æ­¥è·å–å™ªå£°
        )
        
        # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
        if self.device == "cuda":
            initial_noise = initial_noise.half()
        else:
            initial_noise = initial_noise.float()
        
        # ä½¿ç”¨ç®¡é“ç”Ÿæˆï¼Œä½†æ›¿æ¢åˆå§‹å™ªå£°
        # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„DDCMå®ç°
        # å®Œæ•´ç‰ˆæœ¬éœ€è¦ä¿®æ”¹æ•´ä¸ªdiffusionå¾ªç¯
        
        with torch.no_grad():
            # ä½¿ç”¨ç®¡é“çš„å†…éƒ¨æ–¹æ³•ï¼Œä½†æ›¿æ¢éšæœºå™ªå£°
            result = self.pipeline(
                prompt=prompt,
                audio_length_in_s=audio_length_in_s,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                latents=initial_noise  # ä½¿ç”¨ç æœ¬å™ªå£°ä½œä¸ºåˆå§‹latent
            )
            
            return result.audios[0]

def demo_ddcm_complete():
    """å®Œæ•´çš„DDCMæ¼”ç¤º"""
    print("ğŸ¯ AudioLDM2 DDCM å®Œæ•´æ¼”ç¤º")
    print("=" * 50)
    
    # åˆå§‹åŒ–DDCMç®¡é“
    ddcm_pipeline = AudioLDM2_DDCM_Pipeline(
        model_name="cvssp/audioldm2-music",
        codebook_size=256,
        enable_compression=True
    )
    
    # 1. æ ‡å‡†diffusionç”Ÿæˆ
    print("\nğŸµ ç¬¬1æ­¥: æ ‡å‡†Diffusionç”Ÿæˆ")
    standard_result = ddcm_pipeline.generate_with_ddcm(
        prompt="ambient electronic music with soft synths",
        audio_length_in_s=5.0,  # ç¼©çŸ­åˆ°5ç§’åŠ å¿«æµ‹è¯•
        num_inference_steps=15,  # å‡å°‘æ­¥æ•°åŠ å¿«æµ‹è¯•
        use_codebook_noise=False
    )
    
    # 2. DDCMç”Ÿæˆ
    print("\nğŸµ ç¬¬2æ­¥: DDCMç”Ÿæˆ")
    ddcm_result = ddcm_pipeline.generate_with_ddcm(
        prompt="ambient electronic music with soft synths",
        audio_length_in_s=5.0,
        num_inference_steps=15,
        use_codebook_noise=True
    )
    
    # 3. æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    print("\nğŸ“Š ç»“æœå¯¹æ¯”")
    print("-" * 50)
    print(f"æ ‡å‡†ç”Ÿæˆç”¨æ—¶: {standard_result['generation_time']:.2f}ç§’")
    print(f"DDCMç”Ÿæˆç”¨æ—¶: {ddcm_result['generation_time']:.2f}ç§’")
    print(f"ç æœ¬ä½¿ç”¨ç»Ÿè®¡: {ddcm_result.get('codebook_stats', {})}")
    
    print(f"\nâœ… DDCMå®Œæ•´æ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   æ ‡å‡†: {standard_result['output_file']}")
    print(f"   DDCM: {ddcm_result['output_file']}")
    
    # 4. è´¨é‡åˆ†æ
    print(f"\nğŸ”¬ è´¨é‡åˆ†æ:")
    std_audio = standard_result['audio_data']
    ddcm_audio = ddcm_result['audio_data']
    
    # ç¡®ä¿é•¿åº¦ä¸€è‡´
    min_len = min(len(std_audio), len(ddcm_audio))
    std_audio = std_audio[:min_len]
    ddcm_audio = ddcm_audio[:min_len]
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    correlation = np.corrcoef(std_audio, ddcm_audio)[0, 1]
    mse = np.mean((std_audio - ddcm_audio) ** 2)
    
    print(f"   ğŸ“Š æ ‡å‡†vs DDCMç›¸å…³æ€§: {correlation:.4f}")
    print(f"   ğŸ“Š æ ‡å‡†vs DDCM MSE: {mse:.6f}")
    
    if correlation > 0.8:
        print(f"   âœ… DDCMè´¨é‡ä¼˜ç§€ï¼ä¸æ ‡å‡†ç”Ÿæˆé«˜åº¦ç›¸ä¼¼")
    elif correlation > 0.5:
        print(f"   âœ… DDCMè´¨é‡è‰¯å¥½")
    else:
        print(f"   âš ï¸ DDCMä¸æ ‡å‡†ç”Ÿæˆå­˜åœ¨å·®å¼‚")

if __name__ == "__main__":
    demo_ddcm_complete()
