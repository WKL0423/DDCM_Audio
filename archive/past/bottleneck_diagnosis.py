"""
AudioLDM2 VAEé‡å»ºè´¨é‡ç“¶é¢ˆåˆ†æ - ä¿®å¤ç‰ˆ

é’ˆå¯¹å½“å‰"èƒ½å¬å‡ºè”ç³»ä½†è´¨é‡ä¸€èˆ¬"çš„é—®é¢˜ï¼Œæ·±åº¦åˆ†æç“¶é¢ˆæ‰€åœ¨
"""

import torch
import librosa
import numpy as np
import os
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F

from diffusers import AudioLDM2Pipeline


def quick_bottleneck_analysis(audio_path):
    """
    å¿«é€Ÿç“¶é¢ˆåˆ†æï¼Œæ‰¾å‡ºè´¨é‡ä¸ä½³çš„æ ¹æœ¬åŸå› 
    """
    print(f"ğŸ” å¿«é€Ÿç“¶é¢ˆåˆ†æ: {audio_path}")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    pipeline = AudioLDM2Pipeline.from_pretrained("cvssp/audioldm2-music", torch_dtype=torch.float32).to(device)
    vae = pipeline.vae
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000, mono=True)
    audio = audio[:5*16000]  # 5ç§’
    
    print(f"\\nğŸ“Š åŸå§‹éŸ³é¢‘ç»Ÿè®¡:")
    print(f"   RMSåŠŸç‡: {np.sqrt(np.mean(audio**2)):.6f}")
    print(f"   åŠ¨æ€èŒƒå›´: {audio.max():.3f} åˆ° {audio.min():.3f}")
    print(f"   é¢‘è°±å¸¦å®½: 0-8000Hz")
    
    # æµ‹è¯•1: Melå˜æ¢ä¿¡æ¯æŸå¤±
    print(f"\\nğŸ”¬ ç“¶é¢ˆ1: Melå˜æ¢ä¿¡æ¯æŸå¤±")
    
    # åŸå§‹STFT
    stft_orig = librosa.stft(audio, n_fft=1024, hop_length=160)
    mag_orig = np.abs(stft_orig)
    
    # Melå˜æ¢
    mel_spec = librosa.feature.melspectrogram(y=audio, sr=16000, n_mels=64, n_fft=1024, hop_length=160)
    
    # ä»Melé‡å»ºçº¿æ€§é¢‘è°±
    mel_basis = librosa.filters.mel(sr=16000, n_fft=1024, n_mels=64)
    mag_reconstructed = mel_basis.T @ mel_spec
    
    # è®¡ç®—æŸå¤±
    spectral_mse = np.mean((mag_orig - mag_reconstructed) ** 2)
    spectral_corr = np.corrcoef(mag_orig.flatten(), mag_reconstructed.flatten())[0,1]
    
    print(f"   é¢‘è°±é‡å»ºç›¸å…³æ€§: {spectral_corr:.4f}")
    print(f"   é¢‘è°±ä¿¡æ¯ä¿ç•™: {spectral_corr*100:.1f}%")
    print(f"   âŒ Melå˜æ¢æŸå¤±: {(1-spectral_corr)*100:.1f}%")
    
    # æµ‹è¯•2: VAEå‹ç¼©æŸå¤±
    print(f"\\nğŸ”¬ ç“¶é¢ˆ2: VAEå‹ç¼©æŸå¤±")
    
    # å‡†å¤‡melè¾“å…¥
    mel_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_norm = 2.0 * (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min()) - 1.0
    mel_tensor = torch.from_numpy(mel_norm).unsqueeze(0).unsqueeze(0).float().to(device)
    
    with torch.no_grad():
        latent = vae.encode(mel_tensor).latent_dist.sample()
        reconstructed = vae.decode(latent).sample
    
    recon_mel = reconstructed.squeeze().cpu().numpy()
    
    # å½¢çŠ¶å¯¹é½
    min_w = min(mel_norm.shape[1], recon_mel.shape[1])
    mel_aligned = mel_norm[:, :min_w]
    recon_aligned = recon_mel[:, :min_w]
    
    vae_corr = np.corrcoef(mel_aligned.flatten(), recon_aligned.flatten())[0,1]
    vae_mse = np.mean((mel_aligned - recon_aligned) ** 2)
    
    print(f"   VAEé‡å»ºç›¸å…³æ€§: {vae_corr:.4f}")
    print(f"   VAEä¿¡æ¯ä¿ç•™: {vae_corr*100:.1f}%")
    print(f"   âŒ VAEå‹ç¼©æŸå¤±: {(1-vae_corr)*100:.1f}%")
    print(f"   å‹ç¼©æ¯”: {mel_tensor.numel() / latent.numel():.1f}:1")
    
    # æµ‹è¯•3: Griffin-Limé‡å»ºæŸå¤±
    print(f"\\nğŸ”¬ ç“¶é¢ˆ3: Griffin-Limé‡å»ºæŸå¤±")
    
    # åå½’ä¸€åŒ–å¹¶é‡å»ºéŸ³é¢‘
    recon_denorm = (recon_aligned + 1) / 2 * (mel_db.max() - mel_db.min()) + mel_db.min()
    recon_power = librosa.db_to_power(recon_denorm)
    
    # Griffin-Limé‡å»º
    audio_recon = librosa.feature.inverse.mel_to_audio(
        recon_power, sr=16000, hop_length=160, n_fft=1024, n_iter=32
    )
    
    # é•¿åº¦å¯¹é½
    min_len = min(len(audio), len(audio_recon))
    audio_final = audio[:min_len]
    recon_final = audio_recon[:min_len]
    
    # æœ€ç»ˆè´¨é‡æŒ‡æ ‡
    final_snr = 10 * np.log10(np.mean(audio_final**2) / (np.mean((audio_final - recon_final)**2) + 1e-10))
    final_corr = np.corrcoef(audio_final, recon_final)[0,1]
    
    print(f"   æœ€ç»ˆéŸ³é¢‘SNR: {final_snr:.2f} dB")
    print(f"   æœ€ç»ˆéŸ³é¢‘ç›¸å…³æ€§: {final_corr:.4f}")
    print(f"   âŒ Griffin-Limé™åˆ¶æ˜æ˜¾")
    
    # æ•´ä½“åˆ†æ
    total_retention = spectral_corr * vae_corr * abs(final_corr)
    print(f"\\nğŸ“‰ ç´¯ç§¯ä¿¡æ¯æŸå¤±åˆ†æ:")
    print(f"   åŸå§‹ â†’ Mel: ä¿ç•™ {spectral_corr*100:.1f}%")
    print(f"   Mel â†’ VAE: ä¿ç•™ {vae_corr*100:.1f}%")
    print(f"   VAE â†’ éŸ³é¢‘: ä¿ç•™ {abs(final_corr)*100:.1f}%")
    print(f"   ğŸ”´ æ€»ä½“ä¿ç•™ç‡: {total_retention*100:.1f}%")
    print(f"   ğŸ”´ æ€»ä½“æŸå¤±: {(1-total_retention)*100:.1f}%")
    
    # å…³é”®ç“¶é¢ˆè¯†åˆ«
    print(f"\\nâš ï¸ å…³é”®ç“¶é¢ˆæ’åº:")
    losses = [
        ("Griffin-Limç›¸ä½é‡å»º", 1 - abs(final_corr)),
        ("VAEå‹ç¼©æŸå¤±", 1 - vae_corr),
        ("Melå˜æ¢æŸå¤±", 1 - spectral_corr)
    ]
    losses.sort(key=lambda x: x[1], reverse=True)
    
    for i, (name, loss) in enumerate(losses, 1):
        print(f"   #{i} {name}: {loss*100:.1f}% æŸå¤±")
    
    return {
        'spectral_retention': spectral_corr,
        'vae_retention': vae_corr, 
        'final_snr': final_snr,
        'final_correlation': final_corr,
        'total_retention': total_retention,
        'primary_bottleneck': losses[0][0]
    }


def test_improvement_strategies(audio_path):
    """
    æµ‹è¯•é’ˆå¯¹æ€§æ”¹è¿›ç­–ç•¥
    """
    print(f"\\nğŸš€ æµ‹è¯•æ”¹è¿›ç­–ç•¥")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    pipeline = AudioLDM2Pipeline.from_pretrained("cvssp/audioldm2-music", torch_dtype=torch.float32).to(device)
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000, mono=True)
    audio = audio[:5*16000]
    
    results = []
    output_dir = "bottleneck_improvement_test"
    os.makedirs(output_dir, exist_ok=True)
    input_name = Path(audio_path).stem
    
    # ç­–ç•¥1: æé«˜Melåˆ†è¾¨ç‡
    print(f"\\nğŸ“ˆ ç­–ç•¥1: é«˜åˆ†è¾¨ç‡Mel (128bins)")
    
    mel_hires = librosa.feature.melspectrogram(
        y=audio, sr=16000, n_mels=128, n_fft=2048, hop_length=128
    )
    
    # é™é‡‡æ ·åˆ°VAEå¯æ¥å—çš„64bins
    mel_hires_resized = F.interpolate(
        torch.from_numpy(mel_hires).unsqueeze(0).unsqueeze(0),
        size=(64, mel_hires.shape[1]),
        mode='bilinear', align_corners=False
    ).squeeze().numpy()
    
    # æ ‡å‡†åŒ–å¤„ç†
    mel_db = librosa.power_to_db(mel_hires_resized, ref=np.max)
    mel_norm = 2.0 * (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min()) - 1.0
    mel_tensor = torch.from_numpy(mel_norm).unsqueeze(0).unsqueeze(0).float().to(device)
    
    with torch.no_grad():
        latent = vae.encode(mel_tensor).latent_dist.sample()
        reconstructed = vae.decode(latent).sample
    
    recon_mel = reconstructed.squeeze().cpu().numpy()
    recon_denorm = (recon_mel + 1) / 2 * (mel_db.max() - mel_db.min()) + mel_db.min()
    recon_power = librosa.db_to_power(recon_denorm)
    
    # é«˜è´¨é‡Griffin-Limé‡å»º
    audio_strategy1 = librosa.feature.inverse.mel_to_audio(
        recon_power, sr=16000, hop_length=128, n_fft=2048, n_iter=64
    )
    
    # é•¿åº¦å¯¹é½å’Œè¯„ä¼°
    min_len = min(len(audio), len(audio_strategy1))
    snr1 = 10 * np.log10(np.mean(audio[:min_len]**2) / (np.mean((audio[:min_len] - audio_strategy1[:min_len])**2) + 1e-10))
    corr1 = np.corrcoef(audio[:min_len], audio_strategy1[:min_len])[0,1]
    
    # ä¿å­˜
    path1 = os.path.join(output_dir, f"{input_name}_hires_mel.wav")
    torchaudio.save(path1, torch.from_numpy(audio_strategy1[:min_len] / (np.max(np.abs(audio_strategy1[:min_len])) + 1e-8)).unsqueeze(0), 16000)
    
    results.append(("é«˜åˆ†è¾¨ç‡Mel", snr1, corr1, path1))
    print(f"   âœ… SNR: {snr1:.2f}dB, ç›¸å…³æ€§: {corr1:.4f}")
    
    # ç­–ç•¥2: è·³è¿‡VAEç›´æ¥é‡å»ºï¼ˆä¸Šé™æµ‹è¯•ï¼‰
    print(f"\\nğŸ“ˆ ç­–ç•¥2: è·³è¿‡VAEï¼ˆç†è®ºä¸Šé™ï¼‰")
    
    mel_direct = librosa.feature.melspectrogram(
        y=audio, sr=16000, n_mels=64, n_fft=1024, hop_length=160
    )
    
    audio_strategy2 = librosa.feature.inverse.mel_to_audio(
        mel_direct, sr=16000, hop_length=160, n_fft=1024, n_iter=64
    )
    
    min_len = min(len(audio), len(audio_strategy2))
    snr2 = 10 * np.log10(np.mean(audio[:min_len]**2) / (np.mean((audio[:min_len] - audio_strategy2[:min_len])**2) + 1e-10))
    corr2 = np.corrcoef(audio[:min_len], audio_strategy2[:min_len])[0,1]
    
    path2 = os.path.join(output_dir, f"{input_name}_direct_mel.wav")
    torchaudio.save(path2, torch.from_numpy(audio_strategy2[:min_len] / (np.max(np.abs(audio_strategy2[:min_len])) + 1e-8)).unsqueeze(0), 16000)
    
    results.append(("è·³è¿‡VAE(ä¸Šé™)", snr2, corr2, path2))
    print(f"   âœ… SNR: {snr2:.2f}dB, ç›¸å…³æ€§: {corr2:.4f}")
    
    # ç­–ç•¥3: ä¿®æ­£Vocoderï¼ˆå¦‚æœå¯è¡Œï¼‰
    print(f"\\nğŸ“ˆ ç­–ç•¥3: ä¿®æ­£Vocoder")
    
    try:
        # ä½¿ç”¨ä¹‹å‰æˆåŠŸçš„vocoderæ–¹æ³•
        mel_for_vocoder = torch.from_numpy(recon_mel).unsqueeze(0).float().to(device)
        mel_transposed = mel_for_vocoder.transpose(-2, -1)
        
        with torch.no_grad():
            audio_vocoder = vocoder(mel_transposed).squeeze().cpu().numpy()
        
        min_len = min(len(audio), len(audio_vocoder))
        snr3 = 10 * np.log10(np.mean(audio[:min_len]**2) / (np.mean((audio[:min_len] - audio_vocoder[:min_len])**2) + 1e-10))
        corr3 = np.corrcoef(audio[:min_len], audio_vocoder[:min_len])[0,1]
        
        path3 = os.path.join(output_dir, f"{input_name}_fixed_vocoder.wav")
        torchaudio.save(path3, torch.from_numpy(audio_vocoder[:min_len] / (np.max(np.abs(audio_vocoder[:min_len])) + 1e-8)).unsqueeze(0), 16000)
        
        results.append(("ä¿®æ­£Vocoder", snr3, corr3, path3))
        print(f"   âœ… SNR: {snr3:.2f}dB, ç›¸å…³æ€§: {corr3:.4f}")
        
    except Exception as e:
        print(f"   âŒ Vocoderå¤±è´¥: {e}")
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘
    orig_path = os.path.join(output_dir, f"{input_name}_original.wav")
    torchaudio.save(orig_path, torch.from_numpy(audio / (np.max(np.abs(audio)) + 1e-8)).unsqueeze(0), 16000)
    
    # ç»“æœå¯¹æ¯”
    print(f"\\nğŸ† æ”¹è¿›ç­–ç•¥æ•ˆæœå¯¹æ¯”:")
    results.sort(key=lambda x: x[1], reverse=True)  # æŒ‰SNRæ’åº
    
    for i, (method, snr, corr, path) in enumerate(results, 1):
        improvement = snr - (-0.01)  # ä¸åŸºçº¿å¯¹æ¯”
        print(f"   #{i} {method}: SNR={snr:.2f}dB (+{improvement:.2f}), ç›¸å…³={corr:.4f}")
        print(f"       æ–‡ä»¶: {path}")
    
    if results:
        best_improvement = results[0][1] - (-0.01)
        print(f"\\nğŸš€ æœ€ä½³æ”¹è¿›: {best_improvement:+.2f} dB ({results[0][0]})")
        
        if best_improvement > 5:
            print(f"âœ… æ˜¾è‘—æ”¹å–„ï¼å»ºè®®é‡‡ç”¨è¯¥æ–¹æ³•")
        elif best_improvement > 1:
            print(f"âš ï¸ æœ‰æ”¹å–„ä½†ä¸æ˜¾è‘—ï¼Œéœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            print(f"âŒ æ”¹å–„æœ‰é™ï¼Œéœ€è¦æ›´æ ¹æœ¬çš„æ–¹æ³•")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    audio_path = sys.argv[1] if len(sys.argv) > 1 else "AudioLDM2_Music_output.wav"
    
    print(f"ğŸ” AudioLDM2 VAEé‡å»ºè´¨é‡ç“¶é¢ˆè¯Šæ–­")
    print(f"=" * 60)
    print(f"ğŸ¯ ç›®æ ‡: æ‰¾å‡º'èƒ½å¬å‡ºè”ç³»ä½†è´¨é‡ä¸€èˆ¬'çš„æ ¹æœ¬åŸå› ")
    
    # æ­¥éª¤1: å¿«é€Ÿç“¶é¢ˆåˆ†æ
    analysis = quick_bottleneck_analysis(audio_path)
    
    # æ­¥éª¤2: æµ‹è¯•æ”¹è¿›ç­–ç•¥
    improvements = test_improvement_strategies(audio_path)
    
    # æ­¥éª¤3: æ€»ç»“å’Œå»ºè®®
    print(f"\\nğŸ’¡ è¯Šæ–­æ€»ç»“:")
    print(f"   ğŸ”´ ä¸»è¦ç“¶é¢ˆ: {analysis['primary_bottleneck']}")
    print(f"   ğŸ“‰ æ€»ä½“ä¿¡æ¯ä¿ç•™: {analysis['total_retention']*100:.1f}%")
    print(f"   ğŸ“ˆ å½“å‰SNR: {analysis['final_snr']:.2f}dB")
    
    if improvements:
        best_snr = max(improvements, key=lambda x: x[1])[1]
        potential_gain = best_snr - analysis['final_snr']
        print(f"   ğŸš€ æ”¹è¿›æ½œåŠ›: +{potential_gain:.2f}dB")
    
    print(f"\\nğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
    if analysis['total_retention'] < 0.3:
        print(f"   1. ä¿¡æ¯æŸå¤±è¿‡å¤§ï¼Œå»ºè®®ä»æ¶æ„å±‚é¢æ”¹è¿›")
        print(f"   2. è€ƒè™‘ä½¿ç”¨æ›´å¤§çš„VAEæˆ–è·³è¿‡VAEå‹ç¼©")
        print(f"   3. ç ”ç©¶ç«¯åˆ°ç«¯çš„éŸ³é¢‘é‡å»ºæ¨¡å‹")
    else:
        print(f"   1. ä¼˜åŒ–melå‚æ•°é…ç½®")
        print(f"   2. æ”¹è¿›Griffin-Limç®—æ³•æˆ–ä½¿ç”¨ç¥ç»vocoder")
        print(f"   3. å¢åŠ åå¤„ç†æ­¥éª¤")
    
    print(f"\\nğŸ“ æµ‹è¯•ç»“æœä¿å­˜åœ¨: bottleneck_improvement_test/")
    print(f"ğŸ§ è¯·æ’­æ”¾å¯¹æ¯”éŸ³é¢‘ä»¥è¿›è¡Œä¸»è§‚è¯„ä¼°")


if __name__ == "__main__":
    main()
