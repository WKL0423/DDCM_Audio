#!/usr/bin/env python3
"""
AudioLDM2 å¼•å¯¼å¼Diffusioné‡å»º - ç®€åŒ–ç¨³å®šç‰ˆæœ¬
===========================================

åŸºäºä¹‹å‰æˆåŠŸçš„VAEè„šæœ¬ï¼Œå®ç°ä¸€ä¸ªæ›´ç¨³å®šçš„guided diffusion reconstructionã€‚
è¯¥ç‰ˆæœ¬ä¸“æ³¨äºæ ¸å¿ƒåˆ›æ–°æ€æƒ³çš„å®ç°ï¼Œè€Œä¸æ˜¯å¤æ‚çš„éŸ³é¢‘å¤„ç†ç»†èŠ‚ã€‚

æ ¸å¿ƒåˆ›æ–°ï¼š
1. ä½¿ç”¨VAE encoderè·å–ç›®æ ‡éŸ³é¢‘çš„latent representation
2. ä»éšæœºå™ªå£°å¼€å§‹ï¼Œä½¿ç”¨diffusionè¿‡ç¨‹
3. åœ¨æ¯ä¸ªdiffusionæ­¥éª¤ä¸­ï¼Œæ·»åŠ æŒ‡å‘ç›®æ ‡latentçš„å¼•å¯¼åŠ›
4. æœ€ç»ˆä½¿ç”¨VAE decoderé‡å»ºéŸ³é¢‘

Author: AI Assistant  
Date: 2025-01-27
"""

import os
import sys
import torch
import numpy as np
import soundfile as sf
import librosa
from typing import Optional, Tuple
import argparse
from pathlib import Path
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from New_pipeline_audioldm2 import AudioLDM2Pipeline
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥AudioLDM2Pipelineï¼Œè¯·ç¡®ä¿New_pipeline_audioldm2.pyåœ¨å½“å‰ç›®å½•")
    sys.exit(1)


class SimpleGuidedDiffusionReconstructor:
    """ç®€åŒ–ç‰ˆå¼•å¯¼å¼Diffusioné‡å»ºå™¨"""
    
    def __init__(self, device="cuda", dtype=torch.float16):
        self.device = device
        self.dtype = dtype
        
        print(f"ğŸš€ åˆå§‹åŒ–ç®€åŒ–ç‰ˆå¼•å¯¼å¼Diffusioné‡å»ºå™¨")
        print(f"   è®¾å¤‡: {device}, æ•°æ®ç±»å‹: {dtype}")
        
        # åŠ è½½æ¨¡å‹
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        repo_id = "cvssp/audioldm2"
        self.pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=dtype)
        self.pipe = self.pipe.to(device)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # è·å–ç»„ä»¶
        self.vae = self.pipe.vae
        self.unet = self.pipe.unet
        self.scheduler = self.pipe.scheduler
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.vae.eval()
        self.unet.eval()
    
    def save_audio_compatible(self, audio: np.ndarray, filepath: str, sr: int = 16000):
        """ä¿å­˜å…¼å®¹çš„éŸ³é¢‘æ–‡ä»¶"""
        try:
            # ç¡®ä¿éŸ³é¢‘åœ¨[-1, 1]èŒƒå›´å†…
            if np.max(np.abs(audio)) > 0:
                audio = audio / np.max(np.abs(audio)) * 0.8
            
            # è½¬æ¢ä¸º16ä½PCM
            audio_int16 = (audio * 32767).astype(np.int16)
            sf.write(filepath, audio_int16, sr, subtype='PCM_16')
            
            print(f"ğŸ’¾ ä¿å­˜éŸ³é¢‘: {Path(filepath).name}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜éŸ³é¢‘å¤±è´¥: {e}")
            raise
    
    def load_and_encode_audio(self, audio_path: str) -> torch.Tensor:
        """
        åŠ è½½éŸ³é¢‘å¹¶ç¼–ç åˆ°latent space
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            latent tensor
        """
        print(f"\nğŸ“ åŠ è½½éŸ³é¢‘: {Path(audio_path).name}")
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(audio_path, sr=16000, mono=True)
        print(f"   é‡‡æ ·ç‡: {sr}Hz, æ—¶é•¿: {len(audio)/sr:.2f}s")
        
        # å½’ä¸€åŒ–
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio))
        
        # ç¡®ä¿éŸ³é¢‘é•¿åº¦ç¬¦åˆæ¨¡å‹è¦æ±‚
        target_length = 163840  # 16000 * 10.24s, AudioLDM2çš„æ ‡å‡†é•¿åº¦
        if len(audio) < target_length:
            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')
        elif len(audio) > target_length:
            audio = audio[:target_length]
        
        print(f"   å¤„ç†åé•¿åº¦: {len(audio)/sr:.2f}s")
        
        # ç”Ÿæˆmelé¢‘è°±å›¾
        mel_spec = librosa.feature.melspectrogram(
            y=audio, 
            sr=16000, 
            n_mels=64, 
            n_fft=1024, 
            hop_length=160
        )
        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        # å½’ä¸€åŒ–melé¢‘è°±å›¾åˆ°[-1, 1]
        mel_spec = (mel_spec + 80) / 80 * 2 - 1
        
        # è½¬æ¢ä¸ºtensorå¹¶è°ƒæ•´å½¢çŠ¶
        mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).unsqueeze(0).to(
            device=self.device, dtype=self.dtype
        )
        
        # è°ƒæ•´åˆ°æ ‡å‡†å°ºå¯¸ (1, 1, 64, 1024)
        if mel_tensor.shape[-1] != 1024:
            mel_tensor = torch.nn.functional.interpolate(
                mel_tensor, size=(64, 1024), mode='bilinear', align_corners=False
            )
        
        print(f"ğŸµ Melé¢‘è°±å›¾å½¢çŠ¶: {mel_tensor.shape}")
        
        # VAEç¼–ç 
        with torch.no_grad():
            latent_dist = self.vae.encode(mel_tensor)
            if hasattr(latent_dist, 'latent_dist'):
                latent = latent_dist.latent_dist.sample()
            else:
                latent = latent_dist.sample()
            
            latent = latent * self.vae.config.scaling_factor
        
        print(f"ğŸ”¢ Latentè¡¨ç¤ºå½¢çŠ¶: {latent.shape}")
        print(f"   èŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
        
        return latent
    
    def decode_latent_to_audio(self, latent: torch.Tensor) -> np.ndarray:
        """
        å°†latentè§£ç ä¸ºéŸ³é¢‘
        
        Args:
            latent: latent tensor
            
        Returns:
            éŸ³é¢‘æ•°ç»„
        """
        print(f"ğŸ”„ è§£ç latentåˆ°éŸ³é¢‘...")
        
        with torch.no_grad():
            # VAEè§£ç 
            latent_scaled = latent / self.vae.config.scaling_factor
            mel_spec = self.vae.decode(latent_scaled).sample
            
            print(f"   è§£ç çš„melé¢‘è°±å›¾å½¢çŠ¶: {mel_spec.shape}")
              # è½¬æ¢ä¸ºnumpyï¼Œç¡®ä¿ä½¿ç”¨float32
            mel_spec_np = mel_spec.squeeze().cpu().float().numpy()
            
            # åå½’ä¸€åŒ–
            mel_spec_np = (mel_spec_np + 1) / 2 * 80 - 80
            
            # Griffin-Limé‡å»ºéŸ³é¢‘
            audio = librosa.feature.inverse.mel_to_audio(
                librosa.db_to_power(mel_spec_np),
                sr=16000,
                n_fft=1024,
                hop_length=160,
                n_iter=32
            )
            
            print(f"ğŸ”Š é‡å»ºéŸ³é¢‘å½¢çŠ¶: {audio.shape}")
            
            return audio
    
    def compute_guidance_loss(self, current_latent: torch.Tensor, target_latent: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—å¼•å¯¼æŸå¤±"""
        # ä½¿ç”¨MSEæŸå¤±
        loss = torch.nn.functional.mse_loss(current_latent, target_latent)
        return loss
    
    def guided_diffusion_reconstruction(self, 
                                      target_latent: torch.Tensor,
                                      num_steps: int = 50,
                                      guidance_scale: float = 0.1,
                                      guidance_decay: float = 0.95) -> torch.Tensor:
        """
        å¼•å¯¼å¼diffusioné‡å»º
        
        Args:
            target_latent: ç›®æ ‡latent
            num_steps: diffusionæ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            guidance_decay: å¼•å¯¼å¼ºåº¦è¡°å‡
            
        Returns:
            é‡å»ºçš„latent
        """
        print(f"\nğŸŒŸ å¼€å§‹å¼•å¯¼å¼Diffusioné‡å»º")
        print(f"   æ­¥æ•°: {num_steps}")
        print(f"   å¼•å¯¼å¼ºåº¦: {guidance_scale}")
        print(f"   å¼ºåº¦è¡°å‡: {guidance_decay}")
        
        # åˆå§‹åŒ–å™ªå£°
        latent_shape = target_latent.shape
        noise = torch.randn(latent_shape, device=self.device, dtype=self.dtype)
        
        # è®¾ç½®scheduler
        self.scheduler.set_timesteps(num_steps)
        timesteps = self.scheduler.timesteps
        
        # ä»å™ªå£°å¼€å§‹
        latents = noise.clone()
        
        current_guidance = guidance_scale
        
        for i, t in enumerate(timesteps):
            if i % 10 == 0:
                print(f"   æ­¥éª¤ {i+1}/{num_steps}, æ—¶é—´æ­¥: {t.item():.0f}, å¼•å¯¼å¼ºåº¦: {current_guidance:.4f}")
            
            # å‡†å¤‡è¾“å…¥
            latent_model_input = self.scheduler.scale_model_input(latents, t)
            
            # å‡†å¤‡æ¡ä»¶åµŒå…¥ï¼ˆä½¿ç”¨é›¶åµŒå…¥è¿›è¡Œæ— æ¡ä»¶ç”Ÿæˆï¼‰
            batch_size = latent_model_input.shape[0]
            
            # åˆ›å»ºç©ºçš„æ¡ä»¶åµŒå…¥
            encoder_hidden_states = torch.zeros(
                (batch_size, 77, 768), 
                device=self.device, 
                dtype=self.dtype
            )
            encoder_hidden_states_1 = torch.zeros(
                (batch_size, 77, 1024), 
                device=self.device, 
                dtype=self.dtype
            )
            
            # é¢„æµ‹å™ªå£°
            with torch.no_grad():
                noise_pred = self.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=encoder_hidden_states,
                    encoder_hidden_states_1=encoder_hidden_states_1,
                    return_dict=False,
                )[0]
            
            # æ ‡å‡†diffusionæ­¥éª¤
            step_output = self.scheduler.step(noise_pred, t, latents)
            latents = step_output.prev_sample
            
            # åº”ç”¨å¼•å¯¼ï¼ˆå¦‚æœä¸æ˜¯æœ€åå‡ æ­¥ï¼‰
            if i < num_steps - 5 and current_guidance > 1e-4:
                # è®¡ç®—åˆ°ç›®æ ‡çš„æ¢¯åº¦
                latents.requires_grad_(True)
                loss = self.compute_guidance_loss(latents, target_latent)
                grad = torch.autograd.grad(loss, latents)[0]
                
                # åº”ç”¨å¼•å¯¼
                with torch.no_grad():
                    latents = latents - current_guidance * grad
                    latents.requires_grad_(False)
                
                # è¡°å‡å¼•å¯¼å¼ºåº¦
                current_guidance *= guidance_decay
        
        print(f"âœ… å¼•å¯¼å¼é‡å»ºå®Œæˆ")
        print(f"   æœ€ç»ˆlatentèŒƒå›´: [{latents.min():.3f}, {latents.max():.3f}]")
        
        return latents
    
    def reconstruct_audio(self, 
                         input_path: str,
                         output_dir: str = "guided_diffusion_simple_output",
                         num_steps: int = 50,
                         guidance_scale: float = 0.1,
                         compare_vae: bool = True) -> Tuple[str, Optional[str]]:
        """
        å®Œæ•´çš„é‡å»ºæµç¨‹
        
        Args:
            input_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            num_steps: diffusionæ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            compare_vae: æ˜¯å¦ä¸çº¯VAEå¯¹æ¯”
            
        Returns:
            (guided_path, vae_path)
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ ç®€åŒ–ç‰ˆå¼•å¯¼å¼DiffusionéŸ³é¢‘é‡å»º")
        print(f"{'='*60}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½å¹¶ç¼–ç éŸ³é¢‘
        target_latent = self.load_and_encode_audio(input_path)
        
        # æ‰§è¡Œå¼•å¯¼å¼diffusioné‡å»º
        guided_latent = self.guided_diffusion_reconstruction(
            target_latent=target_latent,
            num_steps=num_steps,
            guidance_scale=guidance_scale
        )
        
        # è§£ç ä¸ºéŸ³é¢‘
        guided_audio = self.decode_latent_to_audio(guided_latent)
        
        # ä¿å­˜å¼•å¯¼å¼é‡å»ºç»“æœ
        input_name = Path(input_path).stem
        guided_path = os.path.join(
            output_dir, 
            f"{input_name}_guided_{num_steps}steps.wav"
        )
        self.save_audio_compatible(guided_audio, guided_path)
        
        vae_path = None
        
        # çº¯VAEé‡å»ºå¯¹æ¯”
        if compare_vae:
            print(f"\nğŸ”„ æ‰§è¡Œçº¯VAEé‡å»ºå¯¹æ¯”...")
            vae_audio = self.decode_latent_to_audio(target_latent)
            vae_path = os.path.join(
                output_dir, 
                f"{input_name}_vae_only.wav"
            )
            self.save_audio_compatible(vae_audio, vae_path)
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        self.compute_quality_metrics(input_path, guided_audio, 
                                   self.decode_latent_to_audio(target_latent) if compare_vae else None)
        
        return guided_path, vae_path
    
    def compute_quality_metrics(self, original_path: str, guided_audio: np.ndarray, vae_audio: Optional[np.ndarray] = None):
        """è®¡ç®—é‡å»ºè´¨é‡æŒ‡æ ‡"""
        print(f"\nğŸ“Š é‡å»ºè´¨é‡åˆ†æ")
        print(f"â”€" * 40)
        
        # åŠ è½½åŸå§‹éŸ³é¢‘
        original_audio, _ = librosa.load(original_path, sr=16000, mono=True)
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(original_audio), len(guided_audio))
        original_audio = original_audio[:min_len]
        guided_audio = guided_audio[:min_len]
        
        # å½’ä¸€åŒ–
        if np.max(np.abs(original_audio)) > 0:
            original_audio = original_audio / np.max(np.abs(original_audio))
        if np.max(np.abs(guided_audio)) > 0:
            guided_audio = guided_audio / np.max(np.abs(guided_audio))
        
        # è®¡ç®—MSEå’ŒSNR
        mse_guided = np.mean((original_audio - guided_audio) ** 2)
        snr_guided = 10 * np.log10(np.var(original_audio) / mse_guided) if mse_guided > 0 else float('inf')
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation_guided = np.corrcoef(original_audio, guided_audio)[0, 1]
        
        print(f"å¼•å¯¼å¼Diffusioné‡å»º:")
        print(f"  MSE: {mse_guided:.6f}")
        print(f"  SNR: {snr_guided:.2f} dB")
        print(f"  ç›¸å…³ç³»æ•°: {correlation_guided:.4f}")
        
        if vae_audio is not None:
            vae_audio = vae_audio[:min_len]
            if np.max(np.abs(vae_audio)) > 0:
                vae_audio = vae_audio / np.max(np.abs(vae_audio))
            
            mse_vae = np.mean((original_audio - vae_audio) ** 2)
            snr_vae = 10 * np.log10(np.var(original_audio) / mse_vae) if mse_vae > 0 else float('inf')
            correlation_vae = np.corrcoef(original_audio, vae_audio)[0, 1]
            
            print(f"\nçº¯VAEé‡å»º:")
            print(f"  MSE: {mse_vae:.6f}")
            print(f"  SNR: {snr_vae:.2f} dB")
            print(f"  ç›¸å…³ç³»æ•°: {correlation_vae:.4f}")
            
            print(f"\næ”¹è¿›ç¨‹åº¦:")
            print(f"  MSEæ”¹è¿›: {((mse_vae - mse_guided) / mse_vae * 100):.1f}%")
            print(f"  SNRæ”¹è¿›: {(snr_guided - snr_vae):.2f} dB")
            print(f"  ç›¸å…³ç³»æ•°æ”¹è¿›: {(correlation_guided - correlation_vae):.4f}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç®€åŒ–ç‰ˆå¼•å¯¼å¼DiffusionéŸ³é¢‘é‡å»º")
    parser.add_argument("input_path", type=str, help="è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", type=str, default="guided_diffusion_simple_output", 
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--steps", "-s", type=int, default=50, 
                       help="Diffusionæ­¥æ•°")
    parser.add_argument("--guidance", "-g", type=float, default=0.1, 
                       help="å¼•å¯¼å¼ºåº¦")
    parser.add_argument("--no-compare", action="store_true", 
                       help="ä¸è¿›è¡ŒVAEå¯¹æ¯”")
    parser.add_argument("--device", type=str, default="cuda", 
                       help="è®¡ç®—è®¾å¤‡")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input_path):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_path}")
        return
    
    try:
        # åˆå§‹åŒ–é‡å»ºå™¨
        reconstructor = SimpleGuidedDiffusionReconstructor(device=args.device)
        
        # æ‰§è¡Œé‡å»º
        guided_path, vae_path = reconstructor.reconstruct_audio(
            input_path=args.input_path,
            output_dir=args.output,
            num_steps=args.steps,
            guidance_scale=args.guidance,
            compare_vae=not args.no_compare
        )
        
        print(f"\nâœ… é‡å»ºå®Œæˆ!")
        print(f"   å¼•å¯¼å¼é‡å»º: {guided_path}")
        if vae_path:
            print(f"   VAEé‡å»º: {vae_path}")
        
        print(f"\nğŸ’¡ æŠ€æœ¯åˆ›æ–°æ€»ç»“:")
        print(f"   - ç»“åˆäº†diffusionå’ŒVAEé‡å»ºçš„ä¼˜åŠ¿")
        print(f"   - åœ¨æ¯ä¸ªdiffusionæ­¥éª¤ä¸­åŠ å…¥ç›®æ ‡å¼•å¯¼")
        print(f"   - é€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–é‡å»ºè´¨é‡")
        print(f"   - å¯è°ƒèŠ‚çš„å¼•å¯¼å¼ºåº¦å’Œè¡°å‡ç­–ç•¥")
        
    except Exception as e:
        print(f"âŒ é‡å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
