"""
AudioLDM2 VAE æ”¹è¿›ç‰ˆæœ¬ - ä½¿ç”¨å†…ç½®Vocoder
å®ç°æœ€æœ‰å‰æ™¯çš„æ”¹è¿›ï¼šä½¿ç”¨AudioLDM2çš„SpeechT5HifiGan vocoderæ›¿ä»£Griffin-Lim

ä¸»è¦æ”¹è¿›ï¼š
1. ç›´æ¥ä½¿ç”¨AudioLDM2å†…ç½®vocoderè¿›è¡Œmelåˆ°éŸ³é¢‘è½¬æ¢
2. ä¼˜åŒ–mel-spectrogramå‚æ•°é…ç½®
3. æ”¹è¿›å½’ä¸€åŒ–ç­–ç•¥
4. æ·»åŠ å¤šç§é‡å»ºæ–¹æ³•å¯¹æ¯”
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F

from diffusers import AudioLDM2Pipeline


def enhanced_vae_test(audio_path, model_id="cvssp/audioldm2-music", max_length=10):
    """
    ä½¿ç”¨AudioLDM2å†…ç½®vocoderçš„å¢å¼ºVAEæµ‹è¯•
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")
    
    if not os.path.exists(audio_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ {audio_path}")
        return
    
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½ AudioLDM2 æ¨¡å‹: {model_id}")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    # æ‰“å°vocoderä¿¡æ¯
    print(f"ğŸ”§ Vocoderä¿¡æ¯: {type(vocoder).__name__}")
    if hasattr(vocoder, 'config'):
        config = vocoder.config
        print(f"   - è¾“å…¥ç»´åº¦: {config.model_in_dim}")
        print(f"   - é‡‡æ ·ç‡: {config.sampling_rate}")
        print(f"   - ä¸Šé‡‡æ ·ç‡: {config.upsample_rates}")
    
    print(f"ğŸ“ æ­£åœ¨åŠ è½½éŸ³é¢‘: {audio_path}")
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    
    if len(audio) > max_length * sample_rate:
        audio = audio[:int(max_length * sample_rate)]
        print(f"âœ‚ï¸ éŸ³é¢‘å·²è£å‰ªåˆ° {max_length} ç§’")
    
    print(f"ğŸ“Š éŸ³é¢‘ä¿¡æ¯: é•¿åº¦={len(audio)/sample_rate:.2f}ç§’, æ ·æœ¬æ•°={len(audio)}")
    
    # æµ‹è¯•å¤šç§æ–¹æ³•
    methods = [
        ("åŸå§‹Griffin-Lim", test_original_griffinlim),
        ("AudioLDM2 Vocoder", test_audioldm2_vocoder), 
        ("ä¼˜åŒ–å‚æ•°+Vocoder", test_optimized_vocoder),
        ("æœ€ä½³é…ç½®", test_best_config)
    ]
    
    results = {}
    output_dir = "vae_enhanced_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    audio_save = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio
    torchaudio.save(original_path, torch.from_numpy(audio_save).unsqueeze(0), sample_rate)
    
    print(f"\\nğŸ§ª å¼€å§‹æµ‹è¯• {len(methods)} ç§é‡å»ºæ–¹æ³•...")
    
    for i, (method_name, method_func) in enumerate(methods, 1):
        print(f"\\n{'='*50}")
        print(f"ğŸ”¬ æ–¹æ³• {i}: {method_name}")
        print(f"{'='*50}")
        
        try:
            result = method_func(audio, vae, vocoder, device, sample_rate)
            
            if result and result.get('audio') is not None:
                recon_path = os.path.join(output_dir, f"{input_name}_{method_name.replace(' ', '_')}_{timestamp}.wav")
                
                recon_audio = result['audio']
                
                # å®‰å…¨çš„å½’ä¸€åŒ–
                if len(recon_audio) > 0:
                    max_val = np.max(np.abs(recon_audio))
                    if max_val > 0:
                        recon_audio = recon_audio / max_val
                
                # ç¡®ä¿é•¿åº¦ä¸€è‡´
                target_length = len(audio)
                if len(recon_audio) > target_length:
                    recon_audio = recon_audio[:target_length]
                elif len(recon_audio) < target_length:
                    recon_audio = np.pad(recon_audio, (0, target_length - len(recon_audio)))
                
                # ä¿å­˜éŸ³é¢‘
                torchaudio.save(recon_path, torch.from_numpy(recon_audio).unsqueeze(0), sample_rate)
                
                # è®¡ç®—è¯¦ç»†è´¨é‡æŒ‡æ ‡
                metrics = calculate_detailed_metrics(audio, recon_audio, sample_rate)
                
                results[method_name] = {
                    'path': recon_path,
                    'metrics': metrics,
                    'processing_time': result.get('processing_time', 0),
                    'compression_info': result.get('compression_info', {}),
                    'success': True
                }
                
                print(f"âœ… {method_name} æˆåŠŸ!")
                print(f"   ğŸ“ˆ SNR: {metrics['snr']:.2f} dB")
                print(f"   ğŸ”— ç›¸å…³ç³»æ•°: {metrics['correlation']:.4f}")
                print(f"   â±ï¸ å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f}ç§’")
            else:
                print(f"âŒ {method_name} å¤±è´¥: æ— éŸ³é¢‘è¾“å‡º")
                results[method_name] = {'success': False}
                
        except Exception as e:
            print(f"âŒ {method_name} å‡ºé”™: {str(e)}")
            results[method_name] = {'success': False, 'error': str(e)}
    
    # æ‰“å°æœ€ç»ˆå¯¹æ¯”ç»“æœ
    print_enhanced_results(original_path, results)
    
    return results


def test_original_griffinlim(audio, vae, vocoder, device, sample_rate):
    """åŸå§‹Griffin-Limæ–¹æ³•ï¼ˆåŸºçº¿ï¼‰"""
    start_time = time.time()
    
    # ä½¿ç”¨åŸå§‹å‚æ•°
    mel_spec = librosa.feature.melspectrogram(
        y=audio, sr=sample_rate, n_fft=1024, hop_length=160, n_mels=64,
        fmin=0, fmax=8000, power=2.0
    )
    
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_min, mel_max = mel_spec_db.min(), mel_spec_db.max()
    mel_spec_normalized = 2 * (mel_spec_db - mel_min) / (mel_max - mel_min) - 1
    mel_spec_normalized = np.clip(mel_spec_normalized, -1, 1)
    
    # VAEå¤„ç†
    recon_audio, comp_info = process_through_vae(
        mel_spec_normalized, vae, device, 
        norm_params={'min': mel_min, 'max': mel_max}
    )
    
    # Griffin-Limé‡å»º
    recon_mel_denorm = (recon_audio + 1) / 2 * (mel_max - mel_min) + mel_min
    recon_mel_power = librosa.db_to_power(recon_mel_denorm)
    final_audio = librosa.feature.inverse.mel_to_audio(
        recon_mel_power, sr=sample_rate, hop_length=160, n_fft=1024, n_iter=32
    )
    
    processing_time = time.time() - start_time
    
    return {
        'audio': final_audio,
        'processing_time': processing_time,
        'compression_info': comp_info
    }


def test_audioldm2_vocoder(audio, vae, vocoder, device, sample_rate):
    """ä½¿ç”¨AudioLDM2å†…ç½®vocoder"""
    start_time = time.time()
    
    # æ ‡å‡†melå‚æ•°
    mel_spec = librosa.feature.melspectrogram(
        y=audio, sr=sample_rate, n_fft=1024, hop_length=160, n_mels=64,
        fmin=0, fmax=8000, power=2.0
    )
    
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_min, mel_max = mel_spec_db.min(), mel_spec_db.max()
    mel_spec_normalized = 2 * (mel_spec_db - mel_min) / (mel_max - mel_min) - 1
    mel_spec_normalized = np.clip(mel_spec_normalized, -1, 1)
    
    # VAEå¤„ç†
    recon_mel, comp_info = process_through_vae(
        mel_spec_normalized, vae, device,
        norm_params={'min': mel_min, 'max': mel_max},
        return_mel=True
    )
    
    # ä½¿ç”¨AudioLDM2 vocoder
    try:
        print("ğŸ¤ å°è¯•ä½¿ç”¨AudioLDM2å†…ç½®vocoder...")
        
        # å‡†å¤‡vocoderè¾“å…¥
        mel_tensor = torch.from_numpy(recon_mel).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.to(torch.float16)
        
        # è°ƒæ•´å½¢çŠ¶é€‚é…vocoder
        if mel_tensor.dim() == 2:
            mel_tensor = mel_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        print(f"   Vocoderè¾“å…¥å½¢çŠ¶: {mel_tensor.shape}")
        
        with torch.no_grad():
            # è°ƒç”¨vocoder
            vocoder_output = vocoder(mel_tensor)
            
            if isinstance(vocoder_output, tuple):
                final_audio = vocoder_output[0]
            else:
                final_audio = vocoder_output
            
            # è½¬æ¢ä¸ºnumpy
            if torch.is_tensor(final_audio):
                final_audio = final_audio.squeeze().cpu().numpy()
            
            print(f"âœ… Vocoderé‡å»ºæˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {final_audio.shape}")
        
    except Exception as e:
        print(f"âŒ Vocoderå¤±è´¥: {e}")
        print("ğŸ”„ å›é€€åˆ°Griffin-Lim...")
        
        # å›é€€åˆ°Griffin-Lim
        recon_mel_denorm = (recon_mel + 1) / 2 * (mel_max - mel_min) + mel_min
        recon_mel_power = librosa.db_to_power(recon_mel_denorm)
        final_audio = librosa.feature.inverse.mel_to_audio(
            recon_mel_power, sr=sample_rate, hop_length=160, n_fft=1024
        )
    
    processing_time = time.time() - start_time
    
    return {
        'audio': final_audio,
        'processing_time': processing_time,
        'compression_info': comp_info
    }


def test_optimized_vocoder(audio, vae, vocoder, device, sample_rate):
    """ä¼˜åŒ–å‚æ•° + vocoder"""
    start_time = time.time()
    
    # ä¼˜åŒ–çš„melå‚æ•°
    mel_spec = librosa.feature.melspectrogram(
        y=audio, sr=sample_rate, 
        n_fft=2048,      # æ›´é«˜åˆ†è¾¨ç‡
        hop_length=256,  # æ›´é«˜æ—¶é—´åˆ†è¾¨ç‡  
        win_length=2048,
        n_mels=80,       # æ›´å¤šmel bins
        fmin=0, fmax=8000, power=1.0  # ä½¿ç”¨å¹…åº¦è°±
    )
    
    # æ”¹è¿›çš„å½’ä¸€åŒ–
    mel_spec_log = np.log(mel_spec + 1e-8)
    p5, p95 = np.percentile(mel_spec_log, [5, 95])
    mel_spec_normalized = 2 * (mel_spec_log - p5) / (p95 - p5) - 1
    mel_spec_normalized = np.clip(mel_spec_normalized, -1, 1)
    
    # è°ƒæ•´åˆ°64 mels (vocoderè¦æ±‚)
    if mel_spec_normalized.shape[0] != 64:
        from scipy.interpolate import interp1d
        old_indices = np.linspace(0, 1, mel_spec_normalized.shape[0])
        new_indices = np.linspace(0, 1, 64)
        interpolator = interp1d(old_indices, mel_spec_normalized, axis=0, kind='linear')
        mel_spec_normalized = interpolator(new_indices)
    
    # è°ƒæ•´æ—¶é—´ç»´åº¦åˆ°æ ‡å‡†hop_length
    target_frames = len(audio) // 160
    if mel_spec_normalized.shape[1] != target_frames:
        from scipy.interpolate import interp1d
        old_indices = np.linspace(0, 1, mel_spec_normalized.shape[1])
        new_indices = np.linspace(0, 1, target_frames)
        interpolator = interp1d(old_indices, mel_spec_normalized, axis=1, kind='linear')
        mel_spec_normalized = interpolator(new_indices)
    
    # VAEå¤„ç†
    recon_mel, comp_info = process_through_vae(
        mel_spec_normalized, vae, device,
        norm_params={'p5': p5, 'p95': p95, 'method': 'percentile'},
        return_mel=True
    )
    
    # ä½¿ç”¨vocoderé‡å»º
    try:
        mel_tensor = torch.from_numpy(recon_mel).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.to(torch.float16)
        
        if mel_tensor.dim() == 2:
            mel_tensor = mel_tensor.unsqueeze(0)
        
        with torch.no_grad():
            vocoder_output = vocoder(mel_tensor)
            if isinstance(vocoder_output, tuple):
                final_audio = vocoder_output[0]
            else:
                final_audio = vocoder_output
            
            if torch.is_tensor(final_audio):
                final_audio = final_audio.squeeze().cpu().numpy()
        
        print("âœ… ä¼˜åŒ–å‚æ•°+Vocoderé‡å»ºæˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–vocoderå¤±è´¥: {e}, ä½¿ç”¨Griffin-Lim")
        # å›é€€å¤„ç†
        if 'method' in comp_info:
            recon_mel_denorm = (recon_mel + 1) / 2 * (p95 - p5) + p5
        else:
            recon_mel_denorm = recon_mel
        
        recon_mel_linear = np.exp(recon_mel_denorm)
        final_audio = librosa.feature.inverse.mel_to_audio(
            recon_mel_linear, sr=sample_rate, hop_length=160, n_fft=1024
        )
    
    processing_time = time.time() - start_time
    
    return {
        'audio': final_audio,
        'processing_time': processing_time,
        'compression_info': comp_info
    }


def test_best_config(audio, vae, vocoder, device, sample_rate):
    """æœ€ä½³é…ç½®ï¼šç»“åˆæ‰€æœ‰ä¼˜åŒ–"""
    start_time = time.time()
    
    # æœ€ä¼˜melå‚æ•°é…ç½®
    mel_spec = librosa.feature.melspectrogram(
        y=audio, sr=sample_rate,
        n_fft=1024,      # ä¸vocoderåŒ¹é…
        hop_length=160,  # ä¸vocoderåŒ¹é…
        win_length=1024,
        n_mels=64,       # ä¸vocoderåŒ¹é…
        fmin=0, fmax=8000,
        power=1.0,       # å¹…åº¦è°±æ›´å¥½
        norm='slaney'    # æ›´å¥½çš„melæ»¤æ³¢å™¨
    )
    
    # æœ€ç¨³å¥çš„å½’ä¸€åŒ–
    mel_spec_log = np.log(mel_spec + 1e-8)
    mel_mean = mel_spec_log.mean()
    mel_std = mel_spec_log.std()
    mel_spec_normalized = (mel_spec_log - mel_mean) / (mel_std + 1e-8)
    mel_spec_normalized = np.clip(mel_spec_normalized, -3, 3)  # åˆç†èŒƒå›´
    
    # VAEå¤„ç†
    recon_mel, comp_info = process_through_vae(
        mel_spec_normalized, vae, device,
        norm_params={'mean': mel_mean, 'std': mel_std, 'method': 'zscore'},
        return_mel=True
    )
    
    # ä½¿ç”¨vocoder + åå¤„ç†
    try:
        mel_tensor = torch.from_numpy(recon_mel).to(device)
        if device == "cuda":
            mel_tensor = mel_tensor.to(torch.float16)
        
        if mel_tensor.dim() == 2:
            mel_tensor = mel_tensor.unsqueeze(0)
        
        with torch.no_grad():
            vocoder_output = vocoder(mel_tensor)
            if isinstance(vocoder_output, tuple):
                final_audio = vocoder_output[0]
            else:
                final_audio = vocoder_output
            
            if torch.is_tensor(final_audio):
                final_audio = final_audio.squeeze().cpu().numpy()
        
        # åå¤„ç†ï¼šè½»å¾®å¹³æ»‘
        if len(final_audio) > 21:
            from scipy.signal import savgol_filter
            try:
                final_audio = savgol_filter(final_audio, 21, 3)
            except:
                pass
        
        print("âœ… æœ€ä½³é…ç½®é‡å»ºæˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ æœ€ä½³é…ç½®å¤±è´¥: {e}")
        # é«˜è´¨é‡å›é€€
        recon_mel_denorm = recon_mel * mel_std + mel_mean
        recon_mel_linear = np.exp(recon_mel_denorm)
        final_audio = librosa.feature.inverse.mel_to_audio(
            recon_mel_linear, sr=sample_rate, hop_length=160, n_fft=1024, n_iter=100
        )
    
    processing_time = time.time() - start_time
    
    return {
        'audio': final_audio,
        'processing_time': processing_time,
        'compression_info': comp_info
    }


def process_through_vae(mel_spec_normalized, vae, device, norm_params, return_mel=False):
    """é€šè¿‡VAEå¤„ç†mel-spectrogram"""
    
    # è½¬æ¢ä¸ºå¼ é‡
    mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.to(torch.float16)
    
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, mels, time)
    
    # å¡«å……åˆ°8çš„å€æ•°
    if mel_input.shape[-1] % 8 != 0:
        pad_width = 8 - (mel_input.shape[-1] % 8)
        mel_input = F.pad(mel_input, (0, pad_width))
    
    with torch.no_grad():
        # VAEç¼–ç è§£ç 
        latents = vae.encode(mel_input).latent_dist.sample()
        latents = latents * vae.config.scaling_factor
        latents = latents / vae.config.scaling_factor
        reconstructed_mel = vae.decode(latents).sample
        
        # è½¬æ¢å›numpy
        recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
        
        if return_mel:
            return recon_mel_np, {
                'compression_ratio': mel_input.numel() / latents.numel(),
                'latent_shape': latents.shape
            }
        else:
            # åå½’ä¸€åŒ–å¤„ç†
            if norm_params.get('method') == 'percentile':
                p5, p95 = norm_params['p5'], norm_params['p95']
                recon_mel_denorm = (recon_mel_np + 1) / 2 * (p95 - p5) + p5
            elif norm_params.get('method') == 'zscore':
                mean, std = norm_params['mean'], norm_params['std']
                recon_mel_denorm = recon_mel_np * std + mean
            else:
                mel_min, mel_max = norm_params['min'], norm_params['max']
                recon_mel_denorm = (recon_mel_np + 1) / 2 * (mel_max - mel_min) + mel_min
            
            return recon_mel_denorm, {
                'compression_ratio': mel_input.numel() / latents.numel(),
                'latent_shape': latents.shape
            }


def calculate_detailed_metrics(original, reconstructed, sample_rate):
    """è®¡ç®—è¯¦ç»†çš„è´¨é‡æŒ‡æ ‡"""
    min_len = min(len(original), len(reconstructed))
    if min_len == 0:
        return {'snr': -np.inf, 'correlation': 0}
    
    orig = original[:min_len]
    recon = reconstructed[:min_len]
    
    # åŸºç¡€æŒ‡æ ‡
    mse = np.mean((orig - recon) ** 2)
    signal_power = np.mean(orig ** 2)
    noise_power = mse
    
    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
    
    try:
        correlation = np.corrcoef(orig, recon)[0, 1]
        if np.isnan(correlation):
            correlation = 0
    except:
        correlation = 0
    
    # é¢‘è°±æŒ‡æ ‡
    try:
        orig_fft = np.abs(np.fft.fft(orig))
        recon_fft = np.abs(np.fft.fft(recon))
        spectral_corr = np.corrcoef(orig_fft, recon_fft)[0, 1]
        if np.isnan(spectral_corr):
            spectral_corr = 0
    except:
        spectral_corr = 0
    
    # æ„ŸçŸ¥æŒ‡æ ‡
    try:
        orig_mfcc = librosa.feature.mfcc(y=orig, sr=sample_rate, n_mfcc=13)
        recon_mfcc = librosa.feature.mfcc(y=recon, sr=sample_rate, n_mfcc=13)
        mfcc_corr = np.corrcoef(orig_mfcc.flatten(), recon_mfcc.flatten())[0, 1]
        if np.isnan(mfcc_corr):
            mfcc_corr = 0
    except:
        mfcc_corr = 0
    
    return {
        'mse': float(mse),
        'snr': float(snr),
        'correlation': float(correlation),
        'spectral_correlation': float(spectral_corr),
        'mfcc_correlation': float(mfcc_corr)
    }


def print_enhanced_results(original_path, results):
    """æ‰“å°å¢å¼ºæµ‹è¯•ç»“æœ"""
    print(f"\\n{'='*70}")
    print(f"ğŸ¯ AudioLDM2 VAE å¢å¼ºé‡å»ºæµ‹è¯•ç»“æœ")
    print(f"{'='*70}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {os.path.basename(original_path)}")
    print()
    
    successful_results = {k: v for k, v in results.items() if v.get('success', False)}
    
    if not successful_results:
        print("âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†")
        return
    
    # æŒ‰SNRæ’åº
    sorted_results = sorted(successful_results.items(), 
                           key=lambda x: x[1]['metrics']['snr'], reverse=True)
    
    print(f"ğŸ“Š æ–¹æ³•å¯¹æ¯” (æŒ‰SNRæ’åº):")
    print("-" * 70)
    
    baseline_snr = None
    for i, (method_name, data) in enumerate(sorted_results, 1):
        metrics = data['metrics']
        
        if i == 1:
            baseline_snr = metrics['snr']
        
        improvement = metrics['snr'] - baseline_snr if baseline_snr is not None else 0
        
        print(f"ğŸ† #{i} {method_name}:")
        print(f"   ğŸ“„ æ–‡ä»¶: {os.path.basename(data['path'])}")
        print(f"   ğŸ“ˆ SNR: {metrics['snr']:.2f} dB ({improvement:+.2f})")
        print(f"   ğŸ”— æ—¶åŸŸç›¸å…³æ€§: {metrics['correlation']:.4f}")
        print(f"   ğŸµ é¢‘è°±ç›¸å…³æ€§: {metrics['spectral_correlation']:.4f}")
        print(f"   ğŸ¤ MFCCç›¸å…³æ€§: {metrics['mfcc_correlation']:.4f}")
        print(f"   â±ï¸ å¤„ç†æ—¶é—´: {data['processing_time']:.2f}ç§’")
        
        if 'compression_info' in data:
            comp_info = data['compression_info']
            print(f"   ğŸ“¦ å‹ç¼©æ¯”: {comp_info.get('compression_ratio', 0):.1f}:1")
        print()
    
    # æ€»ç»“æ”¹è¿›æ•ˆæœ
    if len(sorted_results) > 1:
        best_snr = sorted_results[0][1]['metrics']['snr']
        baseline_snr = sorted_results[-1][1]['metrics']['snr']
        improvement = best_snr - baseline_snr
        
        print(f"ğŸš€ æœ€ä½³æ”¹è¿›æ•ˆæœ:")
        print(f"   ğŸ“ˆ SNRæå‡: {improvement:.2f} dB")
        print(f"   ğŸ† æœ€ä½³æ–¹æ³•: {sorted_results[0][0]}")
    
    print(f"\\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨ vae_enhanced_test/ ç›®å½•")
    print("ğŸ§ å»ºè®®æ’­æ”¾éŸ³é¢‘æ–‡ä»¶æ¥ä¸»è§‚è¯„ä¼°æ”¹è¿›æ•ˆæœ")


def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python enhanced_vae_test.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„> [æœ€å¤§é•¿åº¦ç§’æ•°]")
        
        audio_files = []
        for ext in ['.wav', '.mp3', '.flac']:
            audio_files.extend(Path('.').glob(f'*{ext}'))
        
        if audio_files:
            print(f"\\næ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files, 1):
                print(f"{i}. {file}")
            
            try:
                choice = input("è¯·é€‰æ‹©æ–‡ä»¶åºå·: ").strip()
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(audio_files):
                    audio_path = str(audio_files[file_idx])
                else:
                    print("æ— æ•ˆé€‰æ‹©")
                    return
            except (ValueError, KeyboardInterrupt):
                print("å–æ¶ˆæ“ä½œ")
                return
        else:
            print("å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    max_length = 5  # é»˜è®¤5ç§’ç”¨äºå¿«é€Ÿæµ‹è¯•
    if len(sys.argv) >= 3:
        try:
            max_length = float(sys.argv[2])
        except ValueError:
            print(f"æ— æ•ˆçš„é•¿åº¦å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {max_length} ç§’")
    
    print(f"ğŸš€ å¼€å§‹å¢å¼ºVAEæµ‹è¯•: {audio_path}")
    print(f"â±ï¸ æœ€å¤§é•¿åº¦: {max_length} ç§’")
    
    try:
        results = enhanced_vae_test(audio_path, max_length=max_length)
        if results:
            print("\\nâœ… å¢å¼ºæµ‹è¯•å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
