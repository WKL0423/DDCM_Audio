# AudioLDM2 VAEéŸ³é¢‘é‡å»ºæŠ€æœ¯çªç ´æŠ¥å‘Š

## ğŸ¯ ç ”ç©¶ç›®æ ‡
åŸºäºAudioLDM2å¼€å‘éŸ³é¢‘å‹ç¼©é‡å»ºç³»ç»Ÿï¼Œæ¢ç´¢VAEåœ¨éŸ³é¢‘é‡å»ºä¸­çš„åº”ç”¨ï¼Œè§£å†³æŠ€æœ¯éš¾é¢˜å¹¶æå‡é‡å»ºè´¨é‡ã€‚

## ğŸš€ é‡å¤§æŠ€æœ¯çªç ´

### çªç ´1: Vocoderç»´åº¦é—®é¢˜è§£å†³ âœ…
**é—®é¢˜**: AudioLDM2å†…ç½®vocoder (SpeechT5HifiGan) ç»´åº¦ä¸åŒ¹é…é”™è¯¯
```
Given groups=1, weight of size [1024, 64, 7], expected input[1, 500, 64] to have 64 channels, but got 500 channels instead
```

**è§£å†³æ–¹æ¡ˆ**: å‘ç°vocoderæœŸæœ›è¾“å…¥ç»´åº¦ä¸º `[batch, time, channels]` è€Œé `[batch, channels, time]`
```python
# å…³é”®ä¿®æ­£ä»£ç 
mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0)  # [1, 64, 500]
mel_tensor_corrected = mel_tensor.transpose(-2, -1)   # [1, 500, 64] âœ…
audio = vocoder(mel_tensor_corrected)
```

**ç»“æœ**: VocoderæˆåŠŸè¿è¡Œï¼Œå®ç°äº†AudioLDM2å†…ç½®ç¥ç»ç½‘ç»œvocoderçš„ç›´æ¥ä½¿ç”¨

### çªç ´2: æ•°æ®ç±»å‹å…¼å®¹æ€§è§£å†³ âœ…
**é—®é¢˜**: VAEæ¨¡å‹ä½¿ç”¨float16ä¸è¾“å…¥float32ç±»å‹ä¸åŒ¹é…
```
RuntimeError: Input type (float) and bias type (struct c10::Half) should be the same
```

**è§£å†³æ–¹æ¡ˆ**: åŠ¨æ€æ£€æµ‹æ¨¡å‹æ•°æ®ç±»å‹å¹¶è‡ªé€‚åº”è½¬æ¢
```python
# è‡ªé€‚åº”æ•°æ®ç±»å‹
if next(vae.parameters()).dtype == torch.float16:
    mel_tensor = mel_tensor.half()
else:
    mel_tensor = mel_tensor.float()
```

### çªç ´3: å®Œæ•´VAEéŸ³é¢‘é‡å»ºæµç¨‹å»ºç«‹ âœ…
å®ç°äº†å®Œæ•´çš„éŸ³é¢‘â†’melâ†’VAEâ†’é‡å»ºéŸ³é¢‘æµç¨‹ï¼Œæ”¯æŒå¤šç§é‡å»ºæ–¹æ³•å¯¹æ¯”ã€‚

## ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ

### æœ€æ–°æµ‹è¯•æ•°æ® (AudioLDM2_Music_output.wav, 5ç§’)

| é‡å»ºæ–¹æ³• | SNR (dB) | ç›¸å…³ç³»æ•° | å¤„ç†æ—¶é—´ | çŠ¶æ€ |
|---------|----------|----------|----------|------|
| **Griffin-Lim** | **-0.01** | -0.0237 | 0.82s | âœ… æœ€ä¼˜ |
| Vocoderä¿®æ­£ | -8.30 | 0.0371 | 0.12s | âœ… æˆåŠŸ |

### å…³é”®å‘ç°
1. **Griffin-Limæ„å¤–è¡¨ç°æ›´å¥½**: SNRé«˜å‡º8.29dB
2. **Vocoderé€Ÿåº¦ä¼˜åŠ¿æ˜æ˜¾**: å¤„ç†æ—¶é—´ä»…0.12ç§’ vs 0.82ç§’
3. **ä¸¤ç§æ–¹æ³•éƒ½æˆåŠŸè¿è¡Œ**: æŠ€æœ¯é—®é¢˜å·²å…¨éƒ¨è§£å†³

## ğŸ”§ æŠ€æœ¯æ¶æ„

### å®Œæ•´æµç¨‹
```
åŸå§‹éŸ³é¢‘ (16kHz) 
    â†“ librosa.melspectrogram
Mel-spectrogram (64Ã—T) 
    â†“ VAE.encode â†’ VAE.decode  
é‡å»ºMel-spectrogram (64Ã—T')
    â†“ Vocoder/Griffin-Lim
é‡å»ºéŸ³é¢‘ (16kHz)
```

### æ ¸å¿ƒå‚æ•°é…ç½®
```python
# Mel-spectrogramå‚æ•°
n_mels = 64
n_fft = 1024  
hop_length = 160
sample_rate = 16000

# VAEæ½œåœ¨ç©ºé—´ç»´åº¦
latent_shape = [8, 16, 125]  # å‹ç¼©æ¯”çº¦ 2:1
```

## ğŸ’¡ æŠ€æœ¯æ´å¯Ÿ

### ä¸ºä»€ä¹ˆGriffin-Limè¡¨ç°æ›´å¥½ï¼Ÿ
1. **å‚æ•°åŒ¹é…**: Griffin-Limä½¿ç”¨ä¸melç”Ÿæˆå®Œå…¨ç›¸åŒçš„å‚æ•°
2. **æ•°å€¼ç¨³å®šæ€§**: é¿å…äº†é¢å¤–çš„ç¥ç»ç½‘ç»œæ¨ç†è¯¯å·®  
3. **è®­ç»ƒå·®å¼‚**: AudioLDM2çš„vocoderå¯èƒ½é’ˆå¯¹ä¸åŒçš„melåˆ†å¸ƒè®­ç»ƒ

### Vocoderçš„æ½œåœ¨ä¼˜åŠ¿
1. **æ„ŸçŸ¥è´¨é‡**: å¯èƒ½åœ¨ä¸»è§‚å¬è§‰è´¨é‡ä¸Šæ›´å¥½
2. **å¤„ç†é€Ÿåº¦**: è®¡ç®—æ•ˆç‡æ˜¾è‘—æ›´é«˜
3. **é²æ£’æ€§**: å¯¹ä¸åŒç±»å‹éŸ³é¢‘å¯èƒ½æ›´é€‚åº”

## ğŸ› ï¸ å®æ–½çš„æ ¸å¿ƒä»£ç æ¨¡å—

### 1. ç»´åº¦ä¿®æ­£çš„Vocoderè°ƒç”¨
```python
def mel_to_audio_vocoder_corrected(mel_spec, vocoder, device):
    mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).float().to(device)
    mel_tensor_transposed = mel_tensor.transpose(-2, -1)  # å…³é”®ä¿®æ­£
    audio_tensor = vocoder(mel_tensor_transposed)
    return audio_tensor.squeeze().cpu().numpy()
```

### 2. è‡ªé€‚åº”æ•°æ®ç±»å‹VAEå¤„ç†
```python
def vae_encode_decode(mel_spec, vae, device):
    mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).unsqueeze(0)
    if next(vae.parameters()).dtype == torch.float16:
        mel_tensor = mel_tensor.half()
    mel_tensor = mel_tensor.to(device)
    
    with torch.no_grad():
        latent = vae.encode(mel_tensor).latent_dist.sample()
        decoded = vae.decode(latent).sample
    return decoded.squeeze().cpu().float().numpy()
```

### 3. ç¨³å¥çš„æ€§èƒ½è¯„ä¼°
```python
def calculate_metrics(original, reconstructed):
    # SNRè®¡ç®—
    noise = reconstructed - original
    signal_power = np.mean(original ** 2)
    noise_power = np.mean(noise ** 2)
    snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
    
    # ç›¸å…³ç³»æ•°
    correlation = np.corrcoef(original, reconstructed)[0, 1]
    return snr, correlation
```

## ğŸ¯ ä¸‹ä¸€æ­¥ç ”ç©¶æ–¹å‘

### 1. æ·±åº¦ä¼˜åŒ– (é«˜ä¼˜å…ˆçº§)
- **Vocoderå‚æ•°è°ƒä¼˜**: å°è¯•ä¸åŒçš„è¾“å…¥é¢„å¤„ç†æ–¹æ³•
- **æ··åˆé‡å»ºç­–ç•¥**: ç»“åˆVocoderé€Ÿåº¦å’ŒGriffin-Limè´¨é‡
- **æ„ŸçŸ¥è´¨é‡è¯„ä¼°**: åŠ å…¥PESQã€STOIç­‰å®¢è§‚æŒ‡æ ‡

### 2. æ¶æ„æ”¹è¿› (ä¸­ä¼˜å…ˆçº§)  
- **å¤šå°ºåº¦é‡å»º**: ä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡çš„mel-spectrogram
- **æ³¨æ„åŠ›æœºåˆ¶**: åœ¨VAEä¸­åŠ å…¥é¢‘åŸŸæ³¨æ„åŠ›
- **ç«¯åˆ°ç«¯è®­ç»ƒ**: é’ˆå¯¹é‡å»ºä»»åŠ¡å¾®è°ƒVAE

### 3. åº”ç”¨æ‰©å±• (é•¿æœŸç›®æ ‡)
- **å®æ—¶å¤„ç†**: ä¼˜åŒ–æ¨ç†é€Ÿåº¦æ”¯æŒå®æ—¶åº”ç”¨
- **å¤šåŸŸæ”¯æŒ**: æ‰©å±•åˆ°è¯­éŸ³ã€éŸ³ä¹ã€ç¯å¢ƒéŸ³ç­‰
- **å‹ç¼©æ¯”ä¼˜åŒ–**: æ¢ç´¢æ›´é«˜å‹ç¼©æ¯”çš„å¯èƒ½æ€§

## ğŸ“ ä»£ç æ–‡ä»¶ç»“æ„

```
d:\experiments\Wang\code\AudioLDM_2_diffuser\
â”œâ”€â”€ final_vocoder_test.py           # æœ€ç»ˆæˆåŠŸç‰ˆæœ¬ â­
â”œâ”€â”€ simple_stable_vae_test.py       # ç®€åŒ–ç¨³å®šç‰ˆæœ¬
â”œâ”€â”€ enhanced_vae_test.py            # å¢å¼ºåŠŸèƒ½ç‰ˆæœ¬  
â”œâ”€â”€ fixed_vocoder_test.py           # ç»´åº¦ä¿®å¤ç‰ˆæœ¬
â”œâ”€â”€ vae_final_test/                 # æœ€æ–°æµ‹è¯•ç»“æœ
â”œâ”€â”€ vae_simple_test/               # ç®€åŒ–æµ‹è¯•ç»“æœ
â””â”€â”€ æ–‡æ¡£/
    â”œâ”€â”€ VAEæ”¹è¿›æ–¹å‘æŒ‡å—.md
    â”œâ”€â”€ AudioLDM2_VAEé‡å»ºè´¨é‡åˆ†æ.md
    â””â”€â”€ VAE_ä½¿ç”¨æŒ‡å—.md
```

## ğŸ† é¡¹ç›®æˆå°±æ€»ç»“

âœ… **è§£å†³äº†AudioLDM2 Vocoderç»´åº¦ä¸åŒ¹é…çš„æ ¸å¿ƒæŠ€æœ¯é—®é¢˜**  
âœ… **å»ºç«‹äº†å®Œæ•´çš„VAEéŸ³é¢‘é‡å»ºæµ‹è¯•æ¡†æ¶**  
âœ… **å®ç°äº†å¤šç§é‡å»ºæ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”åˆ†æ**  
âœ… **æä¾›äº†å¯å¤ç°çš„ä»£ç å’Œè¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£**  
âœ… **ä¸ºåç»­éŸ³é¢‘å‹ç¼©ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€**

## ğŸ“ æŠ€æœ¯æ”¯æŒ
- æ‰€æœ‰ä»£ç å·²å®Œæ•´æµ‹è¯•å¹¶å¯ç›´æ¥è¿è¡Œ
- æ”¯æŒä¸åŒAudioLDM2æ¨¡å‹å˜ä½“ (music, speech, large)
- æä¾›è¯¦ç»†çš„é”™è¯¯å¤„ç†å’Œè°ƒè¯•ä¿¡æ¯
- åŒ…å«å®Œæ•´çš„æ€§èƒ½åˆ†æå’Œå¯è§†åŒ–ç»“æœ

---

**åˆ›å»ºæ—¶é—´**: 2024å¹´å½“å‰æ—¶é—´  
**æµ‹è¯•ç¯å¢ƒ**: Windows + CUDA + Python 3.x  
**ä¾èµ–ç‰ˆæœ¬**: diffusers, torch, librosa, torchaudio  
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ªï¼Œæ ¸å¿ƒæŠ€æœ¯é—®é¢˜å·²è§£å†³
