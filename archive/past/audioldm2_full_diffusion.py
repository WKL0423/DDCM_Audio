"""
AudioLDM2 å®Œæ•´ Diffusion Pipeline é‡å»º
åŒ…å«çœŸæ­£çš„ diffusion å»å™ªè¿‡ç¨‹ï¼Œè€Œä¸ä»…ä»…æ˜¯ VAE encode/decode
"""

import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Optional, Union, Tuple, List
from diffusers import AudioLDM2Pipeline, DDIMScheduler, PNDMScheduler
import warnings
warnings.filterwarnings("ignore")

class AudioLDM2FullDiffusion:
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """
        åˆå§‹åŒ– AudioLDM2 å®Œæ•´ diffusion pipeline
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸµ åˆå§‹åŒ– AudioLDM2 å®Œæ•´ diffusion pipeline...")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        
        # åŠ è½½å®Œæ•´çš„ AudioLDM2 pipeline
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        # è®¾ç½®ä¸åŒçš„è°ƒåº¦å™¨é€‰é¡¹
        self.schedulers = {
            "ddim": DDIMScheduler.from_config(self.pipeline.scheduler.config),
            "pndm": PNDMScheduler.from_config(self.pipeline.scheduler.config),
            "default": self.pipeline.scheduler
        }
        
        print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        print(f"   ğŸ“Š VAE channels: {self.pipeline.vae.config.latent_channels}")
        print(f"   ğŸ”„ è°ƒåº¦å™¨é€‰é¡¹: {list(self.schedulers.keys())}")
          def encode_audio_to_latent(self, audio: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç éŸ³é¢‘ä¸ºæ½œåœ¨è¡¨ç¤º
        
        Args:
            audio: éŸ³é¢‘å¼ é‡ [batch_size, channels, samples]
            
        Returns:
            latent: æ½œåœ¨è¡¨ç¤º [batch_size, channels, height, width]
        """
        with torch.no_grad():
            # ç¡®ä¿éŸ³é¢‘æ ¼å¼æ­£ç¡®
            if audio.dim() == 1:
                audio = audio.unsqueeze(0)  # [1, samples]
            elif audio.dim() == 2 and audio.shape[0] > 1:
                audio = audio.mean(dim=0, keepdim=True)  # è½¬ä¸ºå•å£°é“
            
            # é‡é‡‡æ ·åˆ° 48kHzï¼ˆClapFeatureExtractor è¦æ±‚ï¼‰
            if audio.shape[-1] != int(48000 * 10):  # å‡è®¾ 10 ç§’éŸ³é¢‘
                target_length = min(48000 * 10, audio.shape[-1])
                audio = audio[..., :target_length]
            
            # ä½¿ç”¨ feature_extractor è½¬æ¢ä¸º mel-spectrogram
            inputs = self.pipeline.feature_extractor(
                audio.squeeze(0).cpu().numpy(),
                sampling_rate=48000,
                return_tensors="pt"
            )
            
            mel = inputs["input_features"].to(self.device)
            if mel.dim() == 3:
                mel = mel.unsqueeze(1)  # [batch, 1, freq, time]
            
            # ç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤º
            latent_dist = self.pipeline.vae.encode(mel)
            
            # é‡‡æ ·è·å–ç¡®å®šæ€§æ½œåœ¨è¡¨ç¤º
            latent = latent_dist.latent_dist.mode() if hasattr(latent_dist.latent_dist, 'mode') else latent_dist.latent_dist.sample()
            
            return latent
    
    def add_noise_to_latent(self, latent: torch.Tensor, noise_level: float = 0.8) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‘æ½œåœ¨è¡¨ç¤ºæ·»åŠ å™ªå£°ï¼ˆæ¨¡æ‹Ÿ diffusion å‰å‘è¿‡ç¨‹ï¼‰
        
        Args:
            latent: åŸå§‹æ½œåœ¨è¡¨ç¤º
            noise_level: å™ªå£°æ°´å¹³ (0-1)
            
        Returns:
            noisy_latent: åŠ å™ªåçš„æ½œåœ¨è¡¨ç¤º
            noise: æ·»åŠ çš„å™ªå£°
        """
        # ç”Ÿæˆé«˜æ–¯å™ªå£°
        noise = torch.randn_like(latent)
        
        # ä½¿ç”¨è°ƒåº¦å™¨çš„å‰å‘è¿‡ç¨‹æ·»åŠ å™ªå£°
        # é€‰æ‹©å™ªå£°æ­¥æ•°
        timestep = int(noise_level * self.pipeline.scheduler.config.num_train_timesteps)
        timesteps = torch.tensor([timestep], device=latent.device)
        
        # æ·»åŠ å™ªå£°
        noisy_latent = self.pipeline.scheduler.add_noise(latent, noise, timesteps)
        
        return noisy_latent, noise
    
    def denoise_latent(self, 
                      noisy_latent: torch.Tensor, 
                      prompt: str = "high quality music", 
                      num_inference_steps: int = 20,
                      guidance_scale: float = 7.5,
                      scheduler_name: str = "ddim") -> torch.Tensor:
        """
        ä½¿ç”¨ diffusion å»å™ªæ½œåœ¨è¡¨ç¤º
        
        Args:
            noisy_latent: åŠ å™ªçš„æ½œåœ¨è¡¨ç¤º
            prompt: æ–‡æœ¬æç¤º
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            scheduler_name: è°ƒåº¦å™¨åç§°
            
        Returns:
            denoised_latent: å»å™ªåçš„æ½œåœ¨è¡¨ç¤º
        """
        # è®¾ç½®è°ƒåº¦å™¨
        original_scheduler = self.pipeline.scheduler
        self.pipeline.scheduler = self.schedulers[scheduler_name]
        
        # è®¾ç½®æ¨ç†æ­¥æ•°
        self.pipeline.scheduler.set_timesteps(num_inference_steps)
        
        # ç¼–ç æ–‡æœ¬æç¤º
        with torch.no_grad():
            text_embeddings = self.pipeline.text_encoder(
                self.pipeline.tokenizer(
                    prompt,
                    max_length=self.pipeline.tokenizer.model_max_length,
                    padding="max_length",
                    truncation=True,
                    return_tensors="pt",
                ).input_ids.to(self.device)
            )[0]
            
            # æ— æ¡ä»¶åµŒå…¥ï¼ˆç”¨äºclassifier-free guidanceï¼‰
            uncond_embeddings = self.pipeline.text_encoder(
                self.pipeline.tokenizer(
                    "",
                    max_length=self.pipeline.tokenizer.model_max_length,
                    padding="max_length",
                    truncation=True,
                    return_tensors="pt",
                ).input_ids.to(self.device)
            )[0]
            
            # åˆå¹¶æ¡ä»¶å’Œæ— æ¡ä»¶åµŒå…¥
            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])
        
        # å»å™ªå¾ªç¯
        latent = noisy_latent
        
        for i, t in enumerate(self.pipeline.scheduler.timesteps):
            # æ‰©å±•æ½œåœ¨è¡¨ç¤ºç”¨äº classifier-free guidance
            latent_model_input = torch.cat([latent] * 2)
            
            # é¢„æµ‹å™ªå£°
            with torch.no_grad():
                noise_pred = self.pipeline.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=text_embeddings
                ).sample
            
            # æ‰§è¡Œ classifier-free guidance
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
            
            # å»å™ªæ­¥éª¤
            latent = self.pipeline.scheduler.step(noise_pred, t, latent).prev_sample
        
        # æ¢å¤åŸå§‹è°ƒåº¦å™¨
        self.pipeline.scheduler = original_scheduler
        
        return latent
    
    def decode_latent_to_audio(self, latent: torch.Tensor) -> torch.Tensor:
        """
        è§£ç æ½œåœ¨è¡¨ç¤ºä¸ºéŸ³é¢‘
        
        Args:
            latent: æ½œåœ¨è¡¨ç¤º
            
        Returns:
            audio: éŸ³é¢‘å¼ é‡
        """
        with torch.no_grad():
            # è§£ç ä¸º mel-spectrogram
            mel = self.pipeline.vae.decode(latent).sample
            
            # è½¬æ¢ä¸ºéŸ³é¢‘
            audio = self.pipeline.mel_spectrogram_extractor.mel_spectrogram_to_waveform(mel)
            
            return audio
    
    def reconstruct_with_diffusion(self, 
                                  input_audio: torch.Tensor,
                                  prompt: str = "high quality music",
                                  noise_level: float = 0.8,
                                  num_inference_steps: int = 20,
                                  guidance_scale: float = 7.5,
                                  scheduler_name: str = "ddim") -> Tuple[torch.Tensor, dict]:
        """
        ä½¿ç”¨å®Œæ•´ diffusion è¿‡ç¨‹é‡å»ºéŸ³é¢‘
        
        Args:
            input_audio: è¾“å…¥éŸ³é¢‘
            prompt: æ–‡æœ¬æç¤º
            noise_level: å™ªå£°æ°´å¹³
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            scheduler_name: è°ƒåº¦å™¨åç§°
            
        Returns:
            reconstructed_audio: é‡å»ºçš„éŸ³é¢‘
            info: å¤„ç†ä¿¡æ¯
        """
        print(f"ğŸµ å¼€å§‹ AudioLDM2 å®Œæ•´ diffusion é‡å»º...")
        print(f"   ğŸ“ æç¤º: {prompt}")
        print(f"   ğŸ”Š å™ªå£°æ°´å¹³: {noise_level}")
        print(f"   ğŸ”„ æ¨ç†æ­¥æ•°: {num_inference_steps}")
        print(f"   ğŸšï¸ å¼•å¯¼å¼ºåº¦: {guidance_scale}")
        print(f"   â° è°ƒåº¦å™¨: {scheduler_name}")
        
        # 1. ç¼–ç éŸ³é¢‘ä¸ºæ½œåœ¨è¡¨ç¤º
        print("   1ï¸âƒ£ ç¼–ç éŸ³é¢‘ä¸ºæ½œåœ¨è¡¨ç¤º...")
        original_latent = self.encode_audio_to_latent(input_audio)
        print(f"      æ½œåœ¨è¡¨ç¤ºå½¢çŠ¶: {original_latent.shape}")
        
        # 2. æ·»åŠ å™ªå£°
        print("   2ï¸âƒ£ æ·»åŠ å™ªå£°...")
        noisy_latent, noise = self.add_noise_to_latent(original_latent, noise_level)
        print(f"      å™ªå£°å¼ºåº¦: {noise.std().item():.4f}")
        
        # 3. Diffusion å»å™ª
        print("   3ï¸âƒ£ Diffusion å»å™ª...")
        denoised_latent = self.denoise_latent(
            noisy_latent, prompt, num_inference_steps, guidance_scale, scheduler_name
        )
        print(f"      å»å™ªå®Œæˆ")
        
        # 4. è§£ç ä¸ºéŸ³é¢‘
        print("   4ï¸âƒ£ è§£ç ä¸ºéŸ³é¢‘...")
        reconstructed_audio = self.decode_latent_to_audio(denoised_latent)
        print(f"      é‡å»ºéŸ³é¢‘å½¢çŠ¶: {reconstructed_audio.shape}")
        
        # å¤„ç†ä¿¡æ¯
        info = {
            "prompt": prompt,
            "noise_level": noise_level,
            "num_inference_steps": num_inference_steps,
            "guidance_scale": guidance_scale,
            "scheduler": scheduler_name,
            "original_latent_shape": original_latent.shape,
            "reconstructed_audio_shape": reconstructed_audio.shape,
            "noise_std": noise.std().item()
        }
        
        print("   âœ… å®Œæ•´ diffusion é‡å»ºå®Œæˆï¼")
        return reconstructed_audio, info
    
    def compare_methods(self, 
                       input_audio: torch.Tensor,
                       output_dir: str = "diffusion_comparison") -> dict:
        """
        å¯¹æ¯”ä¸åŒé‡å»ºæ–¹æ³•
        
        Args:
            input_audio: è¾“å…¥éŸ³é¢‘
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            results: å¯¹æ¯”ç»“æœ
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        results = {}
        
        # 1. VAE-only é‡å»ºï¼ˆæ—  diffusionï¼‰
        print("\nğŸ”„ æ–¹æ³• 1: VAE-only é‡å»ºï¼ˆæ—  diffusionï¼‰")
        latent = self.encode_audio_to_latent(input_audio)
        vae_only_audio = self.decode_latent_to_audio(latent)
        results["vae_only"] = vae_only_audio
        
        # 2. ä½å™ªå£° diffusion é‡å»º
        print("\nğŸ”„ æ–¹æ³• 2: ä½å™ªå£° diffusion é‡å»º")
        low_noise_audio, _ = self.reconstruct_with_diffusion(
            input_audio, 
            prompt="high quality music",
            noise_level=0.3,
            num_inference_steps=10
        )
        results["low_noise_diffusion"] = low_noise_audio
        
        # 3. ä¸­ç­‰å™ªå£° diffusion é‡å»º
        print("\nğŸ”„ æ–¹æ³• 3: ä¸­ç­‰å™ªå£° diffusion é‡å»º")
        medium_noise_audio, _ = self.reconstruct_with_diffusion(
            input_audio,
            prompt="high quality music",
            noise_level=0.6,
            num_inference_steps=20
        )
        results["medium_noise_diffusion"] = medium_noise_audio
        
        # 4. é«˜å™ªå£° diffusion é‡å»º
        print("\nğŸ”„ æ–¹æ³• 4: é«˜å™ªå£° diffusion é‡å»º")
        high_noise_audio, _ = self.reconstruct_with_diffusion(
            input_audio,
            prompt="high quality music",
            noise_level=0.9,
            num_inference_steps=30
        )
        results["high_noise_diffusion"] = high_noise_audio
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        sample_rate = 16000
        for method_name, audio in results.items():
            output_file = output_path / f"{method_name}_output.wav"
            if audio.dim() == 3:
                audio = audio.squeeze(0)
            torchaudio.save(str(output_file), audio.cpu(), sample_rate)
            print(f"   ğŸ’¾ ä¿å­˜: {output_file}")
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘
        original_file = output_path / "original_input.wav"
        if input_audio.dim() == 1:
            input_audio = input_audio.unsqueeze(0)
        torchaudio.save(str(original_file), input_audio.cpu(), sample_rate)
        print(f"   ğŸ’¾ ä¿å­˜åŸå§‹éŸ³é¢‘: {original_file}")
        
        return results

def save_audio_compatible(audio: torch.Tensor, 
                         filepath: str, 
                         sample_rate: int = 16000,
                         normalize: bool = True) -> None:
    """
    å…¼å®¹æ€§éŸ³é¢‘ä¿å­˜å‡½æ•°
    
    Args:
        audio: éŸ³é¢‘å¼ é‡
        filepath: ä¿å­˜è·¯å¾„
        sample_rate: é‡‡æ ·ç‡
        normalize: æ˜¯å¦å½’ä¸€åŒ–
    """
    # ç¡®ä¿éŸ³é¢‘åœ¨ CPU ä¸Š
    if audio.is_cuda:
        audio = audio.cpu()
    
    # å¤„ç†éŸ³é¢‘ç»´åº¦
    if audio.dim() == 3:
        audio = audio.squeeze(0)
    if audio.dim() == 1:
        audio = audio.unsqueeze(0)
    
    # å½’ä¸€åŒ–
    if normalize:
        audio = audio / (audio.abs().max() + 1e-8)
        audio = audio * 0.8  # é˜²æ­¢å‰Šæ³¢
    
    # ä¿å­˜
    torchaudio.save(filepath, audio, sample_rate)

def main():
    """ä¸»å‡½æ•°ï¼šå¤„ç† AudioLDM2_Music_output.wav"""
    
    # è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        return
    
    print(f"ğŸµ å¤„ç†æ–‡ä»¶: {input_file}")
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = torchaudio.load(input_file)
    print(f"   ğŸ“Š åŸå§‹éŸ³é¢‘: {audio.shape}, é‡‡æ ·ç‡: {sr}")
    
    # é‡é‡‡æ ·åˆ° 16kHzï¼ˆAudioLDM2 æ ‡å‡†ï¼‰
    if sr != 16000:
        resampler = torchaudio.transforms.Resample(sr, 16000)
        audio = resampler(audio)
        print(f"   ğŸ”„ é‡é‡‡æ ·åˆ° 16kHz: {audio.shape}")
    
    # è½¬ä¸ºå•å£°é“
    if audio.shape[0] > 1:
        audio = audio.mean(dim=0, keepdim=True)
        print(f"   ğŸ”Š è½¬ä¸ºå•å£°é“: {audio.shape}")
    
    # åˆå§‹åŒ– AudioLDM2 å®Œæ•´ diffusion pipeline
    try:
        reconstructor = AudioLDM2FullDiffusion("cvssp/audioldm2-music")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("   å°è¯•ä½¿ç”¨åŸºç¡€æ¨¡å‹...")
        try:
            reconstructor = AudioLDM2FullDiffusion("cvssp/audioldm2")
        except Exception as e2:
            print(f"âŒ åŸºç¡€æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
            return
    
    # æ‰§è¡Œå¯¹æ¯”æµ‹è¯•
    print("\n" + "="*50)
    print("ğŸ¯ å¼€å§‹å¯¹æ¯”ä¸åŒé‡å»ºæ–¹æ³•")
    print("="*50)
    
    results = reconstructor.compare_methods(audio.squeeze(0))
    
    # è®¡ç®—åŸºç¡€æŒ‡æ ‡
    def calculate_metrics(original: torch.Tensor, reconstructed: torch.Tensor) -> dict:
        """è®¡ç®—é‡å»ºè´¨é‡æŒ‡æ ‡"""
        # ç¡®ä¿å½¢çŠ¶ä¸€è‡´
        min_len = min(original.shape[-1], reconstructed.shape[-1])
        orig = original[..., :min_len]
        recon = reconstructed[..., :min_len]
        
        # è®¡ç®—æŒ‡æ ‡
        mse = torch.mean((orig - recon) ** 2).item()
        snr = 10 * torch.log10(torch.var(orig) / (mse + 1e-8)).item()
        correlation = torch.corrcoef(torch.stack([orig.flatten(), recon.flatten()]))[0, 1].item()
        
        return {
            "mse": mse,
            "snr": snr,
            "correlation": correlation
        }
    
    # åˆ†æç»“æœ
    print("\nğŸ“Š é‡å»ºè´¨é‡åˆ†æ")
    print("-" * 40)
    
    original_audio = audio.squeeze(0)
    for method_name, reconstructed_audio in results.items():
        if reconstructed_audio.dim() > 1:
            reconstructed_audio = reconstructed_audio.squeeze(0)
        
        metrics = calculate_metrics(original_audio, reconstructed_audio)
        
        print(f"\nğŸ” {method_name.replace('_', ' ').title()}:")
        print(f"   SNR: {metrics['snr']:.2f} dB")
        print(f"   ç›¸å…³æ€§: {metrics['correlation']:.4f}")
        print(f"   MSE: {metrics['mse']:.6f}")
    
    print("\nâœ… å®Œæ•´ diffusion é‡å»ºæµ‹è¯•å®Œæˆï¼")
    print("ğŸ“ æŸ¥çœ‹ diffusion_comparison/ ç›®å½•è·å–ç»“æœæ–‡ä»¶")
    
    # é¢å¤–çš„å•ç‹¬æµ‹è¯•
    print("\nğŸ¯ é¢å¤–æµ‹è¯•ï¼šè‡ªå®šä¹‰ diffusion é‡å»º")
    custom_audio, custom_info = reconstructor.reconstruct_with_diffusion(
        audio.squeeze(0),
        prompt="high quality classical music with rich harmonics",
        noise_level=0.7,
        num_inference_steps=25,
        guidance_scale=9.0,
        scheduler_name="ddim"
    )
    
    # ä¿å­˜è‡ªå®šä¹‰ç»“æœ
    save_audio_compatible(custom_audio, "custom_diffusion_output.wav")
    
    print(f"\nğŸ“‹ è‡ªå®šä¹‰é‡å»ºä¿¡æ¯:")
    for key, value in custom_info.items():
        print(f"   {key}: {value}")
    
    print("\nğŸ‰ AudioLDM2 å®Œæ•´ diffusion é‡å»ºæµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()
