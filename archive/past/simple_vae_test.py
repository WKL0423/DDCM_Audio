"""
AudioLDM2 VAE å¿«é€Ÿé‡å»ºæµ‹è¯•è„šæœ¬
ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“æ³¨äºæ ¸å¿ƒ VAE æµ‹è¯•åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•:
python simple_vae_test.py [éŸ³é¢‘æ–‡ä»¶è·¯å¾„] [å¯é€‰ï¼šéŸ³é¢‘é•¿åº¦ç§’æ•°]

ç¤ºä¾‹:
python simple_vae_test.py techno.wav 5
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F
import soundfile as sf

from diffusers import AudioLDM2Pipeline


def load_and_test_vae(audio_path, model_id="cvssp/audioldm2-music", max_length=10):
    """
    åŠ è½½éŸ³é¢‘æ–‡ä»¶å¹¶æµ‹è¯• VAE é‡å»º
    
    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        model_id: AudioLDM2-Music æ¨¡å‹ID
        max_length: æœ€å¤§éŸ³é¢‘é•¿åº¦ï¼ˆç§’ï¼‰
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(audio_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ {audio_path}")
        return
    
    print(f"æ­£åœ¨åŠ è½½ AudioLDM2 æ¨¡å‹: {model_id}")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    print(f"æ­£åœ¨åŠ è½½éŸ³é¢‘: {audio_path}")
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    
    # é™åˆ¶éŸ³é¢‘é•¿åº¦
    if len(audio) > max_length * sample_rate:
        audio = audio[:int(max_length * sample_rate)]
        print(f"éŸ³é¢‘å·²è£å‰ªåˆ° {max_length} ç§’")
    
    print(f"éŸ³é¢‘ä¿¡æ¯: é•¿åº¦={len(audio)/sample_rate:.2f}ç§’, æ ·æœ¬æ•°={len(audio)}")
    
    # å°†éŸ³é¢‘è½¬æ¢ä¸º mel-spectrogram
    print("è½¬æ¢éŸ³é¢‘ä¸º mel-spectrogram...")
    
    # ä½¿ç”¨ vocoder å°†éŸ³é¢‘è½¬æ¢ä¸º mel-spectrogram
    audio_tensor = torch.from_numpy(audio).unsqueeze(0).to(device)
    
    with torch.no_grad():
        # ä½¿ç”¨ vocoder å°†éŸ³é¢‘è½¬æ¢ä¸º mel-spectrogram
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ vocoder çš„é€†è¿‡ç¨‹æ¥è·å¾— mel-spectrogram
        # ä½†æ˜¯ vocoder é€šå¸¸åªæœ‰ mel->audio çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦ç”¨ä¸åŒçš„æ–¹æ³•
        
        # è®¡ç®— mel-spectrogramï¼ˆä½¿ç”¨ librosaï¼‰
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sample_rate,
            n_mels=64,  # AudioLDM2 ä½¿ç”¨ 64 mel bins
            hop_length=160,  # AudioLDM2 hop length
            n_fft=1024,
            fmin=0,
            fmax=8000
        )
          # è½¬æ¢ä¸ºå¯¹æ•°å°ºåº¦
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        # å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„æ–¹æ³•
        mel_min = mel_spec_db.min()
        mel_max = mel_spec_db.max()
        mel_spec_normalized = 2 * (mel_spec_db - mel_min) / (mel_max - mel_min) - 1
        mel_spec_normalized = np.clip(mel_spec_normalized, -1, 1)
        
        # ä¿å­˜å½’ä¸€åŒ–å‚æ•°ä»¥ä¾¿åç»­åå½’ä¸€åŒ–
        norm_params = {'min_val': mel_min, 'max_val': mel_max}
          # è½¬æ¢ä¸ºå¼ é‡å¹¶è°ƒæ•´å½¢çŠ¶
        mel_tensor = torch.from_numpy(mel_spec_normalized).to(device)
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸æ¨¡å‹ä¸€è‡´
        if device == "cuda":
            mel_tensor = mel_tensor.to(torch.float16)
        else:
            mel_tensor = mel_tensor.to(torch.float32)
        
        # è°ƒæ•´ä¸º VAE æœŸæœ›çš„å½¢çŠ¶: (batch, channels, height, width)
        # mel_tensor å½¢çŠ¶: (n_mels, time_frames) -> (1, 1, n_mels, time_frames)
        mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)
        
        print(f"Mel-spectrogram å½¢çŠ¶: {mel_input.shape}")
    
    # VAE ç¼–ç -è§£ç æµ‹è¯•
    print("å¼€å§‹ VAE ç¼–ç ...")
    start_time = time.time()
    
    with torch.no_grad():
        # ç¼–ç 
        try:
            latents = vae.encode(mel_input).latent_dist.sample()
            latents = latents * vae.config.scaling_factor
            
            encode_time = time.time() - start_time
            print(f"ç¼–ç å®Œæˆ: {encode_time:.2f}ç§’, æ½œåœ¨å½¢çŠ¶: {latents.shape}")
            
            # è§£ç 
            print("å¼€å§‹ VAE è§£ç ...")
            decode_start = time.time()
            
            latents = latents / vae.config.scaling_factor
            reconstructed_mel = vae.decode(latents).sample
            
            decode_time = time.time() - decode_start
            print(f"è§£ç å®Œæˆ: {decode_time:.2f}ç§’, é‡å»º mel å½¢çŠ¶: {reconstructed_mel.shape}")
            
        except Exception as e:
            print(f"VAE ç¼–ç /è§£ç è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            print("å°è¯•ä½¿ç”¨ä¸åŒçš„è¾“å…¥æ ¼å¼...")
            
            # å°è¯•ä¸åŒçš„è¾“å…¥æ ¼å¼
            # å¯èƒ½éœ€è¦è°ƒæ•´ mel-spectrogram çš„å°ºå¯¸
            target_height = 64
            target_width = mel_input.shape[-1]
            
            # ç¡®ä¿å°ºå¯¸æ˜¯ VAE è¦æ±‚çš„å€æ•°
            pad_width = (8 - (target_width % 8)) % 8
            if pad_width > 0:
                mel_input = F.pad(mel_input, (0, pad_width))
                target_width = mel_input.shape[-1]
            
            print(f"è°ƒæ•´åçš„ mel è¾“å…¥å½¢çŠ¶: {mel_input.shape}")
            
            # é‡è¯•ç¼–ç 
            latents = vae.encode(mel_input).latent_dist.sample()
            latents = latents * vae.config.scaling_factor
            
            encode_time = time.time() - start_time
            print(f"ç¼–ç å®Œæˆ: {encode_time:.2f}ç§’, æ½œåœ¨å½¢çŠ¶: {latents.shape}")
            
            # è§£ç 
            print("å¼€å§‹ VAE è§£ç ...")
            decode_start = time.time()
            
            latents = latents / vae.config.scaling_factor
            reconstructed_mel = vae.decode(latents).sample
            
            decode_time = time.time() - decode_start
            print(f"è§£ç å®Œæˆ: {decode_time:.2f}ç§’, é‡å»º mel å½¢çŠ¶: {reconstructed_mel.shape}")    # å°†é‡å»ºçš„ mel-spectrogram è½¬æ¢å›éŸ³é¢‘
    print("å°†é‡å»ºçš„ mel-spectrogram è½¬æ¢å›éŸ³é¢‘...")
    
    with torch.no_grad():
        # æ–¹æ³•1: ä½¿ç”¨AudioLDM2å†…ç½®çš„HiFiGAN (æ¨è)
        print("ğŸ¤ å°è¯•ä½¿ç”¨AudioLDM2å†…ç½®HiFiGAN...")
        try:
            # å‡†å¤‡AudioLDM2 HiFiGANçš„è¾“å…¥æ ¼å¼: [batch, time, mel_dim]
            vocoder_input = reconstructed_mel.squeeze(0)  # [1, 64, time] -> [64, time]
            vocoder_input = vocoder_input.transpose(-2, -1)  # [64, time] -> [time, 64]
            vocoder_input = vocoder_input.unsqueeze(0)  # [time, 64] -> [1, time, 64]
            
            print(f"   HiFiGANè¾“å…¥æ ¼å¼: {vocoder_input.shape}")
            
            # ä½¿ç”¨AudioLDM2å†…ç½®HiFiGAN
            reconstructed_audio_tensor = vocoder(vocoder_input)
            reconstructed_audio = reconstructed_audio_tensor.squeeze().cpu().numpy()
            
            print(f"   âœ… AudioLDM2 HiFiGANæˆåŠŸ: è¾“å‡º{len(reconstructed_audio)}æ ·æœ¬")
            vocoder_method = "AudioLDM2_HiFiGAN"
            
        except Exception as e:
            print(f"   âŒ AudioLDM2 HiFiGANå¤±è´¥: {e}")
            print("ğŸ“Š é™çº§åˆ°Griffin-Limç®—æ³•...")
            
            # æ–¹æ³•2: Griffin-Limç®—æ³• (é™çº§æ–¹æ¡ˆ)
            # åå½’ä¸€åŒ– mel-spectrogram
            recon_mel_np = reconstructed_mel.squeeze().cpu().numpy().astype(np.float32)
            
            # ä½¿ç”¨ä¿å­˜çš„å½’ä¸€åŒ–å‚æ•°è¿›è¡Œåå½’ä¸€åŒ–
            recon_mel_denorm = (recon_mel_np + 1) / 2 * (norm_params['max_val'] - norm_params['min_val']) + norm_params['min_val']
            
            # ç¡®ä¿æ²¡æœ‰æ— æ•ˆå€¼
            recon_mel_denorm = np.nan_to_num(recon_mel_denorm, nan=-80.0, posinf=-20.0, neginf=-80.0)
            recon_mel_denorm = np.clip(recon_mel_denorm, -80.0, 0.0)
            
            # è½¬æ¢å›åŠŸç‡è°±
            recon_mel_power = librosa.db_to_power(recon_mel_denorm)
            
            # ç¡®ä¿åŠŸç‡è°±å€¼åˆç†
            recon_mel_power = np.clip(recon_mel_power, 1e-10, None)
            
            # ä½¿ç”¨ Griffin-Lim ç®—æ³•é‡å»ºéŸ³é¢‘
            try:
                reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                    recon_mel_power,
                    sr=sample_rate,
                    hop_length=160,
                    n_fft=1024,
                    fmin=0,
                    fmax=8000
                )
                vocoder_method = "Griffin_Lim"
                print(f"   âœ… Griffin-LimæˆåŠŸ: è¾“å‡º{len(reconstructed_audio)}æ ·æœ¬")
            except Exception as e:
                print(f"   âŒ Griffin-Limå¤±è´¥: {e}")
                print("   ğŸ”„ ä½¿ç”¨éšæœºéŸ³é¢‘ä½œä¸ºå ä½ç¬¦...")
                
                # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨å ä½ç¬¦
                reconstructed_audio = np.random.randn(len(audio)) * 0.1
                vocoder_method = "Fallback_Noise"
        
        # ç¡®ä¿éŸ³é¢‘å€¼åˆç†
        if len(reconstructed_audio) > 0:
            reconstructed_audio = np.nan_to_num(reconstructed_audio, nan=0.0, posinf=0.0, neginf=0.0)
        else:
            reconstructed_audio = np.zeros_like(audio)
    
    # ä¿å­˜ç»“æœ
    output_dir = "vae_quick_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘ï¼ˆå¤„ç†åçš„ç‰ˆæœ¬ï¼‰
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    audio_save = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio
    save_audio_compatible(audio_save, original_path, sample_rate)
    
    # ä¿å­˜é‡å»ºéŸ³é¢‘
    reconstructed_path = os.path.join(output_dir, f"{input_name}_reconstructed_{timestamp}.wav")
    recon_audio_norm = reconstructed_audio / np.max(np.abs(reconstructed_audio)) if np.max(np.abs(reconstructed_audio)) > 0 else reconstructed_audio
    
    # ç¡®ä¿é‡å»ºéŸ³é¢‘é•¿åº¦ä¸åŸå§‹éŸ³é¢‘ä¸€è‡´
    if len(recon_audio_norm) > len(audio):
        recon_audio_norm = recon_audio_norm[:len(audio)]
    elif len(recon_audio_norm) < len(audio):
        recon_audio_norm = np.pad(recon_audio_norm, (0, len(audio) - len(recon_audio_norm)))
    
    save_audio_compatible(recon_audio_norm, reconstructed_path, sample_rate)
    
    # è®¡ç®—ç®€å•æŒ‡æ ‡
    min_len = min(len(audio), len(recon_audio_norm))
    orig_flat = audio[:min_len]
    recon_flat = recon_audio_norm[:min_len]
    
    # è®¡ç®— MSE å’Œç›¸å…³ç³»æ•°
    mse = np.mean((orig_flat - recon_flat) ** 2)
    correlation = np.corrcoef(orig_flat, recon_flat)[0, 1] if len(orig_flat) > 1 else 0
    
    # è®¡ç®— SNR
    signal_power = np.mean(orig_flat ** 2)
    noise_power = np.mean((orig_flat - recon_flat) ** 2)
    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
      # è®¡ç®—å‹ç¼©æ¯”
    original_size = mel_input.numel()
    compressed_size = latents.numel()
    compression_ratio = original_size / compressed_size
    
    print(f"\n{'='*50}")
    print(f"VAE é‡å»ºæµ‹è¯•ç»“æœ")
    print(f"{'='*50}")
    print(f"åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ç¼–ç æ—¶é—´: {encode_time:.2f}ç§’")
    print(f"è§£ç æ—¶é—´: {decode_time:.2f}ç§’")
    print(f"æ€»æ—¶é—´: {encode_time + decode_time:.2f}ç§’")
    print(f"MSE: {mse:.6f}")
    print(f"SNR: {snr:.2f} dB")
    print(f"ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"Mel-spectrogram å½¢çŠ¶: {mel_input.shape}")
    print(f"æ½œåœ¨è¡¨ç¤ºå½¢çŠ¶: {latents.shape}")
    print(f"å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
    print(f"é‡å»ºæ–¹æ³•: {vocoder_method}")
    
    return {
        'original_path': original_path,
        'reconstructed_path': reconstructed_path,
        'mse': mse,
        'snr': snr,
        'correlation': correlation,
        'encode_time': encode_time,
        'decode_time': decode_time,
        'compression_ratio': compression_ratio
    }


def save_audio_compatible(audio_data, file_path, sample_rate=16000):
    """
    ä½¿ç”¨æœ€ä½³å…¼å®¹æ€§ä¿å­˜éŸ³é¢‘æ–‡ä»¶
    ç¡®ä¿ç”Ÿæˆçš„WAVæ–‡ä»¶å¯ä»¥è¢«å„ç§æ’­æ”¾å™¨æ­£ç¡®æ’­æ”¾
    """
    # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯numpy array
    if isinstance(audio_data, torch.Tensor):
        audio_np = audio_data.detach().cpu().numpy()
    else:
        audio_np = np.array(audio_data)
    
    # ç¡®ä¿æ˜¯1Dæ•°ç»„
    if audio_np.ndim > 1:
        audio_np = audio_np.flatten()
    
    # ç§»é™¤ä»»ä½•æ— æ•ˆå€¼
    audio_np = np.nan_to_num(audio_np, nan=0.0, posinf=0.0, neginf=0.0)
    
    # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†… [-1, 1]
    max_val = np.max(np.abs(audio_np))
    if max_val > 1.0:
        audio_np = audio_np / max_val * 0.95
    elif max_val > 0 and max_val < 0.01:  # å¤ªå®‰é™çš„è¯æ”¾å¤§ä¸€ç‚¹
        audio_np = audio_np / max_val * 0.5
    
    # è½¬æ¢ä¸ºfloat32ç¡®ä¿ç²¾åº¦
    audio_np = audio_np.astype(np.float32)
    
    print(f"   ä¿å­˜éŸ³é¢‘: {file_path}")
    print(f"   éŸ³é¢‘é•¿åº¦: {len(audio_np)} æ ·æœ¬ ({len(audio_np)/sample_rate:.2f} ç§’)")
    print(f"   æ•°å€¼èŒƒå›´: [{np.min(audio_np):.6f}, {np.max(audio_np):.6f}]")
    
    try:
        # æ–¹æ³•1: ä½¿ç”¨soundfileï¼ŒPCM_16æ ¼å¼ (æœ€é€šç”¨)
        sf.write(file_path, audio_np, sample_rate, subtype='PCM_16')
        print(f"   âœ… soundfileä¿å­˜æˆåŠŸ (PCM_16)")
        
        # éªŒè¯æ–‡ä»¶
        test_audio, test_sr = sf.read(file_path)
        print(f"   âœ… éªŒè¯æˆåŠŸ: é•¿åº¦={len(test_audio)}, é‡‡æ ·ç‡={test_sr}")
        return True
        
    except Exception as e:
        print(f"   âŒ soundfileå¤±è´¥: {e}")
        
        # æ–¹æ³•2: é™çº§åˆ°torchaudio
        try:
            audio_tensor = torch.from_numpy(audio_np).unsqueeze(0)
            torchaudio.save(file_path, audio_tensor, sample_rate)
            print(f"   âœ… torchaudioä¿å­˜æˆåŠŸ")
            return True
        except Exception as e2:
            print(f"   âŒ torchaudioä¹Ÿå¤±è´¥: {e2}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python simple_vae_test.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„> [æœ€å¤§é•¿åº¦ç§’æ•°]")
        
        # æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„éŸ³é¢‘æ–‡ä»¶
        audio_files = []
        for ext in ['.wav', '.mp3', '.flac']:
            audio_files.extend(Path('.').glob(f'*{ext}'))
        
        if audio_files:
            print(f"\næ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files, 1):
                print(f"{i}. {file}")
            
            try:
                choice = input("è¯·é€‰æ‹©æ–‡ä»¶åºå·: ").strip()
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(audio_files):
                    audio_path = str(audio_files[file_idx])
                else:
                    print("æ— æ•ˆé€‰æ‹©")
                    return
            except (ValueError, KeyboardInterrupt):
                print("å–æ¶ˆæ“ä½œ")
                return
        else:
            print("å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    # è·å–æœ€å¤§é•¿åº¦å‚æ•°
    max_length = 10  # é»˜è®¤10ç§’
    if len(sys.argv) >= 3:
        try:
            max_length = float(sys.argv[2])
        except ValueError:
            print(f"æ— æ•ˆçš„é•¿åº¦å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {max_length} ç§’")
    
    # æ‰§è¡Œæµ‹è¯•
    print(f"å¼€å§‹æµ‹è¯•éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    print(f"æœ€å¤§é•¿åº¦: {max_length} ç§’")
    
    try:
        result = load_and_test_vae(audio_path, max_length=max_length)
        if result:
            print("\næµ‹è¯•å®Œæˆï¼")
            print("å¯ä»¥æ’­æ”¾åŸå§‹éŸ³é¢‘å’Œé‡å»ºéŸ³é¢‘æ¥æ¯”è¾ƒè´¨é‡ã€‚")
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
