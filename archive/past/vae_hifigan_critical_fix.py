#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
AudioLDM2 VAE+HiFiGAN å…³é”®ä¿®å¤ç‰ˆæœ¬
==============================

è§£å†³çš„æ ¸å¿ƒé—®é¢˜ï¼š
1. HiFiGANè¾“å…¥ç»´åº¦é”™è¯¯ (time vs channels)
2. VAEç¼–ç è§£ç å°ºå¯¸ä¸åŒ¹é…
3. æ­£ç¡®çš„melé¢‘è°±é¢„å¤„ç†å’Œscaling
4. æ•°æ®ç±»å‹å’Œè®¾å¤‡åŒ¹é…

ä½œè€…: åŸºäºAudioLDM2åˆ†æçš„ä¿®å¤
"""

import os
import torch
import numpy as np
import librosa
import soundfile as sf
from pathlib import Path
from diffusers import AudioLDM2Pipeline
import time
from typing import Union, Optional


def save_audio_compatible(audio: np.ndarray, filepath: str, sample_rate: int = 16000):
    """å…¼å®¹æ€§éŸ³é¢‘ä¿å­˜å‡½æ•°"""
    try:
        # ç¡®ä¿éŸ³é¢‘åœ¨æœ‰æ•ˆèŒƒå›´å†…
        audio = np.clip(audio, -1.0, 1.0)
        
        # ä¿å­˜ä¸ºPCM 16bit WAV
        sf.write(filepath, audio, sample_rate, subtype='PCM_16')
        print(f"âœ… éŸ³é¢‘ä¿å­˜æˆåŠŸ: {filepath}")
        
    except Exception as e:
        print(f"âŒ éŸ³é¢‘ä¿å­˜å¤±è´¥: {e}")


def test_vae_hifigan_critical_fix(audio_path: str, max_length: float = 5.0):
    """
    VAE+HiFiGANå…³é”®ä¿®å¤æµ‹è¯•
    
    Args:
        audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
        max_length: æœ€å¤§éŸ³é¢‘é•¿åº¦ï¼ˆç§’ï¼‰
    """
    print(f"\nğŸš€ AudioLDM2 VAE+HiFiGAN å…³é”®ä¿®å¤æµ‹è¯•")
    print(f"ğŸ¯ ç›®æ ‡: è§£å†³ç»´åº¦é”™è¯¯å’Œå™ªå£°é—®é¢˜")
    
    # è®¾å¤‡è®¾ç½®
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2-music",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   VAE scaling_factor: {vae.config.scaling_factor}")
    print(f"   Vocoderç±»å‹: {type(vocoder).__name__}")
    
    # åŠ è½½éŸ³é¢‘
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘: {Path(audio_path).name}")
    audio, sr = librosa.load(audio_path, sr=sample_rate, duration=max_length)
    print(f"   éŸ³é¢‘: {len(audio)/sr:.2f}ç§’, èŒƒå›´[{audio.min():.3f}, {audio.max():.3f}]")
    
    # åˆ›å»ºmelé¢‘è°± - å…³é”®ä¿®å¤1: æ­£ç¡®çš„å‚æ•°
    print("\nğŸµ åˆ›å»ºmelé¢‘è°±...")
    mel_spec = librosa.feature.melspectrogram(
        y=audio, 
        sr=sample_rate, 
        n_mels=64,           # AudioLDM2æ ‡å‡†
        hop_length=160,      # 10ms hop
        n_fft=1024,          # æ ‡å‡†FFT size
        win_length=1024,     # çª—å£é•¿åº¦
        window='hann',       # çª—å£ç±»å‹
        center=True,         # ä¸­å¿ƒåŒ–
        pad_mode='reflect'   # å¡«å……æ¨¡å¼
    )
    
    # mel to db
    mel_db = librosa.power_to_db(mel_spec, ref=np.max)
    print(f"   Melå½¢çŠ¶: {mel_db.shape}")
    print(f"   MelèŒƒå›´: [{mel_db.min():.1f}, {mel_db.max():.1f}] dB")
    
    # å…³é”®ä¿®å¤2: æ­£ç¡®çš„å½’ä¸€åŒ–ï¼ˆå‚è€ƒdiffusersæºç ï¼‰
    # AudioLDM2æœŸæœ›çš„melè°±èŒƒå›´å¤§çº¦æ˜¯ [-5, 5]
    mel_normalized = mel_db / 20.0  # ç¼©æ”¾åˆ°åˆç†èŒƒå›´
    mel_normalized = np.clip(mel_normalized, -1, 1)  # è£å‰ªåˆ°[-1, 1]
    
    print(f"   å½’ä¸€åŒ–å: [{mel_normalized.min():.3f}, {mel_normalized.max():.3f}]")
    
    # è½¬æ¢ä¸ºtensor
    mel_tensor = torch.from_numpy(mel_normalized).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.half()
    
    # å…³é”®ä¿®å¤3: æ­£ç¡®çš„è¾“å…¥ç»´åº¦ [batch, channels, height, width]
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, n_mels, time]
    print(f"   è¾“å…¥ç»´åº¦: {mel_input.shape}, ç±»å‹: {mel_input.dtype}")
    
    # VAEç¼–ç è§£ç 
    print("\nğŸ§  VAEç¼–ç è§£ç ...")
    with torch.no_grad():
        # ç¼–ç 
        latent_dist = vae.encode(mel_input)
        if hasattr(latent_dist, 'latent_dist'):
            latent = latent_dist.latent_dist.sample()
        else:
            latent = latent_dist.sample()
        
        # åº”ç”¨scaling factor
        latent = latent * vae.config.scaling_factor
        print(f"   Latent: {latent.shape}, èŒƒå›´[{latent.min():.3f}, {latent.max():.3f}]")
        
        # è§£ç 
        latent_for_decode = latent / vae.config.scaling_factor
        reconstructed_mel = vae.decode(latent_for_decode).sample
        
        print(f"   é‡å»º: {reconstructed_mel.shape}, èŒƒå›´[{reconstructed_mel.min():.3f}, {reconstructed_mel.max():.3f}]")
      # å…³é”®ä¿®å¤4: æ­£ç¡®çš„HiFiGANè¾“å…¥æ ¼å¼
    print("\nğŸ¤ HiFiGAN vocoder...")
    
    # ä» [batch, channels, height, width] åˆ° [batch, height, width]
    vocoder_input = reconstructed_mel.squeeze(1)  # ç§»é™¤channelç»´åº¦: [1, 64, 500]
    print(f"   æ­¥éª¤1 - ç§»é™¤channel: {vocoder_input.shape}")
      # å…³é”®ä¿®å¤: AudioLDM2çš„HiFiGANæœŸæœ› [batch, time, n_mels] æ ¼å¼
    vocoder_input = vocoder_input.transpose(1, 2)  # [1, 64, 500] -> [1, 500, 64]
    print(f"   æ­¥éª¤2 - è½¬ç½®ä¸º[batch, time, n_mels]: {vocoder_input.shape}")
    
    # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…vocoderçš„æƒé‡ç±»å‹
    vocoder_dtype = next(vocoder.parameters()).dtype
    vocoder_input = vocoder_input.to(vocoder_dtype)
    print(f"   æ•°æ®ç±»å‹: {vocoder_input.dtype}")
      # å°è¯•HiFiGAN
    try:
        print("   ğŸš€ è°ƒç”¨HiFiGAN...")
        waveform = vocoder(vocoder_input)
        reconstructed_audio = waveform.squeeze().detach().cpu().float().numpy()
        vocoder_method = "HiFiGAN_SUCCESS"
        
        print(f"   âœ… HiFiGANæˆåŠŸï¼")
        print(f"   è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
        print(f"   èŒƒå›´: [{reconstructed_audio.min():.3f}, {reconstructed_audio.max():.3f}]")
        
    except Exception as e:
        print(f"   âŒ HiFiGANå¤±è´¥: {e}")
        print(f"   è¾“å…¥æœŸæœ›: [batch, time, n_mels]")
        print(f"   å®é™…è¾“å…¥: {vocoder_input.shape}")
        
        # Griffin-Limå¤‡é€‰æ–¹æ¡ˆ - å…³é”®ä¿®å¤5: å°ºå¯¸å¯¹é½
        print("   ğŸ”„ ä½¿ç”¨Griffin-Lim...")
        
        # ç¡®ä¿å°ºå¯¸ä¸€è‡´ï¼Œå¹¶è½¬ä¸ºfloat32
        mel_for_griffin = reconstructed_mel.squeeze().cpu().float().numpy()
        
        # å¦‚æœéœ€è¦ï¼Œè°ƒæ•´å°ºå¯¸åˆ°åŸå§‹è¾“å…¥
        if mel_for_griffin.shape[-1] != mel_db.shape[-1]:
            print(f"   è°ƒæ•´å°ºå¯¸: {mel_for_griffin.shape[-1]} -> {mel_db.shape[-1]}")
            if mel_for_griffin.shape[-1] > mel_db.shape[-1]:
                mel_for_griffin = mel_for_griffin[:, :mel_db.shape[-1]]
            else:
                pad_width = mel_db.shape[-1] - mel_for_griffin.shape[-1]
                mel_for_griffin = np.pad(mel_for_griffin, ((0, 0), (0, pad_width)), mode='constant')
        
        # åå½’ä¸€åŒ–
        mel_denorm = mel_for_griffin * 20.0  # åå‘ç¼©æ”¾
        mel_power = librosa.db_to_power(mel_denorm)
        
        # Griffin-Limé‡å»º
        reconstructed_audio = librosa.feature.inverse.mel_to_audio(
            mel_power, 
            sr=sample_rate, 
            hop_length=160, 
            n_fft=1024,
            win_length=1024,
            window='hann',
            center=True,
            pad_mode='reflect'
        )
        
        vocoder_method = "Griffin_Lim_Fixed"
        print(f"   âœ… Griffin-LimæˆåŠŸ: {len(reconstructed_audio)}æ ·æœ¬")
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    print("\nğŸ“Š è´¨é‡è¯„ä¼°...")
    
    # è°ƒæ•´é•¿åº¦åŒ¹é…
    min_len = min(len(audio), len(reconstructed_audio))
    audio_aligned = audio[:min_len]
    recon_aligned = reconstructed_audio[:min_len]
    
    # è®¡ç®—æŒ‡æ ‡
    mse = np.mean((audio_aligned - recon_aligned) ** 2)
    correlation = np.corrcoef(audio_aligned, recon_aligned)[0, 1]
    
    # SNRè®¡ç®—
    signal_power = np.mean(audio_aligned ** 2)
    noise_power = np.mean((audio_aligned - recon_aligned) ** 2)
    snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
    
    print(f"   MSE: {mse:.6f}")
    print(f"   ç›¸å…³æ€§: {correlation:.4f}")
    print(f"   SNR: {snr:.2f} dB")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("vae_hifigan_critical_fix")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
    reconstructed_path = output_dir / f"{input_name}_{vocoder_method}_{timestamp}.wav"
    
    save_audio_compatible(audio_aligned, str(original_path))
    save_audio_compatible(reconstructed_audio, str(reconstructed_path))
    
    # åˆ›å»ºç»“æœæŠ¥å‘Š
    report = f"""
AudioLDM2 VAE+HiFiGAN å…³é”®ä¿®å¤æŠ¥å‘Š
================================

è¾“å…¥æ–‡ä»¶: {Path(audio_path).name}
å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
Vocoderæ–¹æ³•: {vocoder_method}

éŸ³é¢‘ä¿¡æ¯:
- åŸå§‹é•¿åº¦: {len(audio)/sr:.2f}ç§’
- é‡å»ºé•¿åº¦: {len(reconstructed_audio)/sr:.2f}ç§’
- é‡‡æ ·ç‡: {sample_rate} Hz

è´¨é‡æŒ‡æ ‡:
- MSE: {mse:.6f}
- ç›¸å…³æ€§: {correlation:.4f}
- SNR: {snr:.2f} dB

æŠ€æœ¯ç»†èŠ‚:
- Melé¢‘è°±: {mel_db.shape}
- VAEè¾“å…¥: {mel_input.shape}
- VAEè¾“å‡º: {reconstructed_mel.shape}
- Vocoderè¾“å…¥: {vocoder_input.shape if 'vocoder_input' in locals() else 'N/A'}

ä¿®å¤è¦ç‚¹:
1. æ­£ç¡®çš„melé¢‘è°±å‚æ•°å’Œå½’ä¸€åŒ–
2. æ­£ç¡®çš„VAE scaling factoråº”ç”¨
3. æ­£ç¡®çš„HiFiGANè¾“å…¥ç»´åº¦ï¼š[batch, n_mels, time]
4. å°ºå¯¸å¯¹é½å’Œæ•°æ®ç±»å‹åŒ¹é…
5. Griffin-Limå¤‡é€‰æ–¹æ¡ˆçš„å°ºå¯¸ä¿®å¤
"""
    
    report_path = output_dir / f"report_{input_name}_{timestamp}.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“„ æŠ¥å‘Šä¿å­˜: {report_path}")
    print(f"ğŸµ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸµ é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    
    return {
        'mse': mse,
        'correlation': correlation,
        'snr': snr,
        'vocoder_method': vocoder_method,
        'original_path': original_path,
        'reconstructed_path': reconstructed_path
    }


def main():
    """ä¸»å‡½æ•°ï¼šé€‰æ‹©éŸ³é¢‘æ–‡ä»¶å¹¶æµ‹è¯•"""
    # æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
    audio_files = []
    for ext in ['*.wav', '*.mp3', '*.flac']:
        audio_files.extend(Path('.').glob(ext))
    
    if not audio_files:
        print("âŒ å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
        return
    
    print("æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
    for i, file in enumerate(audio_files, 1):
        print(f"{i}. {file.name}")
    
    try:
        choice = int(input("é€‰æ‹©æ–‡ä»¶:"))
        audio_path = str(audio_files[choice - 1])
        
        # è¿è¡Œæµ‹è¯•
        result = test_vae_hifigan_critical_fix(audio_path)
        
        print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“ˆ æœ€ç»ˆç»“æœ: {result['vocoder_method']}")
        print(f"ğŸ“Š è´¨é‡: MSE={result['mse']:.6f}, SNR={result['snr']:.2f}dB")
        
    except (ValueError, IndexError):
        print("âŒ æ— æ•ˆé€‰æ‹©")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
