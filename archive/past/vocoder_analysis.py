"""
AudioLDM2 Vocoder æ·±åº¦åˆ†æ
=========================

åˆ†æAudioLDM2å†…ç½®vocoderçš„ç»“æ„å’ŒHiFiGANé›†æˆé—®é¢˜
"""

import torch
from diffusers import AudioLDM2Pipeline
from transformers import SpeechT5HifiGan
import librosa
import numpy as np

def analyze_audioldm2_vocoder():
    """åˆ†æAudioLDM2å†…ç½®vocoder"""
    print("ğŸ” AudioLDM2 Vocoder æ·±åº¦åˆ†æ")
    print("="*50)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ è½½AudioLDM2
    print("ğŸ“¦ åŠ è½½AudioLDM2...")
    pipe = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2-music",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    # åˆ†ævocoder
    vocoder = pipe.vocoder
    print(f"\nğŸ¤ AudioLDM2å†…ç½®Vocoderåˆ†æ:")
    print(f"   ç±»å‹: {type(vocoder)}")
    print(f"   æ¨¡å—: {vocoder.__module__}")
    
    # æ£€æŸ¥é…ç½®
    if hasattr(vocoder, 'config'):
        config = vocoder.config
        print(f"   é…ç½®ç±»å‹: {type(config)}")
        print(f"   é…ç½®å†…å®¹: {config}")
    else:
        print("   âŒ æ²¡æœ‰configå±æ€§")
    
    # æ£€æŸ¥æ¨¡å‹ç»“æ„
    print(f"\nğŸ”§ Vocoderæ¨¡å‹ç»“æ„:")
    for name, module in vocoder.named_children():
        print(f"   {name}: {type(module)}")
    
    # æ£€æŸ¥å‚æ•°
    total_params = sum(p.numel() for p in vocoder.parameters())
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    
    # æ£€æŸ¥è¾“å…¥è¾“å‡ºæœŸæœ›
    print(f"\nğŸ“Š è¾“å…¥è¾“å‡ºåˆ†æ:")
    
    # åˆ›å»ºæµ‹è¯•melæ•°æ®
    test_mel = torch.randn(1, 64, 100).to(device)  # [batch, mel_bins, time]
    if device == "cuda":
        test_mel = test_mel.half()
    
    try:
        print(f"   æµ‹è¯•è¾“å…¥: {test_mel.shape} ({test_mel.dtype})")
        with torch.no_grad():
            output = vocoder(test_mel)
        print(f"   âœ… è¾“å‡ºæˆåŠŸ: {output.shape} ({output.dtype})")
        print(f"   è¾“å…¥æ ¼å¼: [batch, mel_bins, time_frames]")
        
    except Exception as e:
        print(f"   âŒ ç›´æ¥è°ƒç”¨å¤±è´¥: {e}")
        
        # å°è¯•ä¸åŒçš„è¾“å…¥æ ¼å¼
        formats_to_try = [
            ("transpose", test_mel.transpose(-2, -1)),  # [batch, time, mel_bins]
            ("squeeze", test_mel.squeeze(0)),  # [mel_bins, time]
            ("unsqueeze", test_mel.unsqueeze(1)),  # [batch, channels, mel_bins, time]
        ]
        
        for name, test_input in formats_to_try:
            try:
                print(f"   å°è¯•æ ¼å¼ {name}: {test_input.shape}")
                with torch.no_grad():
                    output = vocoder(test_input)
                print(f"   âœ… {name}æ ¼å¼æˆåŠŸ: {output.shape}")
                break
            except Exception as e:
                print(f"   âŒ {name}æ ¼å¼å¤±è´¥: {e}")
    
    return vocoder

def analyze_external_hifigan():
    """åˆ†æå¤–éƒ¨HiFiGAN"""
    print(f"\nğŸ¤ å¤–éƒ¨HiFiGANåˆ†æ:")
    print("="*30)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    try:
        # åŠ è½½å¤–éƒ¨HiFiGAN
        hifigan = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan").to(device)
        
        print(f"   ç±»å‹: {type(hifigan)}")
        print(f"   æ¨¡å—: {hifigan.__module__}")
        
        if hasattr(hifigan, 'config'):
            print(f"   é…ç½®: {hifigan.config}")
        
        # æµ‹è¯•è¾“å…¥
        test_mel = torch.randn(1, 80, 100).to(device)  # SpeechT5æ ‡å‡†ï¼š80ç»´mel
        
        try:
            print(f"   æµ‹è¯•è¾“å…¥: {test_mel.shape}")
            with torch.no_grad():
                output = hifigan(test_mel)
            print(f"   âœ… è¾“å‡ºæˆåŠŸ: {output.shape}")
            
        except Exception as e:
            print(f"   âŒ è°ƒç”¨å¤±è´¥: {e}")
            
    except Exception as e:
        print(f"âŒ HiFiGANåŠ è½½å¤±è´¥: {e}")

def analyze_dimension_mismatch():
    """åˆ†æç»´åº¦ä¸åŒ¹é…é—®é¢˜"""
    print(f"\nğŸ” ç»´åº¦ä¸åŒ¹é…é—®é¢˜åˆ†æ:")
    print("="*40)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ è½½æ¨¡å‹
    pipe = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2-music",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    hifigan = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan").to(device)
    
    # æ¨¡æ‹ŸVAEè¾“å‡º
    print("ğŸ“Š æ¨¡æ‹ŸçœŸå®ä½¿ç”¨åœºæ™¯:")
    
    # åˆ›å»ºæµ‹è¯•éŸ³é¢‘
    audio = np.random.randn(16000 * 3)  # 3ç§’éŸ³é¢‘
    
    # åˆ›å»ºmelé¢‘è°±
    mel_spec = librosa.feature.melspectrogram(
        y=audio, sr=16000, n_mels=64, n_fft=1024, hop_length=256
    )
    mel_spec = librosa.power_to_db(mel_spec)
    mel_spec = 2.0 * (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min()) - 1.0
    
    mel_tensor = torch.from_numpy(mel_spec).float().unsqueeze(0).unsqueeze(0).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.half()
    
    print(f"   åŸå§‹mel: {mel_tensor.shape}")
    
    # VAEå¤„ç†
    if mel_tensor.shape[-1] % 4 != 0:
        pad_length = 4 - (mel_tensor.shape[-1] % 4)
        mel_tensor = torch.nn.functional.pad(mel_tensor, (0, pad_length))
    
    print(f"   å¡«å……åmel: {mel_tensor.shape}")
    
    with torch.no_grad():
        # VAEç¼–ç è§£ç 
        latent = pipe.vae.encode(mel_tensor).latent_dist.sample()
        reconstructed_mel = pipe.vae.decode(latent).sample
        
        print(f"   VAEè¾“å‡º: {reconstructed_mel.shape}")
        
        # å°è¯•AudioLDM2å†…ç½®vocoder
        try:
            # å‡†å¤‡AudioLDM2 vocoderè¾“å…¥
            audioldm2_input = reconstructed_mel.squeeze(0).transpose(-2, -1).unsqueeze(0)
            print(f"   AudioLDM2è¾“å…¥: {audioldm2_input.shape}")
            audioldm2_output = pipe.vocoder(audioldm2_input)
            print(f"   âœ… AudioLDM2 vocoderæˆåŠŸ: {audioldm2_output.shape}")
        except Exception as e:
            print(f"   âŒ AudioLDM2 vocoderå¤±è´¥: {e}")
        
        # å°è¯•å¤–éƒ¨HiFiGAN
        try:
            # å‡†å¤‡HiFiGANè¾“å…¥ (éœ€è¦80ç»´)
            hifigan_input = reconstructed_mel.squeeze()
            if hifigan_input.dim() == 3:
                hifigan_input = hifigan_input.squeeze(0)
            
            print(f"   é‡å»ºmelåŸå§‹: {hifigan_input.shape}")
            
            # è½¬æ¢åˆ°80ç»´
            if hifigan_input.shape[0] != 80:
                hifigan_input = torch.nn.functional.interpolate(
                    hifigan_input.unsqueeze(0).unsqueeze(0),
                    size=(80, hifigan_input.shape[1]),
                    mode='bilinear',
                    align_corners=False
                ).squeeze(0).squeeze(0)
            
            hifigan_input = hifigan_input.unsqueeze(0)
            print(f"   HiFiGANè¾“å…¥: {hifigan_input.shape}")
            
            hifigan_output = hifigan(hifigan_input)
            print(f"   âœ… HiFiGANæˆåŠŸ: {hifigan_output.shape}")
            
        except Exception as e:
            print(f"   âŒ HiFiGANå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ AudioLDM2 + HiFiGAN é›†æˆé—®é¢˜æ·±åº¦åˆ†æ")
    print("="*60)
    
    # åˆ†æAudioLDM2å†…ç½®vocoder
    audioldm2_vocoder = analyze_audioldm2_vocoder()
    
    # åˆ†æå¤–éƒ¨HiFiGAN
    analyze_external_hifigan()
    
    # åˆ†æç»´åº¦ä¸åŒ¹é…é—®é¢˜
    analyze_dimension_mismatch()
    
    print(f"\nğŸ’¡ é—®é¢˜æ€»ç»“:")
    print("1. AudioLDM2å†…ç½®çš„æ˜¯SpeechT5HifiGanï¼Œä½†é…ç½®å’Œæ¥å£å¯èƒ½ä¸åŒ")
    print("2. å¤–éƒ¨HiFiGANæœŸæœ›80ç»´melï¼Œè€ŒAudioLDM2 VAEè¾“å‡ºå¯èƒ½æ˜¯64ç»´")
    print("3. æ•°æ®ç±»å‹å’Œç»´åº¦è½¬æ¢éœ€è¦ç²¾ç¡®åŒ¹é…")
    print("4. AudioLDM2å†…ç½®vocoderå·²ç»ä¼˜åŒ–è¿‡ï¼Œå¯èƒ½æ¯”å¤–éƒ¨HiFiGANæ›´é€‚åˆ")

if __name__ == "__main__":
    main()
