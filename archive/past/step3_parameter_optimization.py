#!/usr/bin/env python3
"""
Step 3: å‚æ•°ä¼˜åŒ–å’Œç²¾ç»†è°ƒæ•´
åŸºäºStep 2çš„æˆåŠŸç»“æœï¼Œå¯¹æœ€æœ‰æ•ˆçš„é¢‘ç‡å¢å¼ºæ–¹æ³•è¿›è¡Œå‚æ•°ä¼˜åŒ–
ç›®æ ‡ï¼šæ‰¾åˆ°è´¨é‡å’Œé«˜é¢‘æ¢å¤çš„æœ€ä½³å¹³è¡¡ç‚¹ï¼Œå®ç°æœ€ä¼˜æ€§èƒ½

ä¼˜åŒ–ç­–ç•¥ï¼š
1. ç³»ç»Ÿæ€§åœ°æµ‹è¯•ä¸åŒçš„å¢å¼ºç³»æ•°
2. ä¼˜åŒ–é«˜é¢‘æ£€æµ‹æ ¸çš„è®¾è®¡
3. æ·»åŠ è‡ªé€‚åº”å¢å¼ºæœºåˆ¶
4. ç»¼åˆè¯„ä¼°è´¨é‡å’Œæ„ŸçŸ¥æ•ˆæœ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import librosa
from pathlib import Path
import time
from diffusers import AudioLDM2Pipeline
from typing import Dict, Tuple, List
import warnings
warnings.filterwarnings("ignore")

class OptimizedLatentEnhancer:
    """
    ä¼˜åŒ–çš„Latentå¢å¼ºå™¨
    åŸºäºStep 2çš„å‘ç°ï¼Œä¸“æ³¨äºé¢‘ç‡å¢å¼ºæ–¹æ³•çš„å‚æ•°ä¼˜åŒ–
    """
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """åˆå§‹åŒ–ä¼˜åŒ–çš„Latentå¢å¼ºå™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸ¯ åˆå§‹åŒ–ä¼˜åŒ–çš„Latentå¢å¼ºå™¨")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        print(f"   ğŸ¤– æ¨¡å‹: {model_name}")
        
        # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        print(f"âœ… ä¼˜åŒ–çš„Latentå¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("optimized_latent_enhanced")
        self.output_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # é¢„å®šä¹‰ä¸åŒçš„é«˜é¢‘æ£€æµ‹æ ¸
        self.kernels = self._create_frequency_kernels()
    
    def _create_frequency_kernels(self) -> Dict[str, torch.Tensor]:
        """åˆ›å»ºä¸åŒç±»å‹çš„é«˜é¢‘æ£€æµ‹æ ¸"""
        kernels = {}
        
        # æ ‡å‡†Laplacianæ ¸
        kernels["laplacian"] = torch.tensor([
            [[-1, -1, -1],
             [-1,  8, -1],
             [-1, -1, -1]]
        ], dtype=torch.float32)
        
        # å¼ºåŒ–çš„Laplacianæ ¸
        kernels["strong_laplacian"] = torch.tensor([
            [[-2, -2, -2],
             [-2, 16, -2],
             [-2, -2, -2]]
        ], dtype=torch.float32)
        
        # Sobelè¾¹ç¼˜æ£€æµ‹æ ¸ï¼ˆæ°´å¹³ï¼‰
        kernels["sobel_h"] = torch.tensor([
            [[-1, 0, 1],
             [-2, 0, 2],
             [-1, 0, 1]]
        ], dtype=torch.float32)
        
        # Sobelè¾¹ç¼˜æ£€æµ‹æ ¸ï¼ˆå‚ç›´ï¼‰
        kernels["sobel_v"] = torch.tensor([
            [[-1, -2, -1],
             [ 0,  0,  0],
             [ 1,  2,  1]]
        ], dtype=torch.float32)
        
        # é«˜é€šæ»¤æ³¢å™¨
        kernels["highpass"] = torch.tensor([
            [[ 0, -1,  0],
             [-1,  5, -1],
             [ 0, -1,  0]]
        ], dtype=torch.float32)
        
        # é”åŒ–æ ¸
        kernels["sharpen"] = torch.tensor([
            [[ 0, -1,  0],
             [-1,  6, -1],
             [ 0, -1,  0]]
        ], dtype=torch.float32)
        
        return kernels
    
    def optimize_parameters(self, audio_path: str) -> Dict:
        """
        ç³»ç»Ÿæ€§åœ°ä¼˜åŒ–å¢å¼ºå‚æ•°
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            ä¼˜åŒ–ç»“æœå’Œæœ€ä½³å‚æ•°
        """
        print(f"\nğŸ”¬ å¼€å§‹å‚æ•°ä¼˜åŒ–: {Path(audio_path).name}")
        
        # åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘ï¼ˆåªåšä¸€æ¬¡ï¼‰
        original_audio, processed_audio = self._load_and_preprocess_audio(audio_path)
        latent = self._encode_audio(processed_audio)
        vae_only_audio = self._decode_audio(latent.clone())
        
        # å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
        test_configs = [
            # åŸºç¡€æµ‹è¯•ï¼šä¸åŒå¢å¼ºç³»æ•°
            {"name": "è½»å¾®å¢å¼º", "kernel": "laplacian", "boost": 1.2, "adaptive": False},
            {"name": "ä¸­ç­‰å¢å¼º", "kernel": "laplacian", "boost": 1.5, "adaptive": False},
            {"name": "å¼ºåº¦å¢å¼º", "kernel": "laplacian", "boost": 2.0, "adaptive": False},
            {"name": "æå¼ºå¢å¼º", "kernel": "laplacian", "boost": 3.0, "adaptive": False},
            
            # ä¸åŒæ ¸ç±»å‹æµ‹è¯•
            {"name": "å¼ºåŒ–Laplacian", "kernel": "strong_laplacian", "boost": 1.5, "adaptive": False},
            {"name": "Sobelæ°´å¹³", "kernel": "sobel_h", "boost": 1.5, "adaptive": False},
            {"name": "Sobelå‚ç›´", "kernel": "sobel_v", "boost": 1.5, "adaptive": False},
            {"name": "é«˜é€šæ»¤æ³¢", "kernel": "highpass", "boost": 1.5, "adaptive": False},
            {"name": "é”åŒ–æ ¸", "kernel": "sharpen", "boost": 1.5, "adaptive": False},
            
            # è‡ªé€‚åº”å¢å¼ºæµ‹è¯•
            {"name": "è‡ªé€‚åº”ä¸­ç­‰", "kernel": "laplacian", "boost": 1.5, "adaptive": True},
            {"name": "è‡ªé€‚åº”å¼ºåº¦", "kernel": "laplacian", "boost": 2.0, "adaptive": True},
        ]
        
        results = []
        timestamp = int(time.time())
        
        print(f"ğŸ§ª æµ‹è¯• {len(test_configs)} ç§å‚æ•°é…ç½®...")
        
        for i, config in enumerate(test_configs):
            print(f"\n   ğŸ”¬ æµ‹è¯• {i+1}/{len(test_configs)}: {config['name']}")
            
            # åº”ç”¨å¢å¼º
            enhanced_latent = self._apply_optimized_enhancement(
                latent.clone(),
                kernel_name=config['kernel'],
                boost_factor=config['boost'],
                adaptive=config['adaptive']
            )
            
            # è§£ç 
            enhanced_audio = self._decode_audio(enhanced_latent)
            
            # åˆ†æè´¨é‡
            quality_metrics = self._analyze_quality(original_audio, vae_only_audio, enhanced_audio)
            freq_metrics = self._analyze_frequency(original_audio, vae_only_audio, enhanced_audio)
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            composite_score = self._calculate_composite_score(quality_metrics, freq_metrics)
            
            # ä¿å­˜éŸ³é¢‘ï¼ˆä»…ä¿å­˜æœ€ä½³å‡ ä¸ªï¼‰
            audio_paths = None
            if i < 3 or composite_score > 7.0:  # ä¿å­˜å‰3ä¸ªå’Œé«˜åˆ†é…ç½®
                audio_paths = self._save_configuration_audio(
                    original_audio, vae_only_audio, enhanced_audio, 
                    timestamp, config['name']
                )
            
            result = {
                "config": config,
                "quality_metrics": quality_metrics,
                "frequency_metrics": freq_metrics,
                "composite_score": composite_score,
                "audio_paths": audio_paths
            }
            
            results.append(result)
            
            print(f"      ğŸ“Š ç»¼åˆå¾—åˆ†: {composite_score:.2f}")
            print(f"      ğŸ¼ é«˜é¢‘æ”¹è¿›: {freq_metrics['improvements']['high_freq_improvement']*100:+.1f}%")
            print(f"      ğŸ“ˆ SNRæ”¹è¿›: {quality_metrics['improvements']['snr_improvement']:+.2f} dB")
        
        # æ‰¾åˆ°æœ€ä½³é…ç½®
        best_result = max(results, key=lambda x: x['composite_score'])
        
        # åˆ›å»ºä¼˜åŒ–æŠ¥å‘Š
        self._create_optimization_report(results, best_result, timestamp)
        
        # åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–
        self._create_optimization_visualizations(results, timestamp)
        
        optimization_result = {
            "input_file": audio_path,
            "timestamp": timestamp,
            "all_results": results,
            "best_result": best_result,
            "total_configs_tested": len(test_configs)
        }
        
        self._display_optimization_results(optimization_result)
        
        return optimization_result
    
    def _apply_optimized_enhancement(self,
                                   latent: torch.Tensor,
                                   kernel_name: str = "laplacian",
                                   boost_factor: float = 1.5,
                                   adaptive: bool = False) -> torch.Tensor:
        """åº”ç”¨ä¼˜åŒ–çš„å¢å¼º"""
        
        kernel = self.kernels[kernel_name].to(latent.device, latent.dtype)
        kernel = kernel.unsqueeze(0).unsqueeze(0)  # [1, 1, 3, 3]
        
        enhanced_latent = latent.clone()
        
        # å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«å¤„ç†
        for c in range(latent.shape[1]):
            channel_latent = latent[:, c:c+1, :, :]  # [B, 1, H, W]
              # åº”ç”¨é«˜é¢‘æ£€æµ‹
            high_freq_response = F.conv2d(channel_latent, kernel, padding=1)
            
            # è‡ªé€‚åº”å¢å¼º
            if adaptive:
                # åŸºäºå±€éƒ¨æ–¹å·®è°ƒæ•´å¢å¼ºå¼ºåº¦
                local_mean = F.avg_pool2d(channel_latent, kernel_size=3, stride=1, padding=1)
                local_var = F.avg_pool2d((channel_latent - local_mean)**2, kernel_size=3, stride=1, padding=1)
                adaptive_factor = torch.clamp(local_var * 10, 0.5, 2.0)  # è‡ªé€‚åº”ç³»æ•°
                high_freq_enhancement = high_freq_response * (boost_factor - 1.0) * adaptive_factor
            else:
                high_freq_enhancement = high_freq_response * (boost_factor - 1.0)
            
            # åº”ç”¨å¢å¼º
            enhanced_channel = channel_latent + high_freq_enhancement
            enhanced_latent[:, c:c+1, :, :] = enhanced_channel
        
        return enhanced_latent
    
    def _calculate_composite_score(self, quality_metrics: Dict, freq_metrics: Dict) -> float:
        """
        è®¡ç®—ç»¼åˆè¯„åˆ†ï¼Œå¹³è¡¡è´¨é‡å’Œé¢‘ç‡æ”¹è¿›
        """
        q_improvements = quality_metrics['improvements']
        f_improvements = freq_metrics['improvements']
        
        # æƒé‡è®¾è®¡
        weights = {
            'snr': 0.2,           # SNRæƒé‡è¾ƒä½ï¼Œå› ä¸ºå¯èƒ½å› ä¸ºæ¢å¤é«˜é¢‘è€Œä¸‹é™
            'correlation': 0.3,    # ç›¸å…³æ€§é‡è¦
            'high_freq': 0.4,      # é«˜é¢‘æ¢å¤æœ€é‡è¦
            'overall_freq': 0.1    # æ•´ä½“é¢‘ç‡ç›¸å…³æ€§
        }
        
        # è®¡ç®—å„é¡¹å¾—åˆ†ï¼ˆå½’ä¸€åŒ–åˆ°0-10ï¼‰
        snr_score = max(0, min(10, (q_improvements['snr_improvement'] + 5) * 2))  # -5åˆ°+5dBæ˜ å°„åˆ°0-10
        corr_score = max(0, min(10, q_improvements['correlation_improvement'] * 100))  # ç›¸å…³æ€§æ”¹è¿›
        high_freq_score = max(0, min(10, f_improvements['high_freq_improvement'] * 10))  # é«˜é¢‘æ”¹è¿›
        overall_freq_score = max(0, min(10, f_improvements['frequency_correlation_improvement'] * 20))
        
        # ç»¼åˆå¾—åˆ†
        composite_score = (
            weights['snr'] * snr_score +
            weights['correlation'] * corr_score +
            weights['high_freq'] * high_freq_score +
            weights['overall_freq'] * overall_freq_score
        )
        
        return composite_score
    
    def _load_and_preprocess_audio(self, audio_path: str) -> Tuple[np.ndarray, torch.Tensor]:
        """åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘"""
        print(f"   ğŸ“‚ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
        
        original_audio, sr = torchaudio.load(audio_path)
        
        if original_audio.shape[0] > 1:
            original_audio = original_audio.mean(dim=0, keepdim=True)
        
        if sr != 48000:
            resampler = torchaudio.transforms.Resample(sr, 48000)
            processed_audio = resampler(original_audio)
        else:
            processed_audio = original_audio.clone()
        
        max_length = 48000 * 10
        if processed_audio.shape[-1] > max_length:
            processed_audio = processed_audio[..., :max_length]
        
        original_audio_np = original_audio.squeeze().numpy()
        return original_audio_np, processed_audio
    
    def _encode_audio(self, audio: torch.Tensor) -> torch.Tensor:
        """VAEç¼–ç """
        with torch.no_grad():
            audio_np = audio.squeeze().numpy()
            inputs = self.pipeline.feature_extractor(
                audio_np, sampling_rate=48000, return_tensors="pt"
            )
            mel_features = inputs["input_features"].to(self.device)
            
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode()
            latent = latent * self.pipeline.vae.config.scaling_factor
            
            return latent
    
    def _decode_audio(self, latent: torch.Tensor) -> np.ndarray:
        """VAEè§£ç """
        with torch.no_grad():
            latent_for_decode = latent / self.pipeline.vae.config.scaling_factor
            vae_dtype = next(self.pipeline.vae.parameters()).dtype
            latent_for_decode = latent_for_decode.to(vae_dtype)
            
            mel_spectrogram = self.pipeline.vae.decode(latent_for_decode).sample
            audio_tensor = self.pipeline.mel_spectrogram_to_waveform(mel_spectrogram)
            audio_np = audio_tensor.squeeze().cpu().numpy()
            
            return audio_np
    
    def _analyze_quality(self, original: np.ndarray, vae_only: np.ndarray, enhanced: np.ndarray) -> Dict:
        """è´¨é‡åˆ†æ"""
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        def calc_metrics(orig, recon):
            min_len = min(len(orig), len(recon))
            o, r = orig[:min_len], recon[:min_len]
            
            mse = np.mean((o - r) ** 2)
            snr = 10 * np.log10(np.mean(o ** 2) / (mse + 1e-10))
            correlation = np.corrcoef(o, r)[0, 1] if min_len > 1 else 0.0
            if np.isnan(correlation):
                correlation = 0.0
            
            return {"snr_db": snr, "correlation": correlation, "mse": mse}
        
        vae_metrics = calc_metrics(original_16k, vae_only)
        enhanced_metrics = calc_metrics(original_16k, enhanced)
        
        improvements = {
            "snr_improvement": enhanced_metrics["snr_db"] - vae_metrics["snr_db"],
            "correlation_improvement": enhanced_metrics["correlation"] - vae_metrics["correlation"],
            "mse_improvement": vae_metrics["mse"] - enhanced_metrics["mse"]
        }
        
        return {
            "vae_only": vae_metrics,
            "enhanced": enhanced_metrics,
            "improvements": improvements
        }
    
    def _analyze_frequency(self, original: np.ndarray, vae_only: np.ndarray, enhanced: np.ndarray) -> Dict:
        """é¢‘ç‡åˆ†æ"""
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        def analyze_bands(orig, recon):
            min_len = min(len(orig), len(recon))
            o, r = orig[:min_len], recon[:min_len]
            
            n_fft = 8192 if min_len >= 8192 else 2 ** int(np.log2(min_len))
            orig_fft = np.abs(np.fft.fft(o[:n_fft]))[:n_fft//2]
            recon_fft = np.abs(np.fft.fft(r[:n_fft]))[:n_fft//2]
            freqs = np.fft.fftfreq(n_fft, 1/16000)[:n_fft//2]
            
            low_mask = freqs < 500
            mid_mask = (freqs >= 500) & (freqs < 4000)
            high_mask = freqs >= 4000
            
            low_retention = np.sum(recon_fft[low_mask]) / (np.sum(orig_fft[low_mask]) + 1e-10)
            mid_retention = np.sum(recon_fft[mid_mask]) / (np.sum(orig_fft[mid_mask]) + 1e-10)
            high_retention = np.sum(recon_fft[high_mask]) / (np.sum(orig_fft[high_mask]) + 1e-10)
            
            freq_corr = np.corrcoef(orig_fft, recon_fft)[0, 1]
            if np.isnan(freq_corr):
                freq_corr = 0.0
            
            return {
                "low_freq_retention": low_retention,
                "mid_freq_retention": mid_retention,
                "high_freq_retention": high_retention,
                "frequency_correlation": freq_corr
            }
        
        vae_freq = analyze_bands(original_16k, vae_only)
        enhanced_freq = analyze_bands(original_16k, enhanced)
        
        improvements = {
            "low_freq_improvement": enhanced_freq["low_freq_retention"] - vae_freq["low_freq_retention"],
            "mid_freq_improvement": enhanced_freq["mid_freq_retention"] - vae_freq["mid_freq_retention"],
            "high_freq_improvement": enhanced_freq["high_freq_retention"] - vae_freq["high_freq_retention"],
            "frequency_correlation_improvement": enhanced_freq["frequency_correlation"] - vae_freq["frequency_correlation"]
        }
        
        return {
            "vae_only": vae_freq,
            "enhanced": enhanced_freq,
            "improvements": improvements
        }
    
    def _save_configuration_audio(self,
                                original: np.ndarray,
                                vae_only: np.ndarray,
                                enhanced: np.ndarray,
                                timestamp: int,
                                config_name: str) -> Dict[str, str]:
        """ä¿å­˜ç‰¹å®šé…ç½®çš„éŸ³é¢‘"""
        
        paths = {}
        safe_name = config_name.replace(" ", "_").replace("/", "_")
        
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        original_path = self.output_dir / f"original_{safe_name}_{timestamp}.wav"
        vae_path = self.output_dir / f"vae_only_{safe_name}_{timestamp}.wav"
        enhanced_path = self.output_dir / f"enhanced_{safe_name}_{timestamp}.wav"
        
        sf.write(str(original_path), original_16k, 16000)
        sf.write(str(vae_path), vae_only, 16000)
        sf.write(str(enhanced_path), enhanced, 16000)
        
        paths["original"] = str(original_path)
        paths["vae_only"] = str(vae_path)
        paths["enhanced"] = str(enhanced_path)
        
        return paths
    
    def _create_optimization_report(self, results: List[Dict], best_result: Dict, timestamp: int):
        """åˆ›å»ºä¼˜åŒ–æŠ¥å‘Š"""
        
        report_path = self.output_dir / f"optimization_report_{timestamp}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# å‚æ•°ä¼˜åŒ–æŠ¥å‘Š\n\n")
            f.write(f"## æµ‹è¯•æ¦‚å†µ\n")
            f.write(f"- æµ‹è¯•é…ç½®æ•°é‡: {len(results)}\n")
            f.write(f"- æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n\n")
            
            f.write("## æœ€ä½³é…ç½®\n")
            best_config = best_result['config']
            f.write(f"- **é…ç½®åç§°**: {best_config['name']}\n")
            f.write(f"- **æ£€æµ‹æ ¸**: {best_config['kernel']}\n")
            f.write(f"- **å¢å¼ºç³»æ•°**: {best_config['boost']}\n")
            f.write(f"- **è‡ªé€‚åº”**: {best_config['adaptive']}\n")
            f.write(f"- **ç»¼åˆå¾—åˆ†**: {best_result['composite_score']:.2f}\n\n")
            
            f.write("## æ‰€æœ‰é…ç½®ç»“æœ\n\n")
            f.write("| é…ç½®åç§° | æ£€æµ‹æ ¸ | å¢å¼ºç³»æ•° | è‡ªé€‚åº” | SNRæ”¹è¿›(dB) | é«˜é¢‘æ”¹è¿›(%) | ç»¼åˆå¾—åˆ† |\n")
            f.write("|---------|--------|----------|--------|-------------|-------------|----------|\n")
            
            # æŒ‰å¾—åˆ†æ’åº
            sorted_results = sorted(results, key=lambda x: x['composite_score'], reverse=True)
            
            for result in sorted_results:
                config = result['config']
                quality = result['quality_metrics']['improvements']
                freq = result['frequency_metrics']['improvements']
                score = result['composite_score']
                
                f.write(f"| {config['name']} | {config['kernel']} | {config['boost']} | "
                       f"{'æ˜¯' if config['adaptive'] else 'å¦'} | "
                       f"{quality['snr_improvement']:+.2f} | "
                       f"{freq['high_freq_improvement']*100:+.1f} | "
                       f"{score:.2f} |\n")
            
            f.write("\n## å…³é”®å‘ç°\n\n")
            
            # åˆ†ææœ€ä½³æ ¸ç±»å‹
            kernel_scores = {}
            for result in results:
                kernel = result['config']['kernel']
                if kernel not in kernel_scores:
                    kernel_scores[kernel] = []
                kernel_scores[kernel].append(result['composite_score'])
            
            f.write("### æ£€æµ‹æ ¸æ€§èƒ½å¯¹æ¯”\n")
            for kernel, scores in kernel_scores.items():
                avg_score = np.mean(scores)
                f.write(f"- **{kernel}**: å¹³å‡å¾—åˆ† {avg_score:.2f}\n")
            
            # åˆ†æå¢å¼ºç³»æ•°æ•ˆæœ
            f.write("\n### å¢å¼ºç³»æ•°æ•ˆæœ\n")
            boost_analysis = {}
            for result in results:
                boost = result['config']['boost']
                if boost not in boost_analysis:
                    boost_analysis[boost] = []
                boost_analysis[boost].append(result['composite_score'])
            
            for boost, scores in sorted(boost_analysis.items()):
                avg_score = np.mean(scores)
                f.write(f"- **{boost}x**: å¹³å‡å¾—åˆ† {avg_score:.2f}\n")
        
        print(f"   ğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def _create_optimization_visualizations(self, results: List[Dict], timestamp: int):
        """åˆ›å»ºä¼˜åŒ–å¯è§†åŒ–"""
        
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('å‚æ•°ä¼˜åŒ–ç»“æœå¯è§†åŒ–', fontsize=16, fontweight='bold')
            
            # æå–æ•°æ®
            config_names = [r['config']['name'] for r in results]
            composite_scores = [r['composite_score'] for r in results]
            snr_improvements = [r['quality_metrics']['improvements']['snr_improvement'] for r in results]
            high_freq_improvements = [r['frequency_metrics']['improvements']['high_freq_improvement'] * 100 for r in results]
            
            # 1. ç»¼åˆå¾—åˆ†å¯¹æ¯”
            axes[0, 0].bar(range(len(config_names)), composite_scores, alpha=0.7)
            axes[0, 0].set_title('ç»¼åˆå¾—åˆ†å¯¹æ¯”')
            axes[0, 0].set_xlabel('é…ç½®')
            axes[0, 0].set_ylabel('ç»¼åˆå¾—åˆ†')
            axes[0, 0].set_xticks(range(len(config_names)))
            axes[0, 0].set_xticklabels(config_names, rotation=45, ha='right')
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. SNRæ”¹è¿› vs é«˜é¢‘æ”¹è¿›æ•£ç‚¹å›¾
            axes[0, 1].scatter(snr_improvements, high_freq_improvements, alpha=0.7, s=100)
            for i, name in enumerate(config_names):
                axes[0, 1].annotate(name, (snr_improvements[i], high_freq_improvements[i]), 
                                  xytext=(5, 5), textcoords='offset points', fontsize=8)
            axes[0, 1].set_title('SNRæ”¹è¿› vs é«˜é¢‘æ”¹è¿›')
            axes[0, 1].set_xlabel('SNRæ”¹è¿› (dB)')
            axes[0, 1].set_ylabel('é«˜é¢‘æ”¹è¿› (%)')
            axes[0, 1].grid(True, alpha=0.3)
            
            # 3. æ ¸ç±»å‹æ€§èƒ½å¯¹æ¯”
            kernel_types = {}
            for result in results:
                kernel = result['config']['kernel']
                if kernel not in kernel_types:
                    kernel_types[kernel] = []
                kernel_types[kernel].append(result['composite_score'])
            
            kernel_names = list(kernel_types.keys())
            kernel_avg_scores = [np.mean(scores) for scores in kernel_types.values()]
            
            axes[1, 0].bar(kernel_names, kernel_avg_scores, alpha=0.7)
            axes[1, 0].set_title('ä¸åŒæ£€æµ‹æ ¸çš„å¹³å‡æ€§èƒ½')
            axes[1, 0].set_xlabel('æ£€æµ‹æ ¸ç±»å‹')
            axes[1, 0].set_ylabel('å¹³å‡ç»¼åˆå¾—åˆ†')
            axes[1, 0].tick_params(axis='x', rotation=45)
            axes[1, 0].grid(True, alpha=0.3)
            
            # 4. å¢å¼ºç³»æ•°æ•ˆæœ
            boost_factors = {}
            for result in results:
                boost = result['config']['boost']
                if boost not in boost_factors:
                    boost_factors[boost] = []
                boost_factors[boost].append(result['composite_score'])
            
            boost_values = sorted(boost_factors.keys())
            boost_avg_scores = [np.mean(boost_factors[boost]) for boost in boost_values]
            
            axes[1, 1].plot(boost_values, boost_avg_scores, 'o-', linewidth=2, markersize=8)
            axes[1, 1].set_title('å¢å¼ºç³»æ•°æ•ˆæœ')
            axes[1, 1].set_xlabel('å¢å¼ºç³»æ•°')
            axes[1, 1].set_ylabel('å¹³å‡ç»¼åˆå¾—åˆ†')
            axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            plot_path = self.output_dir / f"optimization_visualization_{timestamp}.png"
            plt.savefig(str(plot_path), dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"   ğŸ“Š ä¼˜åŒ–å¯è§†åŒ–å·²ä¿å­˜: {plot_path}")
            
        except Exception as e:
            print(f"   âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    def _display_optimization_results(self, result: Dict):
        """æ˜¾ç¤ºä¼˜åŒ–ç»“æœ"""
        print(f"\n{'='*90}")
        print(f"ğŸ¯ å‚æ•°ä¼˜åŒ–å®Œæˆï¼")
        print(f"{'='*90}")
        
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {result['input_file']}")
        print(f"ğŸ§ª æµ‹è¯•é…ç½®æ•°é‡: {result['total_configs_tested']}")
        
        best = result['best_result']
        best_config = best['config']
        best_quality = best['quality_metrics']['improvements']
        best_freq = best['frequency_metrics']['improvements']
        
        print(f"\nğŸ† æœ€ä½³é…ç½®:")
        print(f"   ğŸ“› åç§°: {best_config['name']}")
        print(f"   ğŸ” æ£€æµ‹æ ¸: {best_config['kernel']}")
        print(f"   ğŸ“ˆ å¢å¼ºç³»æ•°: {best_config['boost']}x")
        print(f"   ğŸ§  è‡ªé€‚åº”: {'æ˜¯' if best_config['adaptive'] else 'å¦'}")
        print(f"   ğŸ… ç»¼åˆå¾—åˆ†: {best['composite_score']:.2f}/10")
        
        print(f"\nğŸ“Š æœ€ä½³é…ç½®æ€§èƒ½:")
        print(f"   ğŸ“ˆ SNRæ”¹è¿›: {best_quality['snr_improvement']:+.2f} dB")
        print(f"   ğŸ”— ç›¸å…³æ€§æ”¹è¿›: {best_quality['correlation_improvement']:+.4f}")
        print(f"   ğŸ¼ é«˜é¢‘æ”¹è¿›: {best_freq['high_freq_improvement']*100:+.1f}%")
        print(f"   ğŸµ æ•´ä½“é¢‘ç‡ç›¸å…³æ€§æ”¹è¿›: {best_freq['frequency_correlation_improvement']:+.4f}")
        
        # Top 3é…ç½®
        sorted_results = sorted(result['all_results'], key=lambda x: x['composite_score'], reverse=True)
        print(f"\nğŸ¥‡ Top 3 é…ç½®:")
        for i, r in enumerate(sorted_results[:3]):
            config = r['config']
            score = r['composite_score']
            high_freq = r['frequency_metrics']['improvements']['high_freq_improvement'] * 100
            print(f"   {i+1}. {config['name']}: å¾—åˆ† {score:.2f}, é«˜é¢‘æ”¹è¿› {high_freq:+.1f}%")
        
        # å…³é”®å‘ç°
        print(f"\nğŸ’¡ å…³é”®å‘ç°:")
        
        # æœ€ä½³æ ¸ç±»å‹
        kernel_scores = {}
        for r in result['all_results']:
            kernel = r['config']['kernel']
            if kernel not in kernel_scores:
                kernel_scores[kernel] = []
            kernel_scores[kernel].append(r['composite_score'])
        
        best_kernel = max(kernel_scores.keys(), key=lambda k: np.mean(kernel_scores[k]))
        print(f"   ğŸ” æœ€ä½³æ£€æµ‹æ ¸: {best_kernel} (å¹³å‡å¾—åˆ† {np.mean(kernel_scores[best_kernel]):.2f})")
        
        # æœ€ä½³å¢å¼ºç³»æ•°
        boost_scores = {}
        for r in result['all_results']:
            boost = r['config']['boost']
            if boost not in boost_scores:
                boost_scores[boost] = []
            boost_scores[boost].append(r['composite_score'])
        
        best_boost = max(boost_scores.keys(), key=lambda b: np.mean(boost_scores[b]))
        print(f"   ğŸ“ˆ æœ€ä½³å¢å¼ºç³»æ•°: {best_boost}x (å¹³å‡å¾—åˆ† {np.mean(boost_scores[best_boost]):.2f})")
        
        # è‡ªé€‚åº”æ•ˆæœ
        adaptive_scores = [r['composite_score'] for r in result['all_results'] if r['config']['adaptive']]
        non_adaptive_scores = [r['composite_score'] for r in result['all_results'] if not r['config']['adaptive']]
        
        if adaptive_scores and non_adaptive_scores:
            adaptive_avg = np.mean(adaptive_scores)
            non_adaptive_avg = np.mean(non_adaptive_scores)
            if adaptive_avg > non_adaptive_avg:
                print(f"   ğŸ§  è‡ªé€‚åº”å¢å¼ºæœ‰æ•ˆ: å¹³å‡å¾—åˆ†æå‡ {adaptive_avg - non_adaptive_avg:.2f}")
            else:
                print(f"   ğŸ”’ å›ºå®šå¢å¼ºæ›´ç¨³å®š: å¹³å‡å¾—åˆ†é«˜å‡º {non_adaptive_avg - adaptive_avg:.2f}")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   ğŸ“„ ä¼˜åŒ–æŠ¥å‘Š: optimization_report_{result['timestamp']}.md")
        print(f"   ğŸ“Š å¯è§†åŒ–å›¾è¡¨: optimization_visualization_{result['timestamp']}.png")
        
        if best['audio_paths']:
            print(f"   ğŸµ æœ€ä½³é…ç½®éŸ³é¢‘:")
            for name, path in best['audio_paths'].items():
                print(f"      {name}: {Path(path).name}")

def demo_parameter_optimization():
    """æ¼”ç¤ºå‚æ•°ä¼˜åŒ–"""
    print("ğŸ¯ Step 3: å‚æ•°ä¼˜åŒ–å’Œç²¾ç»†è°ƒæ•´")
    print("=" * 70)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        print("è¯·ç¡®ä¿AudioLDM2_Music_output.wavåœ¨å½“å‰ç›®å½•ä¸­")
        return
    
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = OptimizedLatentEnhancer()
    
    # æ‰§è¡Œå‚æ•°ä¼˜åŒ–
    result = optimizer.optimize_parameters(input_file)
    
    print(f"\nâœ… å‚æ•°ä¼˜åŒ–å®Œæˆï¼")
    print(f"ğŸ“Š æŸ¥çœ‹è¾“å‡ºç›®å½•: optimized_latent_enhanced/")
    print(f"ğŸµ è¯•å¬æœ€ä½³é…ç½®çš„éŸ³é¢‘æ•ˆæœ")
    print(f"ğŸ“„ æŸ¥çœ‹è¯¦ç»†çš„ä¼˜åŒ–æŠ¥å‘Š")
    
    print(f"\nğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"   1. ä½¿ç”¨æœ€ä½³å‚æ•°é…ç½®å¤„ç†æ›´å¤šéŸ³é¢‘æ–‡ä»¶")
    print(f"   2. è€ƒè™‘é›†æˆåˆ°ç”Ÿäº§ç¯å¢ƒä¸­")
    print(f"   3. æ¢ç´¢æ›´é«˜çº§çš„å¢å¼ºæŠ€æœ¯")

if __name__ == "__main__":
    demo_parameter_optimization()
