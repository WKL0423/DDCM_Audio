#!/usr/bin/env python3
"""
Step 1: å•çº¯çš„VAEé‡å»ºæµ‹è¯•
é‡æ–°å¼€å§‹æ„å»ºDDCMç®¡é“ï¼Œç¬¬ä¸€æ­¥å…ˆéªŒè¯åŸºç¡€VAEé‡å»ºæ•ˆæœ
ä¸“æ³¨äºï¼š
1. éŸ³é¢‘åŠ è½½å’Œé¢„å¤„ç†
2. VAEç¼–ç 
3. VAEè§£ç 
4. éŸ³é¢‘ä¿å­˜å’Œè´¨é‡åˆ†æ
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Tuple
# ä½¿ç”¨è‡ªå®šä¹‰çš„AudioLDM2Pipeline
from New_pipeline_audioldm2 import AudioLDM2Pipeline
import torchaudio
from pathlib import Path
import time
import soundfile as sf
import librosa
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

class SimpleVAEReconstructor:
    """
    ç®€å•çš„VAEé‡å»ºå™¨
    åªåšæœ€åŸºç¡€çš„éŸ³é¢‘â†’VAEç¼–ç â†’VAEè§£ç â†’éŸ³é¢‘çš„æµç¨‹
    """
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """åˆå§‹åŒ–VAEé‡å»ºå™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸµ åˆå§‹åŒ–ç®€å•VAEé‡å»ºå™¨")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        
        # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        print(f"âœ… VAEé‡å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š VAE scaling_factor: {self.pipeline.vae.config.scaling_factor}")
        print(f"   ğŸ¤ ç‰¹å¾æå–å™¨é‡‡æ ·ç‡: {self.pipeline.feature_extractor.sampling_rate}Hz")
    
    def reconstruct_audio(self, audio_path: str) -> Dict:
        """
        å®Œæ•´çš„VAEé‡å»ºæµç¨‹
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            
        Returns:
            result: åŒ…å«é‡å»ºç»“æœå’Œåˆ†æçš„å­—å…¸
        """
        print(f"\nğŸµ VAEé‡å»ºå¤„ç†: {Path(audio_path).name}")
        
        # 1. åŠ è½½å’Œé¢„å¤„ç†è¾“å…¥éŸ³é¢‘
        original_audio, input_latent = self._load_and_encode_audio(audio_path)
        
        # 2. VAEè§£ç é‡å»º
        reconstructed_audio = self._decode_latent_to_audio(input_latent)
        
        # 3. ä¿å­˜ç»“æœ
        result = self._save_and_analyze(original_audio, reconstructed_audio, audio_path)
        
        # 4. æ˜¾ç¤ºç»“æœ
        self._display_results(result)
        
        return result
    
    def _load_and_encode_audio(self, audio_path: str) -> Tuple[np.ndarray, torch.Tensor]:
        """åŠ è½½éŸ³é¢‘å¹¶ç¼–ç ä¸ºlatent"""
        print(f"   ğŸ“ åŠ è½½éŸ³é¢‘...")
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = torchaudio.load(audio_path)
        
        # é‡é‡‡æ ·åˆ°AudioLDM2çš„é‡‡æ ·ç‡ (48kHz)
        target_sr = self.pipeline.feature_extractor.sampling_rate
        if sr != target_sr:
            print(f"   ğŸ”„ é‡é‡‡æ ·: {sr}Hz -> {target_sr}Hz")
            resampler = torchaudio.transforms.Resample(sr, target_sr)
            audio = resampler(audio)
        
        # è½¬ä¸ºå•å£°é“
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # é™åˆ¶é•¿åº¦åˆ°10ç§’
        max_length = target_sr * 10
        if audio.shape[-1] > max_length:
            audio = audio[..., :max_length]
            print(f"   âœ‚ï¸ æˆªå–åˆ°10ç§’")
        
        print(f"   âœ… éŸ³é¢‘é¢„å¤„ç†å®Œæˆ: {audio.shape}, {audio.shape[-1]/target_sr:.2f}ç§’")
        
        # ç¼–ç ä¸ºlatent
        print(f"   ğŸ”„ VAEç¼–ç ...")
        with torch.no_grad():
            # ç¡®ä¿éŸ³é¢‘æ˜¯2Då¼ é‡ [channels, samples]
            audio_np = audio.squeeze(0).numpy()
            
            # ä½¿ç”¨AudioLDM2çš„ç‰¹å¾æå–å™¨
            inputs = self.pipeline.feature_extractor(
                audio_np,
                sampling_rate=target_sr,
                return_tensors="pt"
            )
            
            mel_features = inputs["input_features"].to(self.device)
            
            # è°ƒæ•´ç»´åº¦ï¼šç¡®ä¿æ˜¯ [batch, channels, height, width]
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            # è½¬æ¢æ•°æ®ç±»å‹
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            print(f"   ğŸ“Š Melç‰¹å¾: {mel_features.shape}, èŒƒå›´[{mel_features.min():.3f}, {mel_features.max():.3f}]")
            
            # VAEç¼–ç 
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode()  # ä½¿ç”¨modeè€Œä¸æ˜¯sampleï¼Œæ›´ç¨³å®š
            
            # åº”ç”¨scaling factor
            latent = latent * self.pipeline.vae.config.scaling_factor
            
            print(f"   âœ… VAEç¼–ç å®Œæˆ: {latent.shape}")
            print(f"   ğŸ“Š LatentèŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
            print(f"   ğŸ“Š Latent std: {latent.std():.3f}")
        
        # è½¬æ¢åŸå§‹éŸ³é¢‘åˆ°16kHzç”¨äºåˆ†æ
        audio_16k = torchaudio.functional.resample(audio, target_sr, 16000)
        original_audio = audio_16k.squeeze().numpy()
        
        return original_audio, latent
    
    def _decode_latent_to_audio(self, latent: torch.Tensor) -> np.ndarray:
        """å°†latentè§£ç ä¸ºéŸ³é¢‘"""
        print(f"   ğŸ”„ VAEè§£ç ...")
        
        with torch.no_grad():
            # å‡†å¤‡è§£ç 
            latent_for_decode = latent / self.pipeline.vae.config.scaling_factor
            
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            vae_dtype = next(self.pipeline.vae.parameters()).dtype
            latent_for_decode = latent_for_decode.to(vae_dtype)
            
            print(f"   ğŸ“Š è§£ç latent: {latent_for_decode.shape}, {latent_for_decode.dtype}")
            print(f"   ğŸ“Š è§£ç latentèŒƒå›´: [{latent_for_decode.min():.3f}, {latent_for_decode.max():.3f}]")
            
            # VAEè§£ç 
            mel_spectrogram = self.pipeline.vae.decode(latent_for_decode).sample
            
            print(f"   ğŸ“Š è§£ç mel: {mel_spectrogram.shape}")
            print(f"   ğŸ“Š è§£ç melèŒƒå›´: [{mel_spectrogram.min():.3f}, {mel_spectrogram.max():.3f}]")
            
            # ä½¿ç”¨HiFiGAN vocoder
            audio_tensor = self.pipeline.mel_spectrogram_to_waveform(mel_spectrogram)
            audio = audio_tensor.squeeze().cpu().numpy()
            
            print(f"   âœ… VAEè§£ç å®Œæˆ: {len(audio)}æ ·æœ¬ ({len(audio)/16000:.2f}ç§’)")
            
        return audio
    
    def _save_and_analyze(self, original_audio: np.ndarray, reconstructed_audio: np.ndarray, 
                         audio_path: str) -> Dict:
        """ä¿å­˜ç»“æœå¹¶è¿›è¡Œè´¨é‡åˆ†æ"""
        print(f"   ğŸ’¾ ä¿å­˜ç»“æœå’Œè´¨é‡åˆ†æ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("step_1_simple_vae_reconstruction")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = int(time.time())
        input_name = Path(audio_path).stem
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(original_audio), len(reconstructed_audio))
        original_audio = original_audio[:min_len]
        reconstructed_audio = reconstructed_audio[:min_len]
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
        reconstructed_path = output_dir / f"{input_name}_vae_reconstructed_{timestamp}.wav"
        
        sf.write(str(original_path), original_audio, 16000)
        sf.write(str(reconstructed_path), reconstructed_audio, 16000)
        
        # è´¨é‡åˆ†æ
        analysis = self._analyze_audio_quality(original_audio, reconstructed_audio)
        
        # é¢‘è°±åˆ†æ
        spectral_analysis = self._analyze_frequency_content(original_audio, reconstructed_audio)
        
        # ç”Ÿæˆé¢‘è°±å›¾
        self._plot_spectrograms(original_audio, reconstructed_audio, output_dir, timestamp)
        
        result = {
            "input_file": audio_path,
            "original_path": str(original_path),
            "reconstructed_path": str(reconstructed_path),
            "audio_length": min_len / 16000,
            "quality_metrics": analysis,
            "frequency_analysis": spectral_analysis,
            "timestamp": timestamp
        }
        
        return result
    
    def _analyze_audio_quality(self, original: np.ndarray, reconstructed: np.ndarray) -> Dict:
        """åˆ†æéŸ³é¢‘è´¨é‡æŒ‡æ ‡"""
        # åŸºç¡€è´¨é‡æŒ‡æ ‡
        mse = np.mean((original - reconstructed) ** 2)
        snr = 10 * np.log10(np.mean(original ** 2) / (mse + 1e-10))
        correlation = np.corrcoef(original, reconstructed)[0, 1] if len(original) > 1 else 0
        mae = np.mean(np.abs(original - reconstructed))
        
        # RMSæ¯”è¾ƒ
        rms_original = np.sqrt(np.mean(original ** 2))
        rms_reconstructed = np.sqrt(np.mean(reconstructed ** 2))
        rms_ratio = rms_reconstructed / (rms_original + 1e-10)
        
        return {
            "snr_db": snr,
            "correlation": correlation,
            "mse": mse,
            "mae": mae,
            "rms_original": rms_original,
            "rms_reconstructed": rms_reconstructed,
            "rms_ratio": rms_ratio
        }
    
    def _analyze_frequency_content(self, original: np.ndarray, reconstructed: np.ndarray) -> Dict:
        """åˆ†æé¢‘è°±å†…å®¹"""
        if len(original) < 8192:
            return {"error": "éŸ³é¢‘å¤ªçŸ­ï¼Œæ— æ³•è¿›è¡Œé¢‘è°±åˆ†æ"}
        
        # FFTåˆ†æ
        fft_len = 8192
        orig_fft = np.abs(np.fft.fft(original[:fft_len]))[:fft_len//2]
        recon_fft = np.abs(np.fft.fft(reconstructed[:fft_len]))[:fft_len//2]
        
        # é¢‘æ®µåˆ†æ
        freqs = np.fft.fftfreq(fft_len, 1/16000)[:fft_len//2]
        
        # å®šä¹‰é¢‘æ®µ
        low_freq_mask = freqs < 1000    # ä½é¢‘ 0-1kHz
        mid_freq_mask = (freqs >= 1000) & (freqs < 4000)  # ä¸­é¢‘ 1-4kHz
        high_freq_mask = freqs >= 4000   # é«˜é¢‘ 4-8kHz
        
        # è®¡ç®—å„é¢‘æ®µèƒ½é‡
        low_orig = np.sum(orig_fft[low_freq_mask])
        mid_orig = np.sum(orig_fft[mid_freq_mask])
        high_orig = np.sum(orig_fft[high_freq_mask])
        
        low_recon = np.sum(recon_fft[low_freq_mask])
        mid_recon = np.sum(recon_fft[mid_freq_mask])
        high_recon = np.sum(recon_fft[high_freq_mask])
        
        # è®¡ç®—ä¿æŒç‡
        low_retention = low_recon / (low_orig + 1e-10)
        mid_retention = mid_recon / (mid_orig + 1e-10)
        high_retention = high_recon / (high_orig + 1e-10)
        
        return {
            "low_freq_retention": low_retention,
            "mid_freq_retention": mid_retention,
            "high_freq_retention": high_retention,
            "total_energy_ratio": np.sum(recon_fft) / (np.sum(orig_fft) + 1e-10)
        }
    
    def _plot_spectrograms(self, original: np.ndarray, reconstructed: np.ndarray, 
                          output_dir: Path, timestamp: int):
        """ç»˜åˆ¶é¢‘è°±å›¾å¯¹æ¯”"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # åŸå§‹éŸ³é¢‘é¢‘è°±å›¾
            D_orig = librosa.amplitude_to_db(np.abs(librosa.stft(original)), ref=np.max)
            librosa.display.specshow(D_orig, sr=16000, x_axis='time', y_axis='hz', ax=axes[0,0])
            axes[0,0].set_title('åŸå§‹éŸ³é¢‘é¢‘è°±å›¾')
            axes[0,0].set_ylim([0, 8000])
            
            # é‡å»ºéŸ³é¢‘é¢‘è°±å›¾
            D_recon = librosa.amplitude_to_db(np.abs(librosa.stft(reconstructed)), ref=np.max)
            librosa.display.specshow(D_recon, sr=16000, x_axis='time', y_axis='hz', ax=axes[0,1])
            axes[0,1].set_title('VAEé‡å»ºéŸ³é¢‘é¢‘è°±å›¾')
            axes[0,1].set_ylim([0, 8000])
            
            # é¢‘è°±å·®å¼‚
            diff = D_orig - D_recon
            im = librosa.display.specshow(diff, sr=16000, x_axis='time', y_axis='hz', ax=axes[1,0], cmap='RdBu_r')
            axes[1,0].set_title('é¢‘è°±å·®å¼‚ (åŸå§‹ - é‡å»º)')
            axes[1,0].set_ylim([0, 8000])
            plt.colorbar(im, ax=axes[1,0])
            
            # é¢‘ç‡èƒ½é‡å¯¹æ¯”
            fft_len = min(8192, len(original))
            orig_fft = np.abs(np.fft.fft(original[:fft_len]))[:fft_len//2]
            recon_fft = np.abs(np.fft.fft(reconstructed[:fft_len]))[:fft_len//2]
            freqs = np.fft.fftfreq(fft_len, 1/16000)[:fft_len//2]
            
            axes[1,1].semilogy(freqs, orig_fft, label='åŸå§‹', alpha=0.7)
            axes[1,1].semilogy(freqs, recon_fft, label='é‡å»º', alpha=0.7)
            axes[1,1].set_xlabel('é¢‘ç‡ (Hz)')
            axes[1,1].set_ylabel('å¹…åº¦')
            axes[1,1].set_title('é¢‘ç‡å“åº”å¯¹æ¯”')
            axes[1,1].legend()
            axes[1,1].set_xlim([0, 8000])
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            plot_path = output_dir / f"spectral_analysis_{timestamp}.png"
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"   ğŸ“Š é¢‘è°±å›¾å·²ä¿å­˜: {plot_path}")
            
        except Exception as e:
            print(f"   âš ï¸ é¢‘è°±å›¾ç”Ÿæˆå¤±è´¥: {e}")
    
    def _display_results(self, result: Dict):
        """æ˜¾ç¤ºç»“æœ"""
        print(f"\n{'='*70}")
        print(f"ğŸ¯ ç®€å•VAEé‡å»ºç»“æœ")
        print(f"{'='*70}")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {result['input_file']}")
        print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {result['original_path']}")
        print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {result['reconstructed_path']}")
        print(f"â±ï¸ éŸ³é¢‘é•¿åº¦: {result['audio_length']:.2f}ç§’")
        
        metrics = result['quality_metrics']
        print(f"\nğŸ“Š è´¨é‡æŒ‡æ ‡:")
        print(f"   SNR: {metrics['snr_db']:.2f} dB")
        print(f"   ç›¸å…³ç³»æ•°: {metrics['correlation']:.4f}")
        print(f"   MSE: {metrics['mse']:.6f}")
        print(f"   MAE: {metrics['mae']:.6f}")
        print(f"   RMSæ¯”ç‡: {metrics['rms_ratio']:.4f}")
        
        freq_analysis = result['frequency_analysis']
        if 'error' not in freq_analysis:
            print(f"\nğŸµ é¢‘ç‡åˆ†æ:")
            print(f"   ä½é¢‘ä¿æŒç‡ (0-1kHz): {freq_analysis['low_freq_retention']:.3f}")
            print(f"   ä¸­é¢‘ä¿æŒç‡ (1-4kHz): {freq_analysis['mid_freq_retention']:.3f}")
            print(f"   é«˜é¢‘ä¿æŒç‡ (4-8kHz): {freq_analysis['high_freq_retention']:.3f}")
            print(f"   æ€»èƒ½é‡æ¯”ç‡: {freq_analysis['total_energy_ratio']:.3f}")
            
            # åˆ†æé«˜é¢‘ä¸¢å¤±æƒ…å†µ
            high_freq_retention = freq_analysis['high_freq_retention']
            if high_freq_retention < 0.3:
                print(f"   âŒ ä¸¥é‡é«˜é¢‘æŸå¤±ï¼ä¿æŒç‡ä»… {high_freq_retention*100:.1f}%")
            elif high_freq_retention < 0.6:
                print(f"   âš ï¸ æ˜æ˜¾é«˜é¢‘æŸå¤±ï¼Œä¿æŒç‡ {high_freq_retention*100:.1f}%")
            elif high_freq_retention < 0.8:
                print(f"   âš¡ è½»å¾®é«˜é¢‘æŸå¤±ï¼Œä¿æŒç‡ {high_freq_retention*100:.1f}%")
            else:
                print(f"   âœ… é«˜é¢‘ä¿æŒè‰¯å¥½ï¼Œä¿æŒç‡ {high_freq_retention*100:.1f}%")
        
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        if metrics['snr_db'] > 10:
            print(f"   âœ… VAEé‡å»ºè´¨é‡è‰¯å¥½ï¼Œå¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šæ·»åŠ diffusionè¿‡ç¨‹")
        else:
            print(f"   âš ï¸ VAEé‡å»ºè´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®å…ˆä¼˜åŒ–VAEå‚æ•°")
            
        if freq_analysis.get('high_freq_retention', 0) < 0.5:
            print(f"   ğŸ¯ é«˜é¢‘æŸå¤±æ˜æ˜¾ï¼Œdiffusionè¿‡ç¨‹åº”é‡ç‚¹è¡¥å……é«˜é¢‘ä¿¡æ¯")
        
        print(f"   ğŸ“Š å¯è§†åŒ–åˆ†æå›¾å·²ç”Ÿæˆ")

def test_simple_vae():
    """æµ‹è¯•ç®€å•VAEé‡å»º"""
    print("ğŸµ ç®€å•VAEé‡å»ºæµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        print("è¯·ç¡®ä¿AudioLDM2_Music_output.wavåœ¨å½“å‰ç›®å½•ä¸­")
        return
    
    # åˆ›å»ºVAEé‡å»ºå™¨
    vae_reconstructor = SimpleVAEReconstructor()
    
    # æ‰§è¡Œé‡å»º
    result = vae_reconstructor.reconstruct_audio(input_file)
    
    print(f"\nâœ… ç®€å•VAEé‡å»ºæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ æŸ¥çœ‹è¾“å‡ºç›®å½•: simple_vae_reconstruction/")
    print(f"ğŸµ å¯¹æ¯”åŸå§‹å’Œé‡å»ºéŸ³é¢‘ï¼Œä¸ºä¸‹ä¸€æ­¥æ·»åŠ diffusionåšå‡†å¤‡")

if __name__ == "__main__":
    test_simple_vae()
