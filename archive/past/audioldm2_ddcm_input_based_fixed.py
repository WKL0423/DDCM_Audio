#!/usr/bin/env python3
"""
AudioLDM2 DDCM åŸºäºè¾“å…¥éŸ³é¢‘çš„é‡å»ºå’Œå˜æ¢ (ä¿®å¤ç‰ˆ)
çœŸæ­£ä¸è¾“å…¥æ–‡ä»¶ç›¸å…³çš„DDCMå®ç°
æ ¸å¿ƒæ€æƒ³ï¼š
1. å°†è¾“å…¥éŸ³é¢‘ç¼–ç åˆ°latentç©ºé—´
2. ä½¿ç”¨DDCMç æœ¬é‡åŒ–latent
3. é€šè¿‡diffusionè¿‡ç¨‹é‡å»º/å˜æ¢éŸ³é¢‘
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from diffusers import AudioLDM2Pipeline, DDPMScheduler
import torchaudio
from pathlib import Path
import json
import time
import soundfile as sf
import librosa
import warnings
warnings.filterwarnings("ignore")

class AudioDDCMCodebook(nn.Module):
    """
    åŸºäºéŸ³é¢‘å†…å®¹çš„DDCMç æœ¬
    å°†è¾“å…¥éŸ³é¢‘çš„latentè¡¨ç¤ºé‡åŒ–åˆ°ç æœ¬
    """
    
    def __init__(self, 
                 codebook_size: int = 512,
                 latent_shape: Tuple[int, int, int] = (8, 250, 16)):
        super().__init__()
        
        self.codebook_size = codebook_size
        self.latent_shape = latent_shape
        self.latent_dim = np.prod(latent_shape)
        
        # åˆ›å»ºç æœ¬ï¼šä½¿ç”¨æ ‡å‡†é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–
        self.register_buffer("codebook", torch.randn(codebook_size, *latent_shape))
        self.register_buffer("usage_count", torch.zeros(codebook_size))
        
        print(f"ğŸ”§ Audio DDCMç æœ¬åˆå§‹åŒ–:")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {codebook_size}")
        print(f"   ğŸ“ Latentå½¢çŠ¶: {latent_shape}")
    
    def quantize_latent(self, latent: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å°†latenté‡åŒ–åˆ°æœ€è¿‘çš„ç æœ¬å‘é‡
        
        Args:
            latent: è¾“å…¥latent [batch, channels, height, width]
            
        Returns:
            quantized: é‡åŒ–åçš„latent
            indices: ç æœ¬ç´¢å¼•
            distances: è·ç¦»
        """
        batch_size = latent.shape[0]
        
        # å±•å¹³è®¡ç®—è·ç¦»
        latent_flat = latent.view(batch_size, -1).float()
        codebook_flat = self.codebook.view(self.codebook_size, -1).float()
        
        # è®¡ç®—L2è·ç¦»
        distances = torch.cdist(latent_flat, codebook_flat, p=2)
        
        # æ‰¾åˆ°æœ€è¿‘çš„ç æœ¬å‘é‡
        min_distances, indices = torch.min(distances, dim=1)
        
        # è·å–é‡åŒ–åçš„å‘é‡ï¼Œä¿æŒåŸå§‹æ•°æ®ç±»å‹
        quantized = self.codebook[indices].to(latent.dtype)
        
        # æ›´æ–°ä½¿ç”¨è®¡æ•°
        with torch.no_grad():
            for idx in indices:
                self.usage_count[idx] += 1
        
        return quantized, indices, min_distances
    
    def get_codebook_vector(self, indices: torch.Tensor) -> torch.Tensor:
        """æ ¹æ®ç´¢å¼•è·å–ç æœ¬å‘é‡"""
        return self.codebook[indices]

class AudioLDM2_InputBased_DDCM:
    """
    åŸºäºè¾“å…¥éŸ³é¢‘çš„AudioLDM2 DDCMç®¡é“
    å®ç°éŸ³é¢‘â†’latentâ†’ç æœ¬é‡åŒ–â†’é‡å»ºçš„å®Œæ•´æµç¨‹
    """
    
    def __init__(self, 
                 model_name: str = "cvssp/audioldm2-music",
                 codebook_size: int = 256):
        """
        åˆå§‹åŒ–åŸºäºè¾“å…¥çš„DDCMç®¡é“
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸµ åˆå§‹åŒ–åŸºäºè¾“å…¥éŸ³é¢‘çš„DDCMç®¡é“")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {codebook_size}")
        
        # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        # åˆ›å»ºDDCMç æœ¬
        self.ddcm_codebook = AudioDDCMCodebook(
            codebook_size=codebook_size,
            latent_shape=(8, 250, 16)
        ).to(self.device)
        
        print(f"âœ… åŸºäºè¾“å…¥çš„DDCMç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    def process_input_audio(self, audio_path: str, prompt: str = "high quality music") -> Dict:
        """
        å¤„ç†è¾“å…¥éŸ³é¢‘çš„å®Œæ•´DDCMæµç¨‹
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            prompt: é‡å»ºæ—¶çš„æ–‡æœ¬æç¤º
            
        Returns:
            result: åŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸
        """
        print(f"ğŸµ DDCMè¾“å…¥éŸ³é¢‘å¤„ç†: {Path(audio_path).name}")
        print(f"   ğŸ“ é‡å»ºæç¤º: {prompt}")
        
        # 1. åŠ è½½å’Œé¢„å¤„ç†è¾“å…¥éŸ³é¢‘
        input_latent = self._encode_input_audio(audio_path)
        
        # 2. é‡åŒ–åˆ°ç æœ¬
        quantized_latent, indices, distances = self.ddcm_codebook.quantize_latent(input_latent)
        
        # 3. è®¡ç®—å‹ç¼©ä¿¡æ¯
        compression_ratio = input_latent.numel() * 4 / (len(indices) * 4)  # float32 vs int32
        
        print(f"   ğŸ“Š DDCMé‡åŒ–ç»“æœ:")
        print(f"   - ç æœ¬ç´¢å¼•: {indices.cpu().tolist()}")
        print(f"   - å¹³å‡è·ç¦»: {distances.mean():.4f}")
        print(f"   - å‹ç¼©æ¯”: {compression_ratio:.2f}:1")
        
        # 4. ä¸‰ç§é‡å»ºæ–¹æ³•å¯¹æ¯”
        results = {}
        
        # æ–¹æ³•1: ç›´æ¥VAEé‡å»ºï¼ˆåŸå§‹latentï¼‰
        results['original_vae'] = self._reconstruct_with_vae(input_latent, "Original_VAE")
        
        # æ–¹æ³•2: é‡åŒ–VAEé‡å»ºï¼ˆé‡åŒ–latentï¼‰
        results['quantized_vae'] = self._reconstruct_with_vae(quantized_latent, "Quantized_VAE")
        
        # æ–¹æ³•3: DDCM diffusioné‡å»ºï¼ˆä½¿ç”¨é‡åŒ–latentä½œä¸ºæ¡ä»¶ï¼‰
        results['ddcm_diffusion'] = self._reconstruct_with_ddcm_diffusion(
            quantized_latent, prompt, "DDCM_Diffusion"
        )
        
        # 5. åŠ è½½åŸå§‹éŸ³é¢‘ä½œä¸ºå‚è€ƒ
        original_audio, sr = torchaudio.load(audio_path)
        if sr != 16000:
            resampler = torchaudio.transforms.Resample(sr, 16000)
            original_audio = resampler(original_audio)
        
        if original_audio.shape[0] > 1:
            original_audio = original_audio.mean(dim=0, keepdim=True)
        
        original_audio = original_audio.squeeze().numpy()
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘
        output_dir = Path("ddcm_input_based_output")
        output_dir.mkdir(exist_ok=True)
        timestamp = int(time.time())
        
        original_path = output_dir / f"original_{timestamp}.wav"
        sf.write(str(original_path), original_audio, 16000)
        
        # 6. è´¨é‡åˆ†æ
        analysis = self._analyze_reconstruction_quality(original_audio, results)
        
        # 7. æ•´åˆç»“æœ
        final_result = {
            "input_file": audio_path,
            "prompt": prompt,
            "compression_ratio": compression_ratio,
            "codebook_indices": indices.cpu().tolist(),
            "quantization_distance": distances.mean().item(),
            "original_audio_path": str(original_path),
            "reconstructions": results,
            "quality_analysis": analysis,
            "codebook_usage": {
                "used_codes": (self.ddcm_codebook.usage_count > 0).sum().item(),
                "total_codes": self.ddcm_codebook.codebook_size,
            }
        }
        
        # 8. æ˜¾ç¤ºç»“æœ
        self._display_results(final_result)
        
        return final_result
    
    def _encode_input_audio(self, audio_path: str) -> torch.Tensor:
        """å°†è¾“å…¥éŸ³é¢‘ç¼–ç ä¸ºlatentè¡¨ç¤º"""
        print(f"   ğŸ”„ ç¼–ç è¾“å…¥éŸ³é¢‘...")
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = torchaudio.load(audio_path)
        if sr != 48000:  # AudioLDM2ä½¿ç”¨48kHz
            resampler = torchaudio.transforms.Resample(sr, 48000)
            audio = resampler(audio)
        
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # é™åˆ¶é•¿åº¦åˆ°10ç§’
        max_length = 48000 * 10
        if audio.shape[-1] > max_length:
            audio = audio[..., :max_length]
        
        with torch.no_grad():
            audio_np = audio.squeeze(0).numpy()
            
            # ä½¿ç”¨ClapFeatureExtractor
            inputs = self.pipeline.feature_extractor(
                audio_np,
                sampling_rate=48000,
                return_tensors="pt"
            )
            
            mel_features = inputs["input_features"].to(self.device)
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            # VAEç¼–ç 
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode()
            latent = latent * self.pipeline.vae.config.scaling_factor
            
            print(f"   âœ… ç¼–ç å®Œæˆ: {latent.shape}")
            return latent
    
    def _reconstruct_with_vae(self, latent: torch.Tensor, method_name: str) -> Dict:
        """ä½¿ç”¨VAEé‡å»ºéŸ³é¢‘"""
        print(f"   ğŸ”§ {method_name}é‡å»º...")
        
        start_time = time.time()
        
        with torch.no_grad():
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…VAEçš„æœŸæœ›ç±»å‹
            latent_for_decode = latent / self.pipeline.vae.config.scaling_factor
            
            # è·å–VAEæœŸæœ›çš„æ•°æ®ç±»å‹
            vae_dtype = next(self.pipeline.vae.parameters()).dtype
            latent_for_decode = latent_for_decode.to(vae_dtype)
            
            mel_spectrogram = self.pipeline.vae.decode(latent_for_decode).sample
            
            # ä½¿ç”¨vocoder
            audio_tensor = self.pipeline.mel_spectrogram_to_waveform(mel_spectrogram)
            audio = audio_tensor.squeeze().cpu().numpy()
        
        reconstruction_time = time.time() - start_time
        
        # ä¿å­˜éŸ³é¢‘
        output_dir = Path("ddcm_input_based_output")
        output_dir.mkdir(exist_ok=True)
        timestamp = int(time.time())
        
        output_path = output_dir / f"{method_name}_{timestamp}.wav"
        sf.write(str(output_path), audio, 16000)
        
        return {
            "method": method_name,
            "audio": audio,
            "output_path": str(output_path),
            "reconstruction_time": reconstruction_time
        }
    
    def _reconstruct_with_ddcm_diffusion(self, 
                                       quantized_latent: torch.Tensor, 
                                       prompt: str, 
                                       method_name: str) -> Dict:
        """ä½¿ç”¨DDCM guided diffusioné‡å»ºéŸ³é¢‘"""
        print(f"   ğŸ¯ {method_name}é‡å»º...")
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨é‡åŒ–latentä½œä¸ºåˆå§‹å™ªå£°è¿›è¡Œdiffusion
            with torch.no_grad():
                result = self.pipeline(
                    prompt=prompt,
                    num_inference_steps=15,
                    guidance_scale=7.5,
                    audio_length_in_s=10.0,
                    latents=quantized_latent  # å…³é”®ï¼šä½¿ç”¨é‡åŒ–latentä½œä¸ºèµ·ç‚¹
                )
                audio = result.audios[0]
                
        except Exception as e:
            print(f"   âš ï¸ DDCM diffusionå¤±è´¥ï¼Œå›é€€åˆ°VAE: {e}")
            return self._reconstruct_with_vae(quantized_latent, f"{method_name}_Fallback")
        
        reconstruction_time = time.time() - start_time
        
        # ä¿å­˜éŸ³é¢‘
        output_dir = Path("ddcm_input_based_output")
        output_dir.mkdir(exist_ok=True)
        timestamp = int(time.time())
        
        output_path = output_dir / f"{method_name}_{timestamp}.wav"
        sf.write(str(output_path), audio, 16000)
        
        return {
            "method": method_name,
            "audio": audio,
            "output_path": str(output_path),
            "reconstruction_time": reconstruction_time
        }
    
    def _analyze_reconstruction_quality(self, original_audio: np.ndarray, results: Dict) -> Dict:
        """åˆ†æé‡å»ºè´¨é‡"""
        print(f"   ğŸ“Š è´¨é‡åˆ†æ...")
        
        analysis = {}
        
        for method, result in results.items():
            recon_audio = result["audio"]
            
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            min_len = min(len(original_audio), len(recon_audio))
            orig = original_audio[:min_len]
            recon = recon_audio[:min_len]
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            mse = np.mean((orig - recon) ** 2)
            snr = 10 * np.log10(np.mean(orig ** 2) / (mse + 1e-10))
            correlation = np.corrcoef(orig, recon)[0, 1] if min_len > 1 else 0
            mae = np.mean(np.abs(orig - recon))
            
            # é«˜é¢‘åˆ†æ
            if min_len > 8192:
                orig_spec = np.abs(np.fft.fft(orig[:8192]))[:4096]
                recon_spec = np.abs(np.fft.fft(recon[:8192]))[:4096]
                
                high_freq_orig = np.sum(orig_spec[2048:])
                high_freq_recon = np.sum(recon_spec[2048:])
                
                high_freq_retention = high_freq_recon / (high_freq_orig + 1e-10)
            else:
                high_freq_retention = 0
            
            analysis[method] = {
                "snr": snr,
                "correlation": correlation,
                "mse": mse,
                "mae": mae,
                "high_freq_retention": high_freq_retention,
                "reconstruction_time": result["reconstruction_time"]
            }
        
        return analysis
    
    def _display_results(self, result: Dict):
        """æ˜¾ç¤ºç»“æœ"""
        print(f"\n{'='*70}")
        print(f"ğŸ¯ DDCMåŸºäºè¾“å…¥éŸ³é¢‘çš„å¤„ç†ç»“æœ")
        print(f"{'='*70}")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {result['input_file']}")
        print(f"ğŸ“ é‡å»ºæç¤º: {result['prompt']}")
        print(f"ğŸ—œï¸ å‹ç¼©æ¯”: {result['compression_ratio']:.2f}:1")
        print(f"ğŸ“š ç æœ¬ä½¿ç”¨: {result['codebook_usage']['used_codes']}/{result['codebook_usage']['total_codes']}")
        
        print(f"\nğŸ“Š é‡å»ºæ–¹æ³•å¯¹æ¯”:")
        print(f"{'æ–¹æ³•':<20} {'SNR(dB)':<10} {'ç›¸å…³æ€§':<10} {'é«˜é¢‘ä¿æŒ':<10} {'æ—¶é—´(s)':<10}")
        print("-" * 70)
        
        for method, analysis in result['quality_analysis'].items():
            print(f"{method:<20} {analysis['snr']:<10.2f} {analysis['correlation']:<10.4f} "
                  f"{analysis['high_freq_retention']:<10.3f} {analysis['reconstruction_time']:<10.2f}")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   åŸå§‹: {result['original_audio_path']}")
        for method, recon in result['reconstructions'].items():
            print(f"   {method}: {recon['output_path']}")
        
        # æ¨èæœ€ä½³æ–¹æ³•
        best_method = max(result['quality_analysis'].items(), 
                         key=lambda x: x[1]['snr'] + x[1]['correlation'] * 10)
        
        print(f"\nğŸ† æ¨èæ–¹æ³•: {best_method[0]}")
        print(f"   SNR: {best_method[1]['snr']:.2f} dB")
        print(f"   ç›¸å…³æ€§: {best_method[1]['correlation']:.4f}")
        
        if "ddcm_diffusion" in best_method[0].lower():
            print(f"   ğŸ‰ DDCM diffusionè¡¨ç°æœ€ä½³ï¼")
            print(f"   ğŸ’¡ è¿™è¯´æ˜åŸºäºé‡åŒ–latentçš„diffusionç¡®å®ä¸è¾“å…¥éŸ³é¢‘ç›¸å…³")
        elif "quantized" in best_method[0].lower():
            print(f"   âœ… é‡åŒ–VAEé‡å»ºæ•ˆæœè‰¯å¥½")
            print(f"   ğŸ’¡ ç æœ¬é‡åŒ–ä¿æŒäº†è¾“å…¥éŸ³é¢‘çš„å…³é”®ç‰¹å¾")
        else:
            print(f"   ğŸ’¡ åŸå§‹VAEé‡å»ºä»æ˜¯æœ€ä½³é€‰æ‹©")
        
        print(f"\nğŸ” å…³é”®å‘ç°:")
        print(f"   ğŸµ åŸå§‹éŸ³é¢‘å®Œå…¨é‡å»º: SNR {result['quality_analysis']['original_vae']['snr']:.2f}dB")
        print(f"   ğŸ“š ç æœ¬é‡åŒ–é‡å»º: SNR {result['quality_analysis']['quantized_vae']['snr']:.2f}dB")
        print(f"   ğŸ¯ DDCM diffusioné‡å»º: SNR {result['quality_analysis']['ddcm_diffusion']['snr']:.2f}dB")
        
        # è®¡ç®—é‡åŒ–æŸå¤±
        snr_loss = result['quality_analysis']['original_vae']['snr'] - result['quality_analysis']['quantized_vae']['snr']
        print(f"   ğŸ“‰ é‡åŒ–å¼•èµ·çš„è´¨é‡æŸå¤±: {snr_loss:.2f}dB")
        
        if snr_loss < 3:
            print(f"   âœ… é‡åŒ–æŸå¤±å¾ˆå°ï¼Œç æœ¬è¡¨ç¤ºéå¸¸æœ‰æ•ˆ")
        elif snr_loss < 10:
            print(f"   âš ï¸ é‡åŒ–æœ‰ä¸€å®šæŸå¤±ï¼Œä½†ä»å¯æ¥å—")
        else:
            print(f"   âŒ é‡åŒ–æŸå¤±è¾ƒå¤§ï¼Œéœ€è¦ä¼˜åŒ–ç æœ¬")

def demo_input_based_ddcm():
    """æ¼”ç¤ºåŸºäºè¾“å…¥éŸ³é¢‘çš„DDCM"""
    print("ğŸ¯ åŸºäºè¾“å…¥éŸ³é¢‘çš„DDCMæ¼”ç¤º")
    print("=" * 50)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        print("è¯·ç¡®ä¿AudioLDM2_Music_output.wavåœ¨å½“å‰ç›®å½•ä¸­")
        return
    
    # åˆå§‹åŒ–DDCMç®¡é“
    ddcm_pipeline = AudioLDM2_InputBased_DDCM(
        model_name="cvssp/audioldm2-music",
        codebook_size=128  # è¾ƒå°çš„ç æœ¬ç”¨äºæ¼”ç¤º
    )
    
    # å¤„ç†è¾“å…¥éŸ³é¢‘
    result = ddcm_pipeline.process_input_audio(
        audio_path=input_file,
        prompt="high quality instrumental music with rich harmonics"
    )
    
    print(f"\nâœ… åŸºäºè¾“å…¥éŸ³é¢‘çš„DDCMæ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ’¡ ç°åœ¨ç”Ÿæˆçš„éŸ³é¢‘ç¡®å®ä¸è¾“å…¥æ–‡ä»¶ç›¸å…³")
    print(f"ğŸ“Š å‹ç¼©æ¯”: {result['compression_ratio']:.2f}:1")
    print(f"\nğŸ¯ æ ¸å¿ƒéªŒè¯:")
    print(f"   1. Original VAE: ç›´æ¥VAEé‡å»ºè¾“å…¥éŸ³é¢‘")
    print(f"   2. Quantized VAE: ç”¨ç æœ¬é‡åŒ–åçš„latenté‡å»º")
    print(f"   3. DDCM Diffusion: ç”¨é‡åŒ–latentå¼•å¯¼diffusionç”Ÿæˆ")
    print(f"\nå¦‚æœquantized_vaeå’Œddcm_diffusionçš„ç›¸å…³æ€§éƒ½è¾ƒé«˜ï¼Œ")
    print(f"åˆ™è¯´æ˜ç”Ÿæˆçš„éŸ³é¢‘ç¡®å®ä¸è¾“å…¥éŸ³é¢‘ç›¸å…³ï¼")

if __name__ == "__main__":
    demo_input_based_ddcm()
