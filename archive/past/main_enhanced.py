from New_pipeline_audioldm2 import AudioLDM2Pipeline
import torch
import scipy
import time

def main():
    print("ğŸµ ä½¿ç”¨è‡ªå®šä¹‰ AudioLDM2 Pipeline ç”ŸæˆéŸ³é¢‘")
    print("=" * 50)
    
    # AudioLDM2 å˜ä½“é€‰æ‹©
    checkpoints = {
        "1": {
            "name": "AudioLDM2 (æ ‡å‡†ç‰ˆ)",
            "repo_id": "cvssp/audioldm2",
            "task": "æ–‡æœ¬è½¬éŸ³é¢‘",
            "unet_size": "350M",
            "total_size": "1.1B",
            "training_data": "1150kå°æ—¶"
        },
        "2": {
            "name": "AudioLDM2-Large (å¤§å‹ç‰ˆ)",
            "repo_id": "cvssp/audioldm2-large",
            "task": "æ–‡æœ¬è½¬éŸ³é¢‘",
            "unet_size": "750M", 
            "total_size": "1.5B",
            "training_data": "1150kå°æ—¶"
        },
        "3": {
            "name": "AudioLDM2-Music (éŸ³ä¹ä¸“ç”¨)",
            "repo_id": "cvssp/audioldm2-music",
            "task": "æ–‡æœ¬åˆ°éŸ³ä¹",
            "unet_size": "350M",
            "total_size": "1.1B", 
            "training_data": "665kå°æ—¶"
        },
        "4": {
            "name": "AudioLDM2-GigaSpeech (è¯­éŸ³)",
            "repo_id": "anhnct/audioldm2_gigaspeech",
            "task": "æ–‡æœ¬è½¬è¯­éŸ³",
            "unet_size": "350M",
            "total_size": "1.1B",
            "training_data": "10kå°æ—¶"
        }
    }
    
    print("ğŸ“‹ å¯ç”¨çš„ AudioLDM2 å˜ä½“:")
    for key, info in checkpoints.items():
        print(f"  {key}. {info['name']}")
        print(f"     ä»»åŠ¡: {info['task']} | UNet: {info['unet_size']} | æ€»å¤§å°: {info['total_size']}")
        print(f"     è®­ç»ƒæ•°æ®: {info['training_data']}")
        print()
    
    # é€‰æ‹©æ¨¡å‹ (é»˜è®¤ä½¿ç”¨æ ‡å‡†ç‰ˆ)
    choice = "3"  # å¯ä»¥ä¿®æ”¹è¿™é‡Œæ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹
    selected_model = checkpoints[choice]
    repo_id = selected_model["repo_id"]
    
    print(f"ğŸ¯ å·²é€‰æ‹©: {selected_model['name']}")
    print(f"   Checkpoint: {repo_id}")
    print(f"   ä¸“ç”¨ä»»åŠ¡: {selected_model['task']}")
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½è‡ªå®šä¹‰ pipeline
    print("æ­£åœ¨åŠ è½½è‡ªå®šä¹‰ AudioLDM2 Pipeline...")
    
    start_time = time.time()
    
    if device == "cuda":
        pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
        pipe = pipe.to("cuda")
    else:
        pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float32)
        pipe = pipe.to("cpu")
    
    load_time = time.time() - start_time
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f}ç§’)")
      # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    print(f"\nğŸ”§ æ¨¡å‹ä¿¡æ¯:")
    print(f"  - Checkpoint: {repo_id}")
    print(f"  - æ¨¡å‹ç±»å‹: {type(pipe).__name__}")
    print(f"  - ä¸“ç”¨ä»»åŠ¡: {selected_model['task']}")
    print(f"  - UNet å‚æ•°é‡: {sum(p.numel() for p in pipe.unet.parameters()) / 1e6:.1f}M")
    print(f"  - VAE å‚æ•°é‡: {sum(p.numel() for p in pipe.vae.parameters()) / 1e6:.1f}M")
    print(f"  - Text Encoder ç±»å‹: {type(pipe.text_encoder).__name__}")
    print(f"  - Text Encoder 2 ç±»å‹: {type(pipe.text_encoder_2).__name__}")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„æç¤ºè¯
    if "music" in repo_id.lower():
        prompt = "Upbeat electronic dance music with synthesizers and drum beats"
        transcription = None
    elif "gigaspeech" in repo_id.lower() or "speech" in selected_model['task'].lower():
        prompt = "A female speaker with clear pronunciation"
        transcription = "Hello, this is a test of text to speech generation."
    else:
        prompt = "Techno music with a strong, upbeat tempo and high melodic riffs."
        transcription = None
      # éŸ³é¢‘ç”Ÿæˆå‚æ•°
    num_inference_steps = 200
    audio_length_in_s = 10.0
    
    print(f"\nğŸ“ å‚æ•°è®¾ç½®:")
    print(f"  - æç¤ºè¯: {prompt}")
    if transcription:
        print(f"  - è½¬å½•æ–‡æœ¬: {transcription}")
    print(f"  - æ¨ç†æ­¥æ•°: {num_inference_steps}")
    print(f"  - éŸ³é¢‘é•¿åº¦: {audio_length_in_s}ç§’")
    print(f"  - æ•°æ®ç±»å‹: {pipe.unet.dtype}")
    
    # ç”ŸæˆéŸ³é¢‘
    print("\næ­£åœ¨ç”ŸæˆéŸ³é¢‘...")
    gen_start_time = time.time()
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ä¸åŒçš„å‚æ•°
    if transcription:
        audio = pipe(
            prompt,
            transcription=transcription,
            num_inference_steps=num_inference_steps,
            audio_length_in_s=audio_length_in_s,
            max_new_tokens=512  # TTS æ¨¡å‹éœ€è¦è¿™ä¸ªå‚æ•°
        ).audios[0]
    else:
        audio = pipe(
            prompt, 
            num_inference_steps=num_inference_steps,
            audio_length_in_s=audio_length_in_s
        ).audios[0]
      gen_time = time.time() - gen_start_time
    print(f"âœ“ éŸ³é¢‘ç”Ÿæˆå®Œæˆ (è€—æ—¶: {gen_time:.2f}ç§’)")
    
    # ä¿å­˜éŸ³é¢‘
    model_name = selected_model['name'].split('(')[0].strip().replace(' ', '_').replace('-', '_')
    output_file = f"{model_name}_output.wav"
    scipy.io.wavfile.write(output_file, rate=16000, data=audio)
    print(f"âœ“ éŸ³é¢‘å·²ä¿å­˜åˆ°: {output_file}")
    
    # æ€§èƒ½ç»Ÿè®¡
    total_time = load_time + gen_time
    print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    print(f"  - æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
    print(f"  - éŸ³é¢‘ç”Ÿæˆæ—¶é—´: {gen_time:.2f}ç§’")
    print(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"  - ç”Ÿæˆé€Ÿåº¦: {audio_length_in_s/gen_time:.2f}x å®æ—¶é€Ÿåº¦")

if __name__ == "__main__":
    main()
