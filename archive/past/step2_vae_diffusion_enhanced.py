#!/usr/bin/env python3
"""
Step 2: VAE + Diffusionå¢å¼ºé‡å»º
åœ¨VAEç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´åŠ å…¥diffusionè¿‡ç¨‹ï¼Œä¸“é—¨ç”¨äºå¢å¼ºé«˜é¢‘å†…å®¹
ç›®æ ‡ï¼šè§£å†³VAEé«˜é¢‘æŸå¤±é—®é¢˜ï¼Œæå‡éŸ³é¢‘é‡å»ºè´¨é‡

æµç¨‹ï¼š
1. VAEç¼–ç éŸ³é¢‘åˆ°latentç©ºé—´
2. ä½¿ç”¨diffusionè¿‡ç¨‹å¢å¼ºlatentè¡¨ç¤ºï¼ˆç‰¹åˆ«æ˜¯é«˜é¢‘å¯¹åº”çš„éƒ¨åˆ†ï¼‰
3. VAEè§£ç å¢å¼ºåçš„latentåˆ°éŸ³é¢‘
4. å¯¹æ¯”åˆ†æVAE-only vs VAE+Diffusionçš„æ•ˆæœ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import librosa
from pathlib import Path
import time
from diffusers import AudioLDM2Pipeline, DDPMScheduler
from typing import Dict, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

class VAEDiffusionEnhancer:
    """
    VAE + Diffusionå¢å¼ºå™¨
    åœ¨VAEçš„latentç©ºé—´ä¸­ä½¿ç”¨diffusionæ¥å¢å¼ºç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯é«˜é¢‘ä¿¡æ¯
    """
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """åˆå§‹åŒ–VAE+Diffusionå¢å¼ºå™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸš€ åˆå§‹åŒ–VAE+Diffusionå¢å¼ºå™¨")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        print(f"   ğŸ¤– æ¨¡å‹: {model_name}")
        
        # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        # è®¾ç½®diffusionè°ƒåº¦å™¨
        self.scheduler = DDPMScheduler.from_config(self.pipeline.scheduler.config)
        
        print(f"âœ… VAE+Diffusionå¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š VAE scaling_factor: {self.pipeline.vae.config.scaling_factor}")
        print(f"   ğŸ”„ Diffusionè°ƒåº¦å™¨: {type(self.scheduler).__name__}")
        print(f"   ğŸ“ˆ Diffusionæ­¥æ•°: {self.scheduler.num_train_timesteps}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("vae_diffusion_enhanced")
        self.output_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def enhance_reconstruction(self, audio_path: str, 
                            diffusion_steps: int = 20,
                            guidance_scale: float = 3.0,
                            high_freq_boost: float = 1.5) -> Dict:
        """
        æ‰§è¡ŒVAE+Diffusionå¢å¼ºé‡å»º
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            diffusion_steps: diffusionå»å™ªæ­¥æ•°
            guidance_scale: å¼•å¯¼å°ºåº¦
            high_freq_boost: é«˜é¢‘å¢å¼ºç³»æ•°
            
        Returns:
            åŒ…å«é‡å»ºç»“æœå’Œåˆ†æçš„å­—å…¸
        """
        print(f"\nğŸ”„ å¼€å§‹VAE+Diffusionå¢å¼ºé‡å»º: {Path(audio_path).name}")
        print(f"   ğŸ”¢ Diffusionæ­¥æ•°: {diffusion_steps}")
        print(f"   ğŸ¯ å¼•å¯¼å°ºåº¦: {guidance_scale}")
        print(f"   ğŸ¼ é«˜é¢‘å¢å¼º: {high_freq_boost}x")
        
        # 1. åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘
        original_audio, processed_audio = self._load_and_preprocess_audio(audio_path)
        
        # 2. VAEç¼–ç åˆ°latentç©ºé—´
        latent = self._encode_audio(processed_audio)
        
        # 3. åˆ›å»ºå¯¹æ¯”ï¼šæ™®é€šVAEé‡å»º
        vae_only_audio = self._decode_audio(latent.clone())
        
        # 4. Diffusionå¢å¼ºlatent
        enhanced_latent = self._enhance_latent_with_diffusion(
            latent, 
            diffusion_steps=diffusion_steps,
            guidance_scale=guidance_scale,
            high_freq_boost=high_freq_boost
        )
        
        # 5. VAEè§£ç å¢å¼ºåçš„latent
        enhanced_audio = self._decode_audio(enhanced_latent)
        
        # 6. ä¿å­˜ç»“æœ
        timestamp = int(time.time())
        paths = self._save_audio_results(
            original_audio, vae_only_audio, enhanced_audio, timestamp
        )
        
        # 7. è´¨é‡å¯¹æ¯”åˆ†æ
        quality_comparison = self._compare_quality(
            original_audio, vae_only_audio, enhanced_audio
        )
        
        # 8. é¢‘ç‡å¯¹æ¯”åˆ†æ
        frequency_comparison = self._compare_frequency_content(
            original_audio, vae_only_audio, enhanced_audio
        )
        
        # 9. å¯è§†åŒ–åˆ†æ
        self._create_comparison_visualizations(
            original_audio, vae_only_audio, enhanced_audio, timestamp
        )
        
        # 10. æ•´åˆç»“æœ
        result = {
            "input_file": audio_path,
            "timestamp": timestamp,
            "parameters": {
                "diffusion_steps": diffusion_steps,
                "guidance_scale": guidance_scale,
                "high_freq_boost": high_freq_boost
            },
            "paths": paths,
            "quality_comparison": quality_comparison,
            "frequency_comparison": frequency_comparison,
            "latent_shapes": {
                "original": latent.shape,
                "enhanced": enhanced_latent.shape
            },
            "processing_device": str(self.device)
        }
        
        # 11. æ˜¾ç¤ºç»“æœ
        self._display_comparison_results(result)
        
        return result
    
    def _load_and_preprocess_audio(self, audio_path: str) -> Tuple[np.ndarray, torch.Tensor]:
        """åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘"""
        print(f"   ğŸ“‚ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
        
        # åŠ è½½åŸå§‹éŸ³é¢‘
        original_audio, sr = torchaudio.load(audio_path)
        
        # è½¬æ¢ä¸ºå•å£°é“
        if original_audio.shape[0] > 1:
            original_audio = original_audio.mean(dim=0, keepdim=True)
        
        # é‡é‡‡æ ·åˆ°48kHz
        if sr != 48000:
            print(f"   ğŸ”„ é‡é‡‡æ ·: {sr}Hz -> 48000Hz")
            resampler = torchaudio.transforms.Resample(sr, 48000)
            processed_audio = resampler(original_audio)
        else:
            processed_audio = original_audio.clone()
        
        # é™åˆ¶é•¿åº¦åˆ°10ç§’
        max_length = 48000 * 10
        if processed_audio.shape[-1] > max_length:
            print(f"   âœ‚ï¸ æˆªå–å‰10ç§’")
            processed_audio = processed_audio[..., :max_length]
        
        # è½¬æ¢ä¸ºnumpyç”¨äºæœ€ç»ˆè¾“å‡º
        original_audio_np = original_audio.squeeze().numpy()
        
        print(f"   âœ… éŸ³é¢‘é¢„å¤„ç†å®Œæˆ: {processed_audio.shape}")
        
        return original_audio_np, processed_audio
    
    def _encode_audio(self, audio: torch.Tensor) -> torch.Tensor:
        """ä½¿ç”¨VAEç¼–ç éŸ³é¢‘åˆ°latentç©ºé—´"""
        print(f"   ğŸ”— VAEç¼–ç ...")
        
        with torch.no_grad():
            # è½¬æ¢ä¸ºnumpyå¹¶ä½¿ç”¨feature extractor
            audio_np = audio.squeeze().numpy()
            
            # ä½¿ç”¨ClapFeatureExtractorå¤„ç†éŸ³é¢‘
            inputs = self.pipeline.feature_extractor(
                audio_np,
                sampling_rate=48000,
                return_tensors="pt"
            )
            
            # è·å–melé¢‘è°±ç‰¹å¾
            mel_features = inputs["input_features"].to(self.device)
            
            # ç¡®ä¿æ˜¯4Då¼ é‡
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            # åŒ¹é…VAEçš„æ•°æ®ç±»å‹
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            # VAEç¼–ç 
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode()
            
            # åº”ç”¨scaling factor
            latent = latent * self.pipeline.vae.config.scaling_factor
            
            print(f"   âœ… ç¼–ç å®Œæˆ: {mel_features.shape} -> {latent.shape}")
            
            return latent
    
    def _enhance_latent_with_diffusion(self, 
                                     latent: torch.Tensor,
                                     diffusion_steps: int = 20,
                                     guidance_scale: float = 3.0,
                                     high_freq_boost: float = 1.5) -> torch.Tensor:
        """
        ä½¿ç”¨diffusionè¿‡ç¨‹å¢å¼ºlatentè¡¨ç¤º
        ç‰¹åˆ«é’ˆå¯¹é«˜é¢‘ä¿¡æ¯è¿›è¡Œå¢å¼º
        """
        print(f"   ğŸŒŠ Diffusionå¢å¼ºlatent...")
        
        with torch.no_grad():
            batch_size = latent.shape[0]
            
            # è®¾ç½®è°ƒåº¦å™¨çš„æ—¶é—´æ­¥
            self.scheduler.set_timesteps(diffusion_steps)
            
            # æ·»åŠ å™ªå£°åˆ°latentï¼ˆæ¨¡æ‹Ÿdiffusionçš„forwardè¿‡ç¨‹ï¼‰
            # ä½¿ç”¨è¾ƒå°‘çš„å™ªå£°ï¼Œå› ä¸ºæˆ‘ä»¬ä¸æƒ³å®Œå…¨ç ´åVAEå­¦åˆ°çš„è¡¨ç¤º
            noise_level = 0.3  # ç›¸å¯¹è¾ƒå°çš„å™ªå£°æ°´å¹³
            noise = torch.randn_like(latent) * noise_level
            
            # é€‰æ‹©ä¸€ä¸ªä¸­ç­‰çš„æ—¶é—´æ­¥ï¼ˆä¸æ˜¯å®Œå…¨éšæœºå™ªå£°ï¼‰
            timestep_idx = len(self.scheduler.timesteps) // 3  # ä½¿ç”¨1/3å¤„çš„æ—¶é—´æ­¥
            timestep = self.scheduler.timesteps[timestep_idx:timestep_idx+1]
            
            # æ·»åŠ å™ªå£°
            noisy_latent = self.scheduler.add_noise(latent, noise, timestep)
            
            print(f"   ğŸ”¸ å™ªå£°æ°´å¹³: {noise_level}")
            print(f"   â° èµ·å§‹æ—¶é—´æ­¥: {timestep.item()}")
            
            # å‡†å¤‡è¾“å…¥åˆ°UNet
            latent_model_input = noisy_latent
            
            # åˆ›å»ºæ— æ¡ä»¶è¾“å…¥ï¼ˆç”¨äºclassifier-free guidanceï¼‰
            uncond_embeddings = torch.zeros(
                (batch_size, 77, 1024),  # AudioLDM2çš„æ–‡æœ¬åµŒå…¥ç»´åº¦
                device=self.device,
                dtype=latent.dtype
            )
            
            # åˆ›å»ºæ¡ä»¶è¾“å…¥ï¼ˆåŸºäºåŸå§‹latentçš„ç‰¹å¾ï¼‰
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨åŸå§‹latentçš„ç»Ÿè®¡ä¿¡æ¯ä½œä¸º"æ¡ä»¶"
            cond_embeddings = self._create_latent_conditioning(latent)
            
            # Diffusionå»å™ªè¿‡ç¨‹
            enhanced_latent = latent_model_input.clone()
            
            for i, t in enumerate(self.scheduler.timesteps[timestep_idx:]):
                # å‡†å¤‡è¾“å…¥
                latent_input = enhanced_latent
                
                # å¦‚æœä½¿ç”¨guidance
                if guidance_scale > 1.0:
                    # æ¡ä»¶å’Œæ— æ¡ä»¶é¢„æµ‹
                    latent_input_expanded = torch.cat([latent_input] * 2)
                    text_embeddings = torch.cat([uncond_embeddings, cond_embeddings])
                else:
                    latent_input_expanded = latent_input
                    text_embeddings = cond_embeddings
                
                # UNeté¢„æµ‹å™ªå£°
                with torch.no_grad():
                    noise_pred = self.pipeline.unet(
                        latent_input_expanded,
                        t,
                        encoder_hidden_states=text_embeddings,
                        return_dict=False
                    )[0]
                
                # åº”ç”¨classifier-free guidance
                if guidance_scale > 1.0:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
                
                # è°ƒåº¦å™¨å»å™ªæ­¥éª¤
                enhanced_latent = self.scheduler.step(
                    noise_pred, t, enhanced_latent, return_dict=False
                )[0]
                
                # é«˜é¢‘å¢å¼ºï¼ˆåœ¨latentç©ºé—´ä¸­ï¼‰
                enhanced_latent = self._apply_high_freq_boost(
                    enhanced_latent, latent, high_freq_boost
                )
            
            print(f"   âœ… Diffusionå¢å¼ºå®Œæˆ: {diffusion_steps}æ­¥")
            
            return enhanced_latent
    
    def _create_latent_conditioning(self, latent: torch.Tensor) -> torch.Tensor:
        """
        åŸºäºlatentåˆ›å»ºæ¡ä»¶åµŒå…¥
        ä½¿ç”¨latentçš„ç»Ÿè®¡ç‰¹å¾ä½œä¸ºæ¡ä»¶ä¿¡æ¯
        """
        batch_size = latent.shape[0]
        
        # è®¡ç®—latentçš„ç»Ÿè®¡ç‰¹å¾
        latent_mean = latent.mean(dim=[2, 3], keepdim=True)  # [B, C, 1, 1]
        latent_std = latent.std(dim=[2, 3], keepdim=True)    # [B, C, 1, 1]
        latent_max = latent.max(dim=3, keepdim=True)[0].max(dim=2, keepdim=True)[0]  # [B, C, 1, 1]
        
        # ç»„åˆç»Ÿè®¡ç‰¹å¾
        stats_features = torch.cat([latent_mean, latent_std, latent_max], dim=1)  # [B, 3*C, 1, 1]
        
        # å±•å¹³å¹¶æŠ•å½±åˆ°æ–‡æœ¬åµŒå…¥ç»´åº¦
        stats_flat = stats_features.flatten(1)  # [B, 3*C]
        
        # åˆ›å»ºç®€å•çš„çº¿æ€§æŠ•å½±åˆ°æ–‡æœ¬åµŒå…¥ç©ºé—´
        embed_dim = 1024
        seq_len = 77
        
        # é‡å¤å’ŒæŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        conditioning = stats_flat.unsqueeze(1).repeat(1, seq_len, 1)  # [B, 77, 3*C]
        
        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œå¡«å……æˆ–æˆªæ–­
        if conditioning.shape[-1] < embed_dim:
            padding = torch.zeros(
                batch_size, seq_len, embed_dim - conditioning.shape[-1],
                device=conditioning.device, dtype=conditioning.dtype
            )
            conditioning = torch.cat([conditioning, padding], dim=-1)
        elif conditioning.shape[-1] > embed_dim:
            conditioning = conditioning[:, :, :embed_dim]
        
        return conditioning
    
    def _apply_high_freq_boost(self, 
                              enhanced_latent: torch.Tensor,
                              original_latent: torch.Tensor,
                              boost_factor: float) -> torch.Tensor:
        """
        åœ¨latentç©ºé—´ä¸­åº”ç”¨é«˜é¢‘å¢å¼º
        é€šè¿‡åˆ†ælatentçš„ç©ºé—´é¢‘ç‡ç‰¹å¾æ¥å¢å¼ºé«˜é¢‘éƒ¨åˆ†
        """
        if boost_factor <= 1.0:
            return enhanced_latent
        
        # åœ¨latentçš„ç©ºé—´ç»´åº¦ä¸Šåº”ç”¨é«˜é€šæ»¤æ³¢æ¥è¯†åˆ«é«˜é¢‘éƒ¨åˆ†
        # ä½¿ç”¨ç®€å•çš„è¾¹ç¼˜æ£€æµ‹æ ¸æ¥è¯†åˆ«é«˜é¢‘ç‰¹å¾
        high_freq_kernel = torch.tensor([
            [[-1, -1, -1],
             [-1,  8, -1],
             [-1, -1, -1]]
        ], dtype=enhanced_latent.dtype, device=enhanced_latent.device)
        
        high_freq_kernel = high_freq_kernel.unsqueeze(0).unsqueeze(0)  # [1, 1, 3, 3]
        
        # å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«åº”ç”¨é«˜é¢‘æ£€æµ‹
        enhanced_hf = torch.zeros_like(enhanced_latent)
        
        for c in range(enhanced_latent.shape[1]):
            latent_channel = enhanced_latent[:, c:c+1, :, :]  # [B, 1, H, W]
            
            # åº”ç”¨é«˜é¢‘æ£€æµ‹ï¼ˆè¾¹ç¼˜æ£€æµ‹ï¼‰
            hf_response = F.conv2d(
                latent_channel,
                high_freq_kernel,
                padding=1
            )
            
            enhanced_hf[:, c:c+1, :, :] = hf_response
        
        # å¢å¼ºé«˜é¢‘éƒ¨åˆ†
        high_freq_enhancement = enhanced_hf * (boost_factor - 1.0)
        enhanced_result = enhanced_latent + high_freq_enhancement
        
        # ä¿æŒä¸åŸå§‹latentçš„ç›¸ä¼¼æ€§ï¼ˆé¿å…è¿‡åº¦å¢å¼ºï¼‰
        blend_factor = 0.8  # 80%å¢å¼º + 20%åŸå§‹
        final_result = blend_factor * enhanced_result + (1 - blend_factor) * original_latent
        
        return final_result
    
    def _decode_audio(self, latent: torch.Tensor) -> np.ndarray:
        """ä½¿ç”¨VAEè§£ç latentåˆ°éŸ³é¢‘"""
        print(f"   ğŸ”„ VAEè§£ç ...")
        
        with torch.no_grad():
            # åå‘scaling
            latent_for_decode = latent / self.pipeline.vae.config.scaling_factor
            
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            vae_dtype = next(self.pipeline.vae.parameters()).dtype
            latent_for_decode = latent_for_decode.to(vae_dtype)
            
            # VAEè§£ç åˆ°melé¢‘è°±
            mel_spectrogram = self.pipeline.vae.decode(latent_for_decode).sample
            
            # ä½¿ç”¨vocoderè½¬æ¢ä¸ºéŸ³é¢‘æ³¢å½¢
            audio_tensor = self.pipeline.mel_spectrogram_to_waveform(mel_spectrogram)
            audio_np = audio_tensor.squeeze().cpu().numpy()
            
            print(f"   âœ… è§£ç å®Œæˆ: {latent.shape} -> éŸ³é¢‘é•¿åº¦ {len(audio_np)}")
            
            return audio_np
    
    def _save_audio_results(self, 
                          original: np.ndarray,
                          vae_only: np.ndarray,
                          enhanced: np.ndarray,
                          timestamp: int) -> Dict[str, str]:
        """ä¿å­˜éŸ³é¢‘ç»“æœ"""
        print(f"   ğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...")
        
        paths = {}
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘ï¼ˆé‡é‡‡æ ·åˆ°16kHzï¼‰
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        original_path = self.output_dir / f"original_{timestamp}.wav"
        sf.write(str(original_path), original_16k, 16000)
        paths["original"] = str(original_path)
        
        # ä¿å­˜VAE-onlyé‡å»ºéŸ³é¢‘
        vae_only_path = self.output_dir / f"vae_only_{timestamp}.wav"
        sf.write(str(vae_only_path), vae_only, 16000)
        paths["vae_only"] = str(vae_only_path)
        
        # ä¿å­˜VAE+Diffusionå¢å¼ºéŸ³é¢‘
        enhanced_path = self.output_dir / f"vae_diffusion_enhanced_{timestamp}.wav"
        sf.write(str(enhanced_path), enhanced, 16000)
        paths["enhanced"] = str(enhanced_path)
        
        print(f"   âœ… éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜")
        
        return paths
    
    def _compare_quality(self, 
                        original: np.ndarray,
                        vae_only: np.ndarray,
                        enhanced: np.ndarray) -> Dict:
        """å¯¹æ¯”é‡å»ºè´¨é‡"""
        print(f"   ğŸ“Š è´¨é‡å¯¹æ¯”åˆ†æ...")
        
        # é‡é‡‡æ ·åŸå§‹éŸ³é¢‘åˆ°16kHz
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        def calculate_metrics(orig, recon):
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            min_len = min(len(orig), len(recon))
            o = orig[:min_len]
            r = recon[:min_len]
            
            # åŸºæœ¬æŒ‡æ ‡
            mse = np.mean((o - r) ** 2)
            mae = np.mean(np.abs(o - r))
            
            # SNR
            signal_power = np.mean(o ** 2)
            noise_power = mse
            snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
            
            # ç›¸å…³æ€§
            correlation = np.corrcoef(o, r)[0, 1] if min_len > 1 else 0.0
            if np.isnan(correlation):
                correlation = 0.0
            
            return {
                "snr_db": snr,
                "correlation": correlation,
                "mse": mse,
                "mae": mae,
                "signal_power": signal_power
            }
        
        # è®¡ç®—VAE-onlyæŒ‡æ ‡
        vae_metrics = calculate_metrics(original_16k, vae_only)
        
        # è®¡ç®—å¢å¼ºç‰ˆæŒ‡æ ‡
        enhanced_metrics = calculate_metrics(original_16k, enhanced)
        
        # è®¡ç®—æ”¹è¿›é‡
        improvements = {
            "snr_improvement": enhanced_metrics["snr_db"] - vae_metrics["snr_db"],
            "correlation_improvement": enhanced_metrics["correlation"] - vae_metrics["correlation"],
            "mse_improvement": vae_metrics["mse"] - enhanced_metrics["mse"],  # å‡å°‘æ˜¯æ”¹è¿›
            "mae_improvement": vae_metrics["mae"] - enhanced_metrics["mae"]   # å‡å°‘æ˜¯æ”¹è¿›
        }
        
        comparison = {
            "vae_only": vae_metrics,
            "enhanced": enhanced_metrics,
            "improvements": improvements
        }
        
        print(f"   âœ… è´¨é‡å¯¹æ¯”åˆ†æå®Œæˆ")
        
        return comparison
    
    def _compare_frequency_content(self,
                                 original: np.ndarray,
                                 vae_only: np.ndarray,
                                 enhanced: np.ndarray) -> Dict:
        """å¯¹æ¯”é¢‘ç‡å†…å®¹"""
        print(f"   ğŸ¼ é¢‘ç‡å¯¹æ¯”åˆ†æ...")
        
        # é‡é‡‡æ ·åŸå§‹éŸ³é¢‘åˆ°16kHz
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        def analyze_frequency_bands(orig, recon):
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            min_len = min(len(orig), len(recon))
            o = orig[:min_len]
            r = recon[:min_len]
            
            # FFTåˆ†æ
            if min_len >= 8192:
                n_fft = 8192
            else:
                n_fft = 2 ** int(np.log2(min_len))
            
            orig_fft = np.abs(np.fft.fft(o[:n_fft]))[:n_fft//2]
            recon_fft = np.abs(np.fft.fft(r[:n_fft]))[:n_fft//2]
            
            freqs = np.fft.fftfreq(n_fft, 1/16000)[:n_fft//2]
            
            # é¢‘æ®µå®šä¹‰
            low_mask = freqs < 500
            mid_mask = (freqs >= 500) & (freqs < 4000)
            high_mask = freqs >= 4000
            
            # è®¡ç®—å„é¢‘æ®µèƒ½é‡ä¿æŒç‡
            low_retention = np.sum(recon_fft[low_mask]) / (np.sum(orig_fft[low_mask]) + 1e-10)
            mid_retention = np.sum(recon_fft[mid_mask]) / (np.sum(orig_fft[mid_mask]) + 1e-10)
            high_retention = np.sum(recon_fft[high_mask]) / (np.sum(orig_fft[high_mask]) + 1e-10)
            
            # æ€»ä½“é¢‘è°±ç›¸å…³æ€§
            freq_corr = np.corrcoef(orig_fft, recon_fft)[0, 1]
            if np.isnan(freq_corr):
                freq_corr = 0.0
            
            return {
                "low_freq_retention": low_retention,
                "mid_freq_retention": mid_retention,
                "high_freq_retention": high_retention,
                "frequency_correlation": freq_corr
            }
        
        # åˆ†æVAE-onlyé¢‘ç‡ç‰¹æ€§
        vae_freq = analyze_frequency_bands(original_16k, vae_only)
        
        # åˆ†æå¢å¼ºç‰ˆé¢‘ç‡ç‰¹æ€§
        enhanced_freq = analyze_frequency_bands(original_16k, enhanced)
        
        # è®¡ç®—é¢‘ç‡æ”¹è¿›
        freq_improvements = {
            "low_freq_improvement": enhanced_freq["low_freq_retention"] - vae_freq["low_freq_retention"],
            "mid_freq_improvement": enhanced_freq["mid_freq_retention"] - vae_freq["mid_freq_retention"],
            "high_freq_improvement": enhanced_freq["high_freq_retention"] - vae_freq["high_freq_retention"],
            "frequency_correlation_improvement": enhanced_freq["frequency_correlation"] - vae_freq["frequency_correlation"]
        }
        
        comparison = {
            "vae_only": vae_freq,
            "enhanced": enhanced_freq,
            "improvements": freq_improvements
        }
        
        print(f"   âœ… é¢‘ç‡å¯¹æ¯”åˆ†æå®Œæˆ")
        
        return comparison
    
    def _create_comparison_visualizations(self,
                                        original: np.ndarray,
                                        vae_only: np.ndarray,
                                        enhanced: np.ndarray,
                                        timestamp: int):
        """åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–"""
        print(f"   ğŸ“ˆ ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–...")
        
        # é‡é‡‡æ ·åŸå§‹éŸ³é¢‘åˆ°16kHz
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(original_16k), len(vae_only), len(enhanced))
        orig = original_16k[:min_len]
        vae = vae_only[:min_len]
        enh = enhanced[:min_len]
        
        # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
        fig, axes = plt.subplots(3, 2, figsize=(15, 12))
        fig.suptitle('VAE vs VAE+Diffusion éŸ³é¢‘é‡å»ºå¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # 1. æ—¶åŸŸæ³¢å½¢å¯¹æ¯”
        time_samples = min(16000, min_len)  # æ˜¾ç¤ºå‰1ç§’
        axes[0, 0].plot(orig[:time_samples], label='åŸå§‹éŸ³é¢‘', alpha=0.8)
        axes[0, 0].plot(vae[:time_samples], label='VAE-only', alpha=0.8)
        axes[0, 0].plot(enh[:time_samples], label='VAE+Diffusion', alpha=0.8)
        axes[0, 0].set_title('æ—¶åŸŸæ³¢å½¢å¯¹æ¯”ï¼ˆå‰1ç§’ï¼‰')
        axes[0, 0].set_xlabel('é‡‡æ ·ç‚¹')
        axes[0, 0].set_ylabel('æŒ¯å¹…')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. é¢‘è°±å¯¹æ¯”
        if min_len >= 8192:
            n_fft = 8192
            orig_fft = np.abs(np.fft.fft(orig[:n_fft]))[:n_fft//2]
            vae_fft = np.abs(np.fft.fft(vae[:n_fft]))[:n_fft//2]
            enh_fft = np.abs(np.fft.fft(enh[:n_fft]))[:n_fft//2]
            freqs = np.fft.fftfreq(n_fft, 1/16000)[:n_fft//2]
            
            axes[0, 1].loglog(freqs[1:], orig_fft[1:], label='åŸå§‹éŸ³é¢‘', alpha=0.8)
            axes[0, 1].loglog(freqs[1:], vae_fft[1:], label='VAE-only', alpha=0.8)
            axes[0, 1].loglog(freqs[1:], enh_fft[1:], label='VAE+Diffusion', alpha=0.8)
            axes[0, 1].set_title('é¢‘è°±å¯¹æ¯”')
            axes[0, 1].set_xlabel('é¢‘ç‡ (Hz)')
            axes[0, 1].set_ylabel('å¹…åº¦')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. Melé¢‘è°±å›¾ - åŸå§‹
        orig_mel = librosa.feature.melspectrogram(y=orig, sr=16000, n_mels=128)
        orig_mel_db = librosa.power_to_db(orig_mel, ref=np.max)
        im1 = axes[1, 0].imshow(orig_mel_db, aspect='auto', origin='lower', cmap='viridis')
        axes[1, 0].set_title('åŸå§‹éŸ³é¢‘Melé¢‘è°±')
        axes[1, 0].set_xlabel('æ—¶é—´å¸§')
        axes[1, 0].set_ylabel('Melé¢‘ç‡')
        plt.colorbar(im1, ax=axes[1, 0])
        
        # 4. Melé¢‘è°±å›¾ - VAE only
        vae_mel = librosa.feature.melspectrogram(y=vae, sr=16000, n_mels=128)
        vae_mel_db = librosa.power_to_db(vae_mel, ref=np.max)
        im2 = axes[1, 1].imshow(vae_mel_db, aspect='auto', origin='lower', cmap='viridis')
        axes[1, 1].set_title('VAE-onlyé‡å»ºMelé¢‘è°±')
        axes[1, 1].set_xlabel('æ—¶é—´å¸§')
        axes[1, 1].set_ylabel('Melé¢‘ç‡')
        plt.colorbar(im2, ax=axes[1, 1])
        
        # 5. Melé¢‘è°±å›¾ - VAE+Diffusion
        enh_mel = librosa.feature.melspectrogram(y=enh, sr=16000, n_mels=128)
        enh_mel_db = librosa.power_to_db(enh_mel, ref=np.max)
        im3 = axes[2, 0].imshow(enh_mel_db, aspect='auto', origin='lower', cmap='viridis')
        axes[2, 0].set_title('VAE+Diffusionå¢å¼ºMelé¢‘è°±')
        axes[2, 0].set_xlabel('æ—¶é—´å¸§')
        axes[2, 0].set_ylabel('Melé¢‘ç‡')
        plt.colorbar(im3, ax=axes[2, 0])
        
        # 6. é¢‘æ®µèƒ½é‡å¯¹æ¯”
        if min_len >= 8192:
            # è®¡ç®—å„é¢‘æ®µèƒ½é‡
            low_mask = freqs < 500
            mid_mask = (freqs >= 500) & (freqs < 4000)
            high_mask = freqs >= 4000
            
            bands = ['Low\n(<500Hz)', 'Mid\n(500Hz-4kHz)', 'High\n(>4kHz)']
            orig_energies = [
                np.sum(orig_fft[low_mask]),
                np.sum(orig_fft[mid_mask]),
                np.sum(orig_fft[high_mask])
            ]
            vae_energies = [
                np.sum(vae_fft[low_mask]),
                np.sum(vae_fft[mid_mask]),
                np.sum(vae_fft[high_mask])
            ]
            enh_energies = [
                np.sum(enh_fft[low_mask]),
                np.sum(enh_fft[mid_mask]),
                np.sum(enh_fft[high_mask])
            ]
            
            x = np.arange(len(bands))
            width = 0.25
            
            axes[2, 1].bar(x - width, orig_energies, width, label='åŸå§‹', alpha=0.8)
            axes[2, 1].bar(x, vae_energies, width, label='VAE-only', alpha=0.8)
            axes[2, 1].bar(x + width, enh_energies, width, label='VAE+Diffusion', alpha=0.8)
            
            axes[2, 1].set_title('é¢‘æ®µèƒ½é‡å¯¹æ¯”')
            axes[2, 1].set_xlabel('é¢‘æ®µ')
            axes[2, 1].set_ylabel('èƒ½é‡')
            axes[2, 1].set_xticks(x)
            axes[2, 1].set_xticklabels(bands)
            axes[2, 1].legend()
            axes[2, 1].grid(True, alpha=0.3)
        
        # ä¿å­˜å›¾è¡¨
        plt.tight_layout()
        plot_path = self.output_dir / f"comparison_analysis_{timestamp}.png"
        plt.savefig(str(plot_path), dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"   âœ… å¯¹æ¯”å¯è§†åŒ–å·²ä¿å­˜: {plot_path}")
    
    def _display_comparison_results(self, result: Dict):
        """æ˜¾ç¤ºå¯¹æ¯”ç»“æœ"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ VAE vs VAE+Diffusion é‡å»ºå¯¹æ¯”ç»“æœ")
        print(f"{'='*80}")
        
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {result['input_file']}")
        print(f"ğŸ“± å¤„ç†è®¾å¤‡: {result['processing_device']}")
        print(f"ğŸ“ Latentå½¢çŠ¶: {result['latent_shapes']['original']}")
        
        params = result['parameters']
        print(f"\nâš™ï¸ Diffusionå‚æ•°:")
        print(f"   ğŸ”¢ å»å™ªæ­¥æ•°: {params['diffusion_steps']}")
        print(f"   ğŸ¯ å¼•å¯¼å°ºåº¦: {params['guidance_scale']}")
        print(f"   ğŸ¼ é«˜é¢‘å¢å¼º: {params['high_freq_boost']}x")
        
        print(f"\nğŸ“Š è´¨é‡å¯¹æ¯”:")
        quality = result['quality_comparison']
        
        vae_metrics = quality['vae_only']
        enh_metrics = quality['enhanced']
        improvements = quality['improvements']
        
        print(f"   ğŸµ ä¿¡å™ªæ¯” (SNR):")
        print(f"      VAE-only: {vae_metrics['snr_db']:.2f} dB")
        print(f"      VAE+Diffusion: {enh_metrics['snr_db']:.2f} dB")
        print(f"      æ”¹è¿›: {improvements['snr_improvement']:+.2f} dB")
        
        print(f"   ğŸ”— ç›¸å…³æ€§:")
        print(f"      VAE-only: {vae_metrics['correlation']:.4f}")
        print(f"      VAE+Diffusion: {enh_metrics['correlation']:.4f}")
        print(f"      æ”¹è¿›: {improvements['correlation_improvement']:+.4f}")
        
        print(f"\nğŸ¼ é¢‘ç‡å¯¹æ¯”:")
        freq = result['frequency_comparison']
        
        vae_freq = freq['vae_only']
        enh_freq = freq['enhanced']
        freq_improvements = freq['improvements']
        
        print(f"   ğŸ¶ ä½é¢‘ä¿æŒç‡ (<500Hz):")
        print(f"      VAE-only: {vae_freq['low_freq_retention']:.3f}")
        print(f"      VAE+Diffusion: {enh_freq['low_freq_retention']:.3f}")
        print(f"      æ”¹è¿›: {freq_improvements['low_freq_improvement']:+.3f}")
        
        print(f"   ğŸµ ä¸­é¢‘ä¿æŒç‡ (500Hz-4kHz):")
        print(f"      VAE-only: {vae_freq['mid_freq_retention']:.3f}")
        print(f"      VAE+Diffusion: {enh_freq['mid_freq_retention']:.3f}")
        print(f"      æ”¹è¿›: {freq_improvements['mid_freq_improvement']:+.3f}")
        
        print(f"   ğŸ¼ é«˜é¢‘ä¿æŒç‡ (>4kHz):")
        print(f"      VAE-only: {vae_freq['high_freq_retention']:.3f}")
        print(f"      VAE+Diffusion: {enh_freq['high_freq_retention']:.3f}")
        print(f"      æ”¹è¿›: {freq_improvements['high_freq_improvement']:+.3f}")
        
        # æ•ˆæœè¯„ä¼°
        print(f"\nğŸ¯ æ•ˆæœè¯„ä¼°:")
        
        if improvements['snr_improvement'] > 1.0:
            print(f"   âœ… æ˜¾è‘—çš„è´¨é‡æ”¹è¿›ï¼SNRæå‡ {improvements['snr_improvement']:.1f}dB")
        elif improvements['snr_improvement'] > 0.5:
            print(f"   ğŸŸ¢ æ˜æ˜¾çš„è´¨é‡æ”¹è¿›ï¼ŒSNRæå‡ {improvements['snr_improvement']:.1f}dB")
        elif improvements['snr_improvement'] > 0:
            print(f"   ğŸ”¶ è½»å¾®çš„è´¨é‡æ”¹è¿›ï¼ŒSNRæå‡ {improvements['snr_improvement']:.1f}dB")
        else:
            print(f"   âŒ è´¨é‡ä¸‹é™ï¼ŒSNRä¸‹é™ {-improvements['snr_improvement']:.1f}dB")
        
        if freq_improvements['high_freq_improvement'] > 0.1:
            print(f"   ğŸ¼ é«˜é¢‘æ˜¾è‘—å¢å¼ºï¼æå‡ {freq_improvements['high_freq_improvement']*100:.1f}%")
        elif freq_improvements['high_freq_improvement'] > 0.05:
            print(f"   ğŸµ é«˜é¢‘æ˜æ˜¾å¢å¼ºï¼Œæå‡ {freq_improvements['high_freq_improvement']*100:.1f}%")
        elif freq_improvements['high_freq_improvement'] > 0:
            print(f"   ğŸ”¸ é«˜é¢‘è½»å¾®å¢å¼ºï¼Œæå‡ {freq_improvements['high_freq_improvement']*100:.1f}%")
        else:
            print(f"   âš ï¸ é«˜é¢‘æœªæ”¹å–„æˆ–ä¸‹é™")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        for name, path in result['paths'].items():
            print(f"   {name}: {path}")
        
        print(f"\nğŸ’¡ æ€»ç»“å’Œå»ºè®®:")
        
        # æ ¹æ®ç»“æœæä¾›å»ºè®®
        if improvements['snr_improvement'] > 0.5 and freq_improvements['high_freq_improvement'] > 0.05:
            print(f"   âœ… VAE+Diffusionå¢å¼ºæ•ˆæœè‰¯å¥½ï¼")
            print(f"   ğŸ¯ å»ºè®®ï¼šå¯ä»¥å°è¯•è°ƒæ•´diffusionå‚æ•°è¿›ä¸€æ­¥ä¼˜åŒ–")
        elif improvements['snr_improvement'] > 0:
            print(f"   ğŸŸ¢ å¢å¼ºæœ‰æ•ˆæœï¼Œä½†è¿˜æœ‰æ”¹è¿›ç©ºé—´")
            print(f"   ğŸ’¡ å»ºè®®ï¼šå¢åŠ diffusionæ­¥æ•°æˆ–è°ƒæ•´é«˜é¢‘å¢å¼ºç³»æ•°")
        else:
            print(f"   âš ï¸ å½“å‰å‚æ•°å¯èƒ½ä¸å¤Ÿç†æƒ³")
            print(f"   ğŸ”§ å»ºè®®ï¼šå‡å°‘å™ªå£°æ°´å¹³æˆ–è°ƒæ•´å¼•å¯¼å°ºåº¦")

def demo_vae_diffusion_enhancement():
    """æ¼”ç¤ºVAE+Diffusionå¢å¼º"""
    print("ğŸš€ Step 2: VAE + Diffusionå¢å¼ºé‡å»º")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        print("è¯·ç¡®ä¿AudioLDM2_Music_output.wavåœ¨å½“å‰ç›®å½•ä¸­")
        return
    
    # åˆå§‹åŒ–VAE+Diffusionå¢å¼ºå™¨
    enhancer = VAEDiffusionEnhancer()
    
    # æµ‹è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
    test_configs = [
        {
            "name": "è½»åº¦å¢å¼º",
            "diffusion_steps": 10,
            "guidance_scale": 2.0,
            "high_freq_boost": 1.2
        },
        {
            "name": "ä¸­åº¦å¢å¼º",
            "diffusion_steps": 20,
            "guidance_scale": 3.0,
            "high_freq_boost": 1.5
        },
        {
            "name": "å¼ºåº¦å¢å¼º",
            "diffusion_steps": 30,
            "guidance_scale": 4.0,
            "high_freq_boost": 2.0
        }
    ]
    
    results = []
    
    for config in test_configs:
        print(f"\nğŸ”¬ æµ‹è¯•é…ç½®: {config['name']}")
        print(f"   å‚æ•°: {config}")
        
        result = enhancer.enhance_reconstruction(
            input_file,
            diffusion_steps=config['diffusion_steps'],
            guidance_scale=config['guidance_scale'],
            high_freq_boost=config['high_freq_boost']
        )
        
        result['config_name'] = config['name']
        results.append(result)
    
    # å¯¹æ¯”æ‰€æœ‰é…ç½®çš„ç»“æœ
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æ‰€æœ‰é…ç½®å¯¹æ¯”æ€»ç»“")
    print(f"{'='*80}")
    
    for i, result in enumerate(results):
        config_name = result['config_name']
        improvements = result['quality_comparison']['improvements']
        freq_improvements = result['frequency_comparison']['improvements']
        
        print(f"\n{i+1}. {config_name}:")
        print(f"   ğŸ“ˆ SNRæ”¹è¿›: {improvements['snr_improvement']:+.2f} dB")
        print(f"   ğŸ¼ é«˜é¢‘æ”¹è¿›: {freq_improvements['high_freq_improvement']*100:+.1f}%")
        print(f"   ğŸ”— ç›¸å…³æ€§æ”¹è¿›: {improvements['correlation_improvement']:+.4f}")
    
    print(f"\nâœ… VAE+Diffusionå¢å¼ºæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š æŸ¥çœ‹è¾“å‡ºç›®å½•: vae_diffusion_enhanced/")
    print(f"ğŸµ å¯¹æ¯”ä¸åŒé…ç½®çš„éŸ³é¢‘æ•ˆæœï¼Œé€‰æ‹©æœ€ä½³å‚æ•°")

if __name__ == "__main__":
    demo_vae_diffusion_enhancement()
