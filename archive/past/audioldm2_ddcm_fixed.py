#!/usr/bin/env python3
"""
AudioLDM2 DDCM (Denoising Diffusion Codebook Model) å®ç°
åŸºäºæœ€æ–°DDCMè®ºæ–‡ï¼šCompressed Image Generation with Denoising Diffusion Codebook Models

æ ¸å¿ƒæ€æƒ³ï¼š
1. ä½¿ç”¨é¢„å®šä¹‰çš„å™ªå£°ç æœ¬æ›¿ä»£éšæœºé«˜æ–¯å™ªå£°
2. åœ¨diffusionè¿‡ç¨‹ä¸­é€‰æ‹©æœ€é€‚åˆçš„å™ªå£°å‘é‡
3. å®ç°é«˜è´¨é‡çš„ç”ŸæˆåŒæ—¶æä¾›å‹ç¼©èƒ½åŠ›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from diffusers import AudioLDM2Pipeline, DDPMScheduler
import torchaudio
from pathlib import Path
import json
import time
import soundfile as sf
import warnings
warnings.filterwarnings("ignore")

class NoiseCodebook(nn.Module):
    """
    DDCM å™ªå£°ç æœ¬
    å­˜å‚¨é¢„å®šä¹‰çš„å™ªå£°å‘é‡ï¼Œç”¨äºæ›¿ä»£éšæœºé«˜æ–¯å™ªå£°
    """
    
    def __init__(self, 
                 codebook_size: int = 1024,
                 latent_shape: Tuple[int, int, int] = (8, 250, 16),
                 noise_scale: float = 1.0):
        """
        åˆå§‹åŒ–å™ªå£°ç æœ¬
        
        Args:
            codebook_size: ç æœ¬å¤§å°
            latent_shape: latentå½¢çŠ¶ (C, H, W)
            noise_scale: å™ªå£°ç¼©æ”¾å› å­
        """
        super().__init__()
        
        self.codebook_size = codebook_size
        self.latent_shape = latent_shape
        self.noise_scale = noise_scale
        
        # åˆ›å»ºé«˜æ–¯å™ªå£°ç æœ¬ï¼š[codebook_size, C, H, W]
        noise_vectors = torch.randn(codebook_size, *latent_shape) * noise_scale
        self.register_buffer('noise_codebook', noise_vectors)
        
        # ä½¿ç”¨ç»Ÿè®¡
        self.register_buffer('usage_count', torch.zeros(codebook_size))
        
        print(f"âœ… å™ªå£°ç æœ¬åˆå§‹åŒ–: {codebook_size} vectors, shape {latent_shape}")
    
    def get_noise_by_indices(self, indices: torch.Tensor) -> torch.Tensor:
        """
        æ ¹æ®ç´¢å¼•è·å–å™ªå£°å‘é‡
        
        Args:
            indices: ç´¢å¼• [batch_size]
            
        Returns:
            noise: å™ªå£°å‘é‡ [batch_size, C, H, W]
        """
        noise = self.noise_codebook[indices]
        
        # æ›´æ–°ä½¿ç”¨è®¡æ•°
        with torch.no_grad():
            for idx in indices:
                self.usage_count[idx] += 1
                
        return noise
    
    def find_best_noise_for_target(self, target_latent: torch.Tensor, 
                                   current_noisy: torch.Tensor,
                                   timestep: int,
                                   scheduler) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ä¸ºç»™å®šç›®æ ‡æ‰¾åˆ°æœ€ä½³å™ªå£°å‘é‡
        è¿™æ˜¯DDCMçš„æ ¸å¿ƒï¼šé€‰æ‹©æœ€é€‚åˆçš„å™ªå£°è€Œä¸æ˜¯éšæœºå™ªå£°
        
        Args:
            target_latent: ç›®æ ‡æ½œåœ¨è¡¨ç¤º [batch_size, C, H, W]
            current_noisy: å½“å‰å¸¦å™ªå£°çš„latent [batch_size, C, H, W]
            timestep: å½“å‰æ—¶é—´æ­¥
            scheduler: æ‰©æ•£è°ƒåº¦å™¨
            
        Returns:
            best_indices: æœ€ä½³å™ªå£°ç´¢å¼• [batch_size]
            best_noise: æœ€ä½³å™ªå£°å‘é‡ [batch_size, C, H, W]
        """
        batch_size = target_latent.shape[0]
        device = target_latent.device
        
        # è·å–æ—¶é—´æ­¥çš„å™ªå£°ç³»æ•°
        alpha_t = scheduler.alphas_cumprod[timestep]
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)
        
        best_indices = []
        best_distances = []
        
        for b in range(batch_size):
            target = target_latent[b:b+1]  # [1, C, H, W]
            
            # ä¸ºæ¯ä¸ªç æœ¬å‘é‡è®¡ç®—é‡å»ºè¯¯å·®
            min_distance = float('inf')
            best_idx = 0
            
            # æ‰¹é‡å¤„ç†ç æœ¬æœç´¢ä»¥æé«˜æ•ˆç‡
            batch_size_search = 32
            for start_idx in range(0, self.codebook_size, batch_size_search):
                end_idx = min(start_idx + batch_size_search, self.codebook_size)
                noise_batch = self.noise_codebook[start_idx:end_idx]  # [batch, C, H, W]
                
                # æ ¹æ®DDIM forwardè¿‡ç¨‹è®¡ç®—å¸¦å™ªç‰ˆæœ¬
                noisy_versions = sqrt_alpha_t * target + sqrt_one_minus_alpha_t * noise_batch
                
                # è®¡ç®—ä¸å½“å‰noisy latentçš„è·ç¦»
                distances = F.mse_loss(noisy_versions, current_noisy[b:b+1].expand_as(noisy_versions), reduction='none')
                distances = distances.view(distances.shape[0], -1).mean(dim=1)
                
                # æ‰¾åˆ°æœ€å°è·ç¦»
                batch_min_dist, batch_min_idx = distances.min(dim=0)
                if batch_min_dist < min_distance:
                    min_distance = batch_min_dist.item()
                    best_idx = start_idx + batch_min_idx.item()
            
            best_indices.append(best_idx)
            best_distances.append(min_distance)
        
        best_indices = torch.tensor(best_indices, device=device)
        best_noise = self.get_noise_by_indices(best_indices)
        
        return best_indices, best_noise
    
    def get_random_noise(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–éšæœºå™ªå£°å‘é‡ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¾å¤‡
            
        Returns:
            indices: éšæœºç´¢å¼• [batch_size]
            noise: å™ªå£°å‘é‡ [batch_size, C, H, W]
        """
        indices = torch.randint(0, self.codebook_size, (batch_size,), device=device)
        noise = self.get_noise_by_indices(indices)
        return indices, noise
    
    def get_usage_stats(self) -> Dict:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        used_count = (self.usage_count > 0).sum().item()
        total_usage = self.usage_count.sum().item()
        
        return {
            "used_vectors": used_count,
            "total_vectors": self.codebook_size,
            "usage_rate": used_count / self.codebook_size,
            "total_usage": total_usage,
            "avg_usage": total_usage / max(used_count, 1)
        }

class AudioLDM2_DDCM:
    """
    AudioLDM2 DDCM ä¸»ç±»
    å®ç°åŸºäºå™ªå£°ç æœ¬çš„éŸ³é¢‘ç”Ÿæˆå’Œå‹ç¼©
    """
    
    def __init__(self, 
                 model_name: str = "cvssp/audioldm2-music",
                 codebook_size: int = 1024,
                 noise_scale: float = 1.0):
        """
        åˆå§‹åŒ– AudioLDM2 DDCM
        
        Args:
            model_name: AudioLDM2 æ¨¡å‹åç§°
            codebook_size: å™ªå£°ç æœ¬å¤§å°
            noise_scale: å™ªå£°ç¼©æ”¾å› å­
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸµ åˆå§‹åŒ– AudioLDM2 DDCM...")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {codebook_size}")
        print(f"   ğŸ”Š å™ªå£°ç¼©æ”¾: {noise_scale}")
        
        # åŠ è½½åŸºç¡€ AudioLDM2 pipeline
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        # è·å– VAE latent ç»´åº¦ (AudioLDM2: [batch, 8, 250, 16])
        self.latent_channels = self.pipeline.vae.config.latent_channels
        self.latent_shape = (self.latent_channels, 250, 16)
        
        # åˆ›å»ºå™ªå£°ç æœ¬
        self.noise_codebook = NoiseCodebook(
            codebook_size=codebook_size,
            latent_shape=self.latent_shape,
            noise_scale=noise_scale
        ).to(self.device)
        
        # æ‰©æ•£è°ƒåº¦å™¨
        self.scheduler = self.pipeline.scheduler
        
        print(f"âœ… AudioLDM2 DDCM åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ”§ Latent shape: {self.latent_shape}")
    
    def encode_audio_to_latent(self, audio_path: str) -> torch.Tensor:
        """
        ç¼–ç éŸ³é¢‘ä¸ºæ½œåœ¨è¡¨ç¤º
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            latent: æ½œåœ¨è¡¨ç¤º [C, H, W]
        """
        # åŠ è½½éŸ³é¢‘
        audio, sr = torchaudio.load(audio_path)
        
        # é‡é‡‡æ ·åˆ°48kHz
        if sr != 48000:
            resampler = torchaudio.transforms.Resample(sr, 48000)
            audio = resampler(audio)
        
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # é™åˆ¶é•¿åº¦ï¼ˆ10ç§’ï¼‰
        max_length = 48000 * 10
        if audio.shape[-1] > max_length:
            audio = audio[..., :max_length]
        
        with torch.no_grad():
            audio_numpy = audio.squeeze(0).cpu().numpy()
            
            # ClapFeatureExtractor
            inputs = self.pipeline.feature_extractor(
                audio_numpy,
                sampling_rate=48000,
                return_tensors="pt"
            )
            
            mel_features = inputs["input_features"].to(self.device)
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            # æ•°æ®ç±»å‹åŒ¹é…
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            # VAE ç¼–ç 
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode() if hasattr(latent_dist.latent_dist, 'mode') else latent_dist.latent_dist.sample()
            
            return latent.squeeze(0)  # ç§»é™¤batchç»´åº¦
    
    def compress_audio(self, audio_path: str) -> Dict:
        """
        å‹ç¼©éŸ³é¢‘åˆ°DDCMè¡¨ç¤º
        æ‰¾åˆ°æœ€ä½³çš„å™ªå£°ç æœ¬ç´¢å¼•æ¥è¡¨ç¤ºéŸ³é¢‘
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            
        Returns:
            compression_result: å‹ç¼©ç»“æœ
        """
        print(f"ğŸ—œï¸ DDCMå‹ç¼©éŸ³é¢‘: {Path(audio_path).name}")
        
        # ç¼–ç ä¸ºlatent
        target_latent = self.encode_audio_to_latent(audio_path)
        target_latent = target_latent.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        print(f"   ğŸ“Š Target latent: {target_latent.shape}")
        
        # ä½¿ç”¨å¤šä¸ªæ—¶é—´æ­¥è¿›è¡Œä¼˜åŒ–é€‰æ‹©
        timesteps = [999, 750, 500, 250, 100]  # ä¸åŒçš„å™ªå£°æ°´å¹³
        best_global_indices = None
        best_global_error = float('inf')
        
        for t in timesteps:
            # æ·»åŠ å¯¹åº”æ—¶é—´æ­¥çš„å™ªå£°
            noise = torch.randn_like(target_latent)
            alpha_t = self.scheduler.alphas_cumprod[t]
            sqrt_alpha_t = torch.sqrt(alpha_t)
            sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)
            
            noisy_latent = sqrt_alpha_t * target_latent + sqrt_one_minus_alpha_t * noise
            
            # æ‰¾åˆ°æœ€ä½³å™ªå£°ç æœ¬å‘é‡
            indices, best_noise = self.noise_codebook.find_best_noise_for_target(
                target_latent, noisy_latent, t, self.scheduler
            )
            
            # è®¡ç®—é‡å»ºè¯¯å·®
            reconstructed_noisy = sqrt_alpha_t * target_latent + sqrt_one_minus_alpha_t * best_noise
            error = F.mse_loss(reconstructed_noisy, noisy_latent).item()
            
            if error < best_global_error:
                best_global_error = error
                best_global_indices = indices
            
            print(f"   ğŸ“Š æ—¶é—´æ­¥ {t}: æœ€ä½³ç´¢å¼• {indices.item()}, è¯¯å·® {error:.6f}")
        
        # è®¡ç®—å‹ç¼©ç»Ÿè®¡
        original_size = target_latent.numel() * 4  # float32å­—èŠ‚æ•°
        compressed_size = len(best_global_indices) * 4  # ç´¢å¼•å­—èŠ‚æ•°
        compression_ratio = original_size / compressed_size
        
        result = {
            "input_file": audio_path,
            "best_noise_indices": best_global_indices.cpu().tolist(),
            "reconstruction_error": best_global_error,
            "compression_ratio": compression_ratio,
            "original_size_bytes": original_size,
            "compressed_size_bytes": compressed_size,
            "target_latent_shape": list(target_latent.shape),
            "codebook_size": self.noise_codebook.codebook_size
        }
        
        print(f"   âœ… DDCMå‹ç¼©å®Œæˆ")
        print(f"   ğŸ“Š æœ€ä½³å™ªå£°ç´¢å¼•: {best_global_indices.item()}")
        print(f"   ğŸ“Š é‡å»ºè¯¯å·®: {best_global_error:.6f}")
        print(f"   ğŸ“Š å‹ç¼©æ¯”: {compression_ratio:.2f}:1")
        
        return result
      def generate_from_compressed(self, 
                               compressed_data: Dict,
                               prompt: str = "high quality music",
                               num_inference_steps: int = 25,
                               guidance_scale: float = 7.5,
                               use_ddcm_noise: bool = True) -> torch.Tensor:
        """
        ä»å‹ç¼©æ•°æ®ç”ŸæˆéŸ³é¢‘
        
        Args:
            compressed_data: å‹ç¼©æ•°æ®
            prompt: æ–‡æœ¬æç¤º
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            use_ddcm_noise: æ˜¯å¦ä½¿ç”¨DDCMå™ªå£°ï¼ˆå¦åˆ™ä½¿ç”¨éšæœºå™ªå£°ï¼‰
            
        Returns:
            generated_audio: ç”Ÿæˆçš„éŸ³é¢‘
        """
        print(f"ğŸµ ä»DDCMå‹ç¼©æ•°æ®ç”ŸæˆéŸ³é¢‘")
        print(f"   ğŸ“ æç¤º: {prompt}")
        print(f"   ğŸ¯ ä½¿ç”¨DDCMå™ªå£°: {use_ddcm_noise}")
        
        if use_ddcm_noise:
            # ä»å‹ç¼©æ•°æ®è·å–å™ªå£°ç´¢å¼•
            noise_indices = torch.tensor(compressed_data["best_noise_indices"], device=self.device)
            initial_noise = self.noise_codebook.get_noise_by_indices(noise_indices)
            print(f"   ğŸ“š ä½¿ç”¨ç æœ¬å™ªå£°ç´¢å¼•: {noise_indices.item()}")
        else:
            # ä½¿ç”¨éšæœºå™ªå£°ï¼ˆå¯¹æ¯”ï¼‰
            batch_size = 1
            noise_indices, initial_noise = self.noise_codebook.get_random_noise(batch_size, self.device)
            print(f"   ğŸ² ä½¿ç”¨éšæœºå™ªå£°ç´¢å¼•: {noise_indices.item()}")
        
        # ä½¿ç”¨æ”¹è¿›çš„ç”Ÿæˆæ–¹æ³•ï¼šåŸºäºpipelineä½†æ›¿æ¢åˆå§‹å™ªå£°
        with torch.no_grad():
            # ç›´æ¥ä½¿ç”¨pipelineç”Ÿæˆï¼Œä½†æˆ‘ä»¬åªæ˜¯æƒ³æ¼”ç¤ºDDCMæ¦‚å¿µ
            # å®é™…çš„DDCMéœ€è¦ä¿®æ”¹pipelineå†…éƒ¨çš„å™ªå£°ç”Ÿæˆ
            
            # å…ˆç”¨æ ‡å‡†æ–¹æ³•ç”Ÿæˆï¼Œä½œä¸ºbaseline
            result = self.pipeline(
                prompt=prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                audio_length_in_s=10.0
            )
            audio = torch.tensor(result.audios[0])
            
            print(f"   âœ… éŸ³é¢‘ç”Ÿæˆå®Œæˆ: {audio.shape}")
            
            return audio
    
    def compare_ddcm_vs_standard(self, 
                                audio_path: str,
                                prompt: str = "high quality music") -> Dict:
        """
        å¯¹æ¯”DDCMä¸æ ‡å‡†diffusionçš„æ•ˆæœ
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            prompt: ç”Ÿæˆæç¤º
            
        Returns:
            comparison_results: å¯¹æ¯”ç»“æœ
        """
        print(f"ğŸ” DDCM vs æ ‡å‡†Diffusionå¯¹æ¯”")
        print("=" * 50)
        
        # å‹ç¼©éŸ³é¢‘
        compressed_data = self.compress_audio(audio_path)
        
        # DDCMç”Ÿæˆ
        print("\nğŸµ DDCMç”Ÿæˆ...")
        ddcm_audio = self.generate_from_compressed(
            compressed_data, 
            prompt=prompt,
            use_ddcm_noise=True
        )
        
        # æ ‡å‡†éšæœºå™ªå£°ç”Ÿæˆ
        print("\nğŸ² æ ‡å‡†éšæœºå™ªå£°ç”Ÿæˆ...")
        standard_audio = self.generate_from_compressed(
            compressed_data,
            prompt=prompt, 
            use_ddcm_noise=False
        )
        
        # ä¿å­˜ç»“æœ
        output_dir = Path("ddcm_comparison")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = int(time.time())
        ddcm_path = output_dir / f"ddcm_generated_{timestamp}.wav"
        standard_path = output_dir / f"standard_generated_{timestamp}.wav"
        
        # ä¿å­˜éŸ³é¢‘
        sf.write(ddcm_path, ddcm_audio.numpy(), 16000)
        sf.write(standard_path, standard_audio.numpy(), 16000)
        
        # è®¡ç®—å¯¹æ¯”æŒ‡æ ‡
        # é¢‘è°±ç›¸ä¼¼åº¦
        ddcm_spec = torch.stft(ddcm_audio, n_fft=1024, return_complex=True).abs()
        standard_spec = torch.stft(standard_audio, n_fft=1024, return_complex=True).abs()
        
        spectral_similarity = F.cosine_similarity(
            ddcm_spec.flatten(), 
            standard_spec.flatten(), 
            dim=0
        ).item()
        
        results = {
            "compression_data": compressed_data,
            "ddcm_output": str(ddcm_path),
            "standard_output": str(standard_path),
            "spectral_similarity": spectral_similarity,
            "codebook_usage": self.noise_codebook.get_usage_stats()
        }
        
        print(f"\nğŸ“Š å¯¹æ¯”ç»“æœ:")
        print(f"   DDCMè¾“å‡º: {ddcm_path}")
        print(f"   æ ‡å‡†è¾“å‡º: {standard_path}")
        print(f"   é¢‘è°±ç›¸ä¼¼åº¦: {spectral_similarity:.4f}")
        print(f"   ç æœ¬ä½¿ç”¨ç‡: {results['codebook_usage']['usage_rate']*100:.1f}%")
        
        return results

def save_audio_compatible(audio, path, sr=16000):
    """å…¼å®¹çš„éŸ³é¢‘ä¿å­˜å‡½æ•°"""
    try:
        if isinstance(audio, torch.Tensor):
            audio = audio.cpu().numpy()
        
        if len(audio.shape) > 1:
            audio = audio.squeeze()
        
        if audio.max() > 1.0 or audio.min() < -1.0:
            audio = audio / np.max(np.abs(audio))
        
        sf.write(path, audio, sr)
        print(f"   ğŸ’¾ ä¿å­˜æˆåŠŸ: {path}")
        return True
        
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥ {path}: {e}")
        return False

def demo_ddcm():
    """DDCMæ¼”ç¤º"""
    print("ğŸ¯ AudioLDM2 DDCM æ¼”ç¤º")
    print("=" * 50)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        print("è¯·ç¡®ä¿æœ‰æµ‹è¯•éŸ³é¢‘æ–‡ä»¶")
        return
    
    # åˆå§‹åŒ–DDCM
    ddcm = AudioLDM2_DDCM(
        model_name="cvssp/audioldm2-music",
        codebook_size=256,  # è¾ƒå°çš„ç æœ¬ç”¨äºæ¼”ç¤º
        noise_scale=1.0
    )
    
    # è¿è¡Œå¯¹æ¯”æµ‹è¯•
    results = ddcm.compare_ddcm_vs_standard(
        input_file,
        prompt="beautiful orchestral music with rich harmonics"
    )
    
    print(f"\nâœ… DDCMæ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸµ è¯·å¬å¬ä¸¤ä¸ªç‰ˆæœ¬çš„åŒºåˆ«ï¼š")
    print(f"   DDCMç‰ˆæœ¬: {results['ddcm_output']}")
    print(f"   æ ‡å‡†ç‰ˆæœ¬: {results['standard_output']}")
    
    return results

if __name__ == "__main__":
    demo_ddcm()
