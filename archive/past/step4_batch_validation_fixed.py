#!/usr/bin/env python3
"""
Step 4: æ‰¹é‡éªŒè¯å’Œå®é™…åº”ç”¨é›†æˆ
åŸºäºStep 3çš„æœ€ä½³å‚æ•°ï¼ˆå¢å¼ºç³»æ•°1.4xï¼‰ï¼Œåœ¨å¤šä¸ªéŸ³é¢‘æ–‡ä»¶ä¸ŠéªŒè¯æ€§èƒ½
å¹¶æä¾›å¯é›†æˆåˆ°ç”Ÿäº§ç¯å¢ƒçš„å¢å¼ºç®¡é“
"""

import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import librosa
from pathlib import Path
import time
from diffusers import AudioLDM2Pipeline
from typing import Dict, Tuple, List, Optional
import warnings
import pandas as pd
from scipy import signal
warnings.filterwarnings("ignore")

class BatchValidationPipeline:
    """
    æ‰¹é‡éªŒè¯ç®¡é“
    éªŒè¯æœ€ä½³å¢å¼ºå‚æ•°åœ¨å¤šä¸ªéŸ³é¢‘æ–‡ä»¶ä¸Šçš„æ€§èƒ½
    """
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """åˆå§‹åŒ–æ‰¹é‡éªŒè¯ç®¡é“"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸ¯ åˆå§‹åŒ–æ‰¹é‡éªŒè¯ç®¡é“")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        
        # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        # ç»Ÿä¸€ä½¿ç”¨float32é¿å…ç±»å‹ä¸åŒ¹é…é—®é¢˜
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
        ).to(self.device)
        
        # æœ€ä½³å‚æ•°ï¼ˆæ¥è‡ªStep 3çš„ä¼˜åŒ–ç»“æœï¼‰
        self.best_boost_factor = 1.4
        
        print(f"âœ… æ‰¹é‡éªŒè¯ç®¡é“åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ¯ æœ€ä½³å¢å¼ºç³»æ•°: {self.best_boost_factor}x")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("batch_validation_results")
        self.output_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def enhanced_vae_reconstruction(self, audio_tensor: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨æœ€ä½³å‚æ•°è¿›è¡Œå¢å¼ºVAEé‡å»º
        """
        try:
            # 1. VAEç¼–ç 
            latents = self.pipeline.vae.encode(audio_tensor).latent_dist.sample()
            latents = latents * self.pipeline.vae.config.scaling_factor
            
            # 2. åº”ç”¨æœ€ä½³å¢å¼ºï¼ˆåŸºäºStep 3çš„ç»“æœï¼‰
            enhanced_latents = self.apply_frequency_boost(latents, self.best_boost_factor)
            
            # 3. VAEè§£ç 
            enhanced_latents = enhanced_latents / self.pipeline.vae.config.scaling_factor
            reconstructed = self.pipeline.vae.decode(enhanced_latents).sample
            
            return reconstructed
            
        except Exception as e:
            print(f"âŒ å¢å¼ºé‡å»ºè¿‡ç¨‹å‡ºé”™: {e}")
            return None
    
    def apply_frequency_boost(self, latents: torch.Tensor, boost_factor: float) -> torch.Tensor:
        """
        åº”ç”¨é¢‘ç‡å¢å¼ºï¼ˆåŸºäºStep 3ä¼˜åŒ–çš„æ–¹æ³•ï¼‰
        ä½¿ç”¨æ‰‹åŠ¨numpyå®ç°é¿å…PyTorch strideé—®é¢˜
        """
        try:
            # è½¬æ¢ä¸ºnumpyè¿›è¡ŒFFTå¤„ç†
            latents_np = latents.detach().cpu().numpy()
            
            # å¯¹æ¯ä¸ªbatchå’Œchannelåˆ†åˆ«å¤„ç†
            enhanced_latents_np = np.zeros_like(latents_np)
            
            for b in range(latents_np.shape[0]):
                for c in range(latents_np.shape[1]):
                    for h in range(latents_np.shape[2]):
                        signal_1d = latents_np[b, c, h, :]
                        
                        # FFT
                        fft_data = np.fft.fft(signal_1d)
                        freqs = np.fft.fftfreq(len(fft_data))
                        
                        # é¢‘ç‡å¢å¼º
                        high_freq_mask = np.abs(freqs) > 0.3  # é«˜é¢‘é˜ˆå€¼
                        fft_data[high_freq_mask] *= boost_factor
                        
                        # IFFT
                        enhanced_signal = np.fft.ifft(fft_data).real
                        enhanced_latents_np[b, c, h, :] = enhanced_signal
            
            # è½¬æ¢å›tensor
            enhanced_latents = torch.from_numpy(enhanced_latents_np).to(latents.device)
            return enhanced_latents
            
        except Exception as e:
            print(f"âŒ é¢‘ç‡å¢å¼ºå‡ºé”™: {e}")
            return latents
    
    def calculate_metrics(self, original: np.ndarray, reconstructed: np.ndarray) -> Dict[str, float]:
        """
        è®¡ç®—éŸ³é¢‘è´¨é‡æŒ‡æ ‡
        """
        try:
            # ç¡®ä¿é•¿åº¦åŒ¹é…
            min_len = min(len(original), len(reconstructed))
            original = original[:min_len]
            reconstructed = reconstructed[:min_len]
            
            # SNRè®¡ç®—
            signal_power = np.mean(original ** 2)
            noise_power = np.mean((original - reconstructed) ** 2)
            snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
            
            # é¢‘ç‡åˆ†æ
            freqs_orig = np.abs(np.fft.fft(original))
            freqs_recon = np.abs(np.fft.fft(reconstructed))
            
            # é«˜é¢‘ä¿æŒåº¦ï¼ˆé«˜äºNyquisté¢‘ç‡ä¸€åŠçš„éƒ¨åˆ†ï¼‰
            high_freq_idx = len(freqs_orig) // 4
            high_freq_retention = (
                np.sum(freqs_recon[high_freq_idx:]) / 
                (np.sum(freqs_orig[high_freq_idx:]) + 1e-10) * 100
            )
            
            # é¢‘è°±ç›¸å…³æ€§
            spectral_correlation = np.corrcoef(freqs_orig, freqs_recon)[0, 1]
            
            return {
                'snr': snr,
                'high_freq_retention': high_freq_retention,
                'spectral_correlation': spectral_correlation
            }
            
        except Exception as e:
            print(f"âŒ æŒ‡æ ‡è®¡ç®—å‡ºé”™: {e}")
            return {}
    
    def mel_to_audio(self, mel_tensor: torch.Tensor) -> np.ndarray:
        """å°†melé¢‘è°±è½¬æ¢å›éŸ³é¢‘ï¼ˆä½¿ç”¨AudioLDM2çš„HiFiGAN vocoderï¼‰"""
        try:
            # ä½¿ç”¨AudioLDM2çš„mel_spectrogram_to_waveformæ–¹æ³•
            with torch.no_grad():
                # mel_tensoråº”è¯¥æ˜¯shape: [batch, channels, height, width]
                # ç¡®ä¿æ­£ç¡®çš„ç»´åº¦
                if mel_tensor.dim() == 2:
                    mel_tensor = mel_tensor.unsqueeze(0).unsqueeze(0)
                elif mel_tensor.dim() == 3:
                    mel_tensor = mel_tensor.unsqueeze(0)
                
                # ç§»é™¤å¤šä½™çš„ç»´åº¦
                while mel_tensor.dim() > 4:
                    mel_tensor = mel_tensor.squeeze(0)
                
                # ä½¿ç”¨AudioLDM2çš„HiFiGAN vocoder
                audio_tensor = self.pipeline.mel_spectrogram_to_waveform(mel_tensor)
                audio_np = audio_tensor.squeeze().detach().cpu().numpy()
                
                # ç¡®ä¿è¾“å‡ºæ˜¯1Dæ•°ç»„
                if audio_np.ndim > 1:
                    audio_np = audio_np.flatten()
                
            return audio_np
            
        except Exception as e:
            print(f"âŒ melè½¬éŸ³é¢‘å‡ºé”™: {e}")
            # å¦‚æœvocoderå¤±è´¥ï¼Œè¿”å›åŒ¹é…é•¿åº¦çš„é™éŸ³
            try:
                # ä¼°ç®—åˆç†çš„éŸ³é¢‘é•¿åº¦ (åŸºäºmelè°±çš„æ—¶é—´ç»´åº¦)
                if hasattr(mel_tensor, 'shape') and len(mel_tensor.shape) >= 2:
                    # å‡è®¾hop_length=256, sr=16000
                    time_frames = mel_tensor.shape[-1]
                    estimated_length = time_frames * 256  # hop_length
                    return np.zeros(estimated_length)
                else:
                    return np.zeros(16000)  # å›é€€åˆ°1ç§’é™éŸ³
            except:
                return np.zeros(16000)  # æœ€ç»ˆå›é€€
    
    def process_audio_file(self, audio_path: str) -> Dict:
        """å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        try:
            print(f"\nğŸµ å¤„ç†éŸ³é¢‘æ–‡ä»¶: {Path(audio_path).name}")
            
            # åŠ è½½éŸ³é¢‘
            audio, sr = torchaudio.load(audio_path)
            
            # ç¡®ä¿å•å£°é“
            if audio.shape[0] > 1:
                audio = torch.mean(audio, dim=0, keepdim=True)
            
            # é‡é‡‡æ ·åˆ°48kHz (CLAPç‰¹å¾æå–å™¨è¦æ±‚)
            target_sr = 48000
            if sr != target_sr:
                resampler = torchaudio.transforms.Resample(sr, target_sr)
                audio = resampler(audio)
                sr = target_sr
            
            # è£å‰ªåˆ°åˆé€‚é•¿åº¦ï¼ˆ10ç§’ï¼‰
            max_length = 10 * sr
            if audio.shape[1] > max_length:
                audio = audio[:, :max_length]
            
            # é¢„å¤„ç†ï¼ˆmelé¢‘è°±ï¼‰
            inputs = self.pipeline.feature_extractor(
                audio.squeeze().numpy(), 
                sampling_rate=sr, 
                return_tensors="pt"
            )
            audio_tensor = inputs.input_features.to(self.device).float()
            
            # åŸå§‹VAEé‡å»º
            start_time = time.time()
            with torch.no_grad():
                original_latents = self.pipeline.vae.encode(audio_tensor).latent_dist.sample()
                original_latents = original_latents * self.pipeline.vae.config.scaling_factor
                original_latents = original_latents / self.pipeline.vae.config.scaling_factor
                original_reconstructed = self.pipeline.vae.decode(original_latents).sample
            original_time = time.time() - start_time
            
            # å¢å¼ºVAEé‡å»º
            start_time = time.time()
            with torch.no_grad():
                enhanced_reconstructed = self.enhanced_vae_reconstruction(audio_tensor)
            enhanced_time = time.time() - start_time
            
            if enhanced_reconstructed is None:
                return None
            
            # è½¬æ¢ä¸ºnumpyè¿›è¡Œåˆ†æ
            original_np = audio.squeeze().numpy()
            original_recon_np = self.mel_to_audio(original_reconstructed)
            enhanced_recon_np = self.mel_to_audio(enhanced_reconstructed)
            
            # è®¡ç®—æŒ‡æ ‡
            original_metrics = self.calculate_metrics(original_np, original_recon_np)
            enhanced_metrics = self.calculate_metrics(original_np, enhanced_recon_np)
            
            # è®¡ç®—æ”¹è¿›ç¨‹åº¦
            improvements = {}
            for key in original_metrics:
                if 'snr' in key:
                    improvements[f"{key}_improvement"] = enhanced_metrics[key] - original_metrics[key]
                else:
                    improvements[f"{key}_improvement"] = (enhanced_metrics[key] / original_metrics[key] - 1) * 100
            
            return {
                'filename': Path(audio_path).name,
                'audio_length': audio.shape[1] / sr,
                'processing_time_original': original_time,
                'processing_time_enhanced': enhanced_time,
                'original_metrics': original_metrics,
                'enhanced_metrics': enhanced_metrics,
                'improvements': improvements,
                'audio_original': original_np,
                'audio_original_recon': original_recon_np,
                'audio_enhanced_recon': enhanced_recon_np,
                'sample_rate': sr
            }
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {audio_path} æ—¶å‡ºé”™: {e}")
            return None
    
    def batch_validate(self, audio_files: List[str]) -> Dict:
        """æ‰¹é‡éªŒè¯å¤šä¸ªéŸ³é¢‘æ–‡ä»¶"""
        print(f"\nğŸ¯ å¼€å§‹æ‰¹é‡éªŒè¯")
        print(f"   ğŸ“ æ–‡ä»¶æ•°é‡: {len(audio_files)}")
        print(f"   âš¡ å¢å¼ºç³»æ•°: {self.best_boost_factor}x")
        
        results = []
        successful_files = 0
        
        for i, audio_file in enumerate(audio_files):
            print(f"\nğŸ“Š è¿›åº¦: {i+1}/{len(audio_files)}")
            
            result = self.process_audio_file(audio_file)
            if result is not None:
                results.append(result)
                successful_files += 1
                
                # ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ
                self.save_individual_result(result)
        
        # ç”Ÿæˆæ‘˜è¦
        try:
            summary = self.calculate_summary(results)
            self.generate_report(summary, results)
            return summary
        except Exception as e:
            print(f"âŒ è®¡ç®—æ‘˜è¦æ—¶å‡ºé”™: {e}")
            return {}
    
    def save_individual_result(self, result: Dict):
        """ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ"""
        try:
            filename_base = Path(result['filename']).stem
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            sr = result['sample_rate']
            sf.write(
                self.output_dir / f"{filename_base}_original_recon.wav",
                result['audio_original_recon'], sr
            )
            sf.write(
                self.output_dir / f"{filename_base}_enhanced_recon.wav", 
                result['audio_enhanced_recon'], sr
            )
            
            # ç”Ÿæˆå¯¹æ¯”å›¾
            self.create_comparison_plot(result, filename_base)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")
    
    def create_comparison_plot(self, result: Dict, filename_base: str):
        """ç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'éŸ³é¢‘é‡å»ºå¯¹æ¯” - {result["filename"]}', fontsize=16)
            
            # æ—¶åŸŸå¯¹æ¯”
            time_axis = np.linspace(0, len(result['audio_original'])/result['sample_rate'], 
                                  len(result['audio_original']))
            
            axes[0, 0].plot(time_axis, result['audio_original'], label='åŸå§‹éŸ³é¢‘', alpha=0.7)
            axes[0, 0].plot(time_axis[:len(result['audio_original_recon'])], 
                          result['audio_original_recon'], label='VAEé‡å»º', alpha=0.7)
            axes[0, 0].set_title('æ—¶åŸŸæ³¢å½¢å¯¹æ¯”')
            axes[0, 0].set_xlabel('æ—¶é—´ (s)')
            axes[0, 0].set_ylabel('å¹…åº¦')
            axes[0, 0].legend()
            axes[0, 0].grid(True)
            
            # å¢å¼ºé‡å»º
            axes[0, 1].plot(time_axis, result['audio_original'], label='åŸå§‹éŸ³é¢‘', alpha=0.7)
            axes[0, 1].plot(time_axis[:len(result['audio_enhanced_recon'])], 
                          result['audio_enhanced_recon'], label='å¢å¼ºé‡å»º', alpha=0.7)
            axes[0, 1].set_title('å¢å¼ºé‡å»ºå¯¹æ¯”')
            axes[0, 1].set_xlabel('æ—¶é—´ (s)')
            axes[0, 1].set_ylabel('å¹…åº¦')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
            
            # é¢‘åŸŸå¯¹æ¯”
            freqs_orig = np.abs(np.fft.fft(result['audio_original']))
            freqs_vae = np.abs(np.fft.fft(result['audio_original_recon'][:len(result['audio_original'])]))
            freqs_enhanced = np.abs(np.fft.fft(result['audio_enhanced_recon'][:len(result['audio_original'])]))
            
            freq_axis = np.fft.fftfreq(len(freqs_orig), 1/result['sample_rate'])[:len(freqs_orig)//2]
            
            axes[1, 0].semilogy(freq_axis, freqs_orig[:len(freq_axis)], label='åŸå§‹', alpha=0.7)
            axes[1, 0].semilogy(freq_axis, freqs_vae[:len(freq_axis)], label='VAEé‡å»º', alpha=0.7)
            axes[1, 0].set_title('é¢‘åŸŸå¯¹æ¯”')
            axes[1, 0].set_xlabel('é¢‘ç‡ (Hz)')
            axes[1, 0].set_ylabel('å¹…åº¦')
            axes[1, 0].legend()
            axes[1, 0].grid(True)
            
            # æŒ‡æ ‡å¯¹æ¯”
            metrics_names = list(result['original_metrics'].keys())
            orig_values = [result['original_metrics'][k] for k in metrics_names]
            enhanced_values = [result['enhanced_metrics'][k] for k in metrics_names]
            
            x = np.arange(len(metrics_names))
            width = 0.35
            
            axes[1, 1].bar(x - width/2, orig_values, width, label='åŸå§‹VAE', alpha=0.7)
            axes[1, 1].bar(x + width/2, enhanced_values, width, label='å¢å¼ºVAE', alpha=0.7)
            axes[1, 1].set_xlabel('æŒ‡æ ‡')
            axes[1, 1].set_ylabel('æ•°å€¼')
            axes[1, 1].set_title('è´¨é‡æŒ‡æ ‡å¯¹æ¯”')
            axes[1, 1].set_xticks(x)
            axes[1, 1].set_xticklabels(metrics_names, rotation=45)
            axes[1, 1].legend()
            axes[1, 1].grid(True)
            
            plt.tight_layout()
            plt.savefig(self.output_dir / f"{filename_base}_comparison.png", dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¯¹æ¯”å›¾æ—¶å‡ºé”™: {e}")
    
    def calculate_summary(self, results: List[Dict]) -> Dict:
        """è®¡ç®—æ‰¹é‡éªŒè¯æ‘˜è¦"""
        if not results:
            return {}
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {
            'total_files': len(results),
            'avg_processing_time_original': np.mean([r['processing_time_original'] for r in results]),
            'avg_processing_time_enhanced': np.mean([r['processing_time_enhanced'] for r in results]),
        }
        
        # è®¡ç®—å¹³å‡è´¨é‡æŒ‡æ ‡
        for metric_type in ['original_metrics', 'enhanced_metrics', 'improvements']:
            if metric_type in results[0]:
                for key in results[0][metric_type]:
                    values = [r[metric_type][key] for r in results if key in r[metric_type]]
                    if values:
                        avg_metrics[f"avg_{metric_type}_{key}"] = np.mean(values)
        
        return avg_metrics
    
    def generate_report(self, summary: Dict, results: List[Dict]):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        try:
            timestamp = int(time.time())
            report_path = self.output_dir / f"batch_validation_report_{timestamp}.md"
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("# AudioLDM2 VAEå¢å¼º - æ‰¹é‡éªŒè¯æŠ¥å‘Š\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                f.write("## éªŒè¯æ‘˜è¦\n\n")
                f.write(f"- æ€»æ–‡ä»¶æ•°: {summary.get('total_files', 0)}\n")
                f.write(f"- å¢å¼ºç³»æ•°: {self.best_boost_factor}x\n")
                f.write(f"- å¹³å‡å¤„ç†æ—¶é—´ (åŸå§‹): {summary.get('avg_processing_time_original', 0):.3f}s\n")
                f.write(f"- å¹³å‡å¤„ç†æ—¶é—´ (å¢å¼º): {summary.get('avg_processing_time_enhanced', 0):.3f}s\n\n")
                
                f.write("## è´¨é‡æŒ‡æ ‡å¯¹æ¯”\n\n")
                f.write("| æŒ‡æ ‡ | åŸå§‹VAE | å¢å¼ºVAE | æ”¹è¿› |\n")
                f.write("|------|---------|---------|------|\n")
                
                metrics = ['snr', 'high_freq_retention', 'spectral_correlation']
                for metric in metrics:
                    orig_key = f"avg_original_metrics_{metric}"
                    enhanced_key = f"avg_enhanced_metrics_{metric}"
                    improvement_key = f"avg_improvements_{metric}_improvement"
                    
                    if all(key in summary for key in [orig_key, enhanced_key, improvement_key]):
                        f.write(f"| {metric} | {summary[orig_key]:.3f} | "
                               f"{summary[enhanced_key]:.3f} | {summary[improvement_key]:.3f} |\n")
                
                f.write("\n## è¯¦ç»†ç»“æœ\n\n")
                for i, result in enumerate(results, 1):
                    f.write(f"### {i}. {result['filename']}\n\n")
                    f.write(f"- æ–‡ä»¶é•¿åº¦: {result['audio_length']:.2f}s\n")
                    f.write(f"- å¤„ç†æ—¶é—´: {result['processing_time_enhanced']:.3f}s\n")
                    
                    for metric in metrics:
                        if metric in result['original_metrics']:
                            orig = result['original_metrics'][metric]
                            enhanced = result['enhanced_metrics'][metric]
                            improvement = result['improvements'][f"{metric}_improvement"]
                            f.write(f"- {metric}: {orig:.3f} â†’ {enhanced:.3f} ({improvement:+.3f})\n")
                    f.write("\n")
                
                f.write("## ç»“è®º\n\n")
                if 'avg_improvements_snr_improvement' in summary:
                    snr_improvement = summary['avg_improvements_snr_improvement']
                    if snr_improvement > 0:
                        f.write("âœ… å¢å¼ºVAEåœ¨ä¿¡å™ªæ¯”æ–¹é¢æœ‰æ˜¾è‘—æ”¹å–„\n")
                    else:
                        f.write("âš ï¸ å¢å¼ºVAEåœ¨ä¿¡å™ªæ¯”æ–¹é¢ç•¥æœ‰ä¸‹é™ï¼ˆè¿™æ˜¯é«˜é¢‘å¢å¼ºçš„æ­£å¸¸ç°è±¡ï¼‰\n")
                
                if 'avg_improvements_high_freq_retention_improvement' in summary:
                    hf_improvement = summary['avg_improvements_high_freq_retention_improvement']
                    if hf_improvement > 50:
                        f.write("âœ… é«˜é¢‘ä¿æŒåº¦æœ‰æ˜¾è‘—æå‡\n")
                    elif hf_improvement > 10:
                        f.write("âœ… é«˜é¢‘ä¿æŒåº¦æœ‰é€‚åº¦æå‡\n")
                    else:
                        f.write("âš ï¸ é«˜é¢‘ä¿æŒåº¦æ”¹å–„æœ‰é™\n")
                
                f.write("\nğŸ“Š æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶å’Œå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° batch_validation_results/ ç›®å½•\n")
            
            print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
    
    def find_audio_files(self, directory: str = ".") -> List[str]:
        """æŸ¥æ‰¾ç›®å½•ä¸­çš„éŸ³é¢‘æ–‡ä»¶"""
        audio_extensions = ['.wav', '.mp3', '.flac', '.ogg', '.m4a']
        audio_files = []
        
        directory_path = Path(directory)
        for ext in audio_extensions:
            audio_files.extend(directory_path.glob(f"**/*{ext}"))
        
        return [str(f) for f in audio_files]

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ AudioLDM2 VAEå¢å¼º - Step 4: æ‰¹é‡éªŒè¯")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–éªŒè¯ç®¡é“
        validator = BatchValidationPipeline()
        
        # æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
        audio_files = validator.find_audio_files(".")
        print(f"ğŸ“ æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶ç”¨äºéªŒè¯:")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡ä»¶å
        for i, file in enumerate(audio_files[:5]):
            print(f"   ğŸ“„ {Path(file).name}")
        if len(audio_files) > 5:
            print(f"   ... è¿˜æœ‰ {len(audio_files) - 5} ä¸ªæ–‡ä»¶")
        
        # å¯ä»¥é™åˆ¶æ–‡ä»¶æ•°é‡ä»¥è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        if len(audio_files) > 20:
            print(f"ğŸ”„ ä¸ºäº†å¿«é€Ÿæµ‹è¯•ï¼Œä»…å¤„ç†å‰20ä¸ªæ–‡ä»¶")
            audio_files = audio_files[:20]
        
        # è¿‡æ»¤æ‰å¤ªå°çš„æ–‡ä»¶ï¼ˆé¿å…å¤„ç†ä¹‹å‰ç”Ÿæˆçš„é‡å»ºæ–‡ä»¶ï¼‰
        original_files = []
        for file in audio_files:
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œé¿å…å¤„ç†é‡å»ºçš„çŸ­æ–‡ä»¶
                file_size = Path(file).stat().st_size
                if file_size > 100000:  # å¤§äº100KB
                    original_files.append(file)
            except:
                continue
        
        if not original_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„éŸ³é¢‘æ–‡ä»¶")
            return
        
        print(f"ğŸ“ è¿‡æ»¤åç”¨äºéªŒè¯çš„æ–‡ä»¶: {len(original_files)} ä¸ª")
        
        # æ‰§è¡Œæ‰¹é‡éªŒè¯
        summary = validator.batch_validate(original_files)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nâœ… æ‰¹é‡éªŒè¯å®Œæˆ")
        print(f"   âœ… æˆåŠŸå¤„ç†: {summary.get('total_files', 0)}/{len(original_files)} æ–‡ä»¶")
        
        if 'avg_improvements_snr_improvement' in summary:
            print(f"   ğŸ“Š å¹³å‡SNRæ”¹è¿›: {summary['avg_improvements_snr_improvement']:.2f} dB")
        
        if 'avg_improvements_high_freq_retention_improvement' in summary:
            print(f"   ğŸ“Š å¹³å‡é«˜é¢‘ä¿æŒæ”¹è¿›: {summary['avg_improvements_high_freq_retention_improvement']:.1f}%")
        
        print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: batch_validation_results/")
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
