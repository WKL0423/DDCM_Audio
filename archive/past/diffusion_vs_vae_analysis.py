"""
VAE-only vs å®Œæ•´ Diffusion Pipeline å¯¹æ¯”åˆ†æ
é‡ç‚¹ï¼šè¯´æ˜ä¸ºä»€ä¹ˆä¹‹å‰çš„æ‰€æœ‰è„šæœ¬éƒ½æ˜¯ VAE-onlyï¼Œè€Œè¿™æ¬¡æ˜¯çœŸæ­£çš„ diffusion
"""

import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from diffusers import AudioLDM2Pipeline
import librosa
import warnings
warnings.filterwarnings("ignore")

class DiffusionVsVAEAnalysis:
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”¬ åˆå§‹åŒ– Diffusion vs VAE å¯¹æ¯”åˆ†æå™¨")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        
        # åŠ è½½ AudioLDM2 pipeline
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            "cvssp/audioldm2-music",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
    def vae_only_reconstruction(self, audio: torch.Tensor) -> torch.Tensor:
        """
        çœŸæ­£çš„ VAE-only é‡å»ºï¼ˆç±»ä¼¼ä¹‹å‰æ‰€æœ‰è„šæœ¬çš„æ–¹æ³•ï¼‰
        âš ï¸ è¿™åªæ˜¯ encode â†’ decodeï¼Œæ²¡æœ‰ diffusion è¿‡ç¨‹
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘
            
        Returns:
            mel_spectrogram: VAE é‡å»ºçš„ mel-spectrogramï¼ˆä¸æ˜¯éŸ³é¢‘ï¼‰
        """
        print("ğŸ”„ æ‰§è¡Œ VAE-only é‡å»ºï¼ˆencode â†’ decodeï¼‰...")
        
        with torch.no_grad():
            # è½¬æ¢ä¸º mel featuresï¼ˆä½¿ç”¨ ClapFeatureExtractorï¼‰
            if audio.is_cuda:
                audio = audio.cpu()
            if audio.dim() == 2:
                audio = audio.squeeze(0)
            
            audio_numpy = audio.numpy()
            
            # ä½¿ç”¨ ClapFeatureExtractor
            inputs = self.pipeline.feature_extractor(
                audio_numpy,
                sampling_rate=48000,
                return_tensors="pt"
            )
            
            mel_features = inputs["input_features"].to(self.device)
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            print(f"   ğŸ“Š è¾“å…¥ mel å½¢çŠ¶: {mel_features.shape}")
            
            # VAE encode
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode() if hasattr(latent_dist.latent_dist, 'mode') else latent_dist.latent_dist.sample()
            print(f"   ğŸ“Š æ½œåœ¨è¡¨ç¤ºå½¢çŠ¶: {latent.shape}")
            
            # VAE decode  
            reconstructed_mel = self.pipeline.vae.decode(latent).sample
            print(f"   ğŸ“Š é‡å»º mel å½¢çŠ¶: {reconstructed_mel.shape}")
            
            print("   âš ï¸ æ³¨æ„ï¼šVAE-only åªè¿”å› mel-spectrogramï¼Œéœ€è¦é¢å¤–çš„ vocoder è½¬æ¢ä¸ºéŸ³é¢‘")
            print("   âš ï¸ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¹‹å‰æ‰€æœ‰è„šæœ¬çš„éŸ³è´¨éƒ½æœ‰é™ - ç¼ºå°‘äº† diffusion è¿‡ç¨‹ï¼")
            
            return reconstructed_mel
    
    def full_diffusion_reconstruction(self, 
                                    reference_audio: torch.Tensor,
                                    prompt: str = "high quality music") -> torch.Tensor:
        """
        å®Œæ•´çš„ diffusion é‡å»º
        âœ… è¿™åŒ…å«äº†å®Œæ•´çš„ UNet å»å™ªè¿‡ç¨‹ï¼Œæ˜¯çœŸæ­£çš„ diffusion
        
        Args:
            reference_audio: å‚è€ƒéŸ³é¢‘ï¼ˆç”¨äºç¡®å®šé•¿åº¦ï¼‰
            prompt: æ–‡æœ¬æç¤º
            
        Returns:
            generated_audio: ç”Ÿæˆçš„éŸ³é¢‘
        """
        print("ğŸµ æ‰§è¡Œå®Œæ•´ Diffusion é‡å»ºï¼ˆåŒ…å« UNet å»å™ªï¼‰...")
        
        with torch.no_grad():
            # è®¡ç®—éŸ³é¢‘é•¿åº¦
            audio_length = reference_audio.shape[-1] / 48000.0
            audio_length = min(max(audio_length, 2.0), 10.0)
            
            print(f"   ğŸ“ æ–‡æœ¬æç¤º: {prompt}")
            print(f"   â±ï¸ ç›®æ ‡é•¿åº¦: {audio_length:.1f} ç§’")
            
            # å®Œæ•´çš„ diffusion pipeline
            # è¿™åŒ…å«ï¼šæ–‡æœ¬ç¼–ç  â†’ å™ªå£°é‡‡æ · â†’ UNet å»å™ª â†’ VAE è§£ç  â†’ éŸ³é¢‘è¾“å‡º
            result = self.pipeline(
                prompt=prompt,
                num_inference_steps=25,
                guidance_scale=7.5,
                audio_length_in_s=audio_length
            )
            
            generated_audio = result.audios[0]
            print(f"   ğŸ“Š ç”ŸæˆéŸ³é¢‘å½¢çŠ¶: {generated_audio.shape}")
            
            print("   âœ… å®Œæ•´ diffusion åŒ…å«ï¼š")
            print("      1. æ–‡æœ¬ç¼–ç ï¼ˆCLAP text encoderï¼‰")
            print("      2. éšæœºå™ªå£°é‡‡æ ·") 
            print("      3. UNet å»å™ªè¿‡ç¨‹ï¼ˆå¤šæ­¥è¿­ä»£ï¼‰")
            print("      4. VAE è§£ç ")
            print("      5. æœ€ç»ˆéŸ³é¢‘è¾“å‡º")
            
            return generated_audio
    
    def analyze_spectrograms(self, 
                           original_audio: torch.Tensor,
                           vae_mel: torch.Tensor,
                           diffusion_audio: torch.Tensor,
                           output_dir: str = "analysis_results"):
        """
        åˆ†æé¢‘è°±å›¾å·®å¼‚
        
        Args:
            original_audio: åŸå§‹éŸ³é¢‘
            vae_mel: VAE é‡å»ºçš„ mel-spectrogram
            diffusion_audio: diffusion ç”Ÿæˆçš„éŸ³é¢‘
            output_dir: è¾“å‡ºç›®å½•
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        print("\nğŸ“Š åˆ†æé¢‘è°±å›¾å·®å¼‚...")
        
        # ç¡®ä¿éŸ³é¢‘åœ¨ CPU ä¸Š
        if original_audio.is_cuda:
            original_audio = original_audio.cpu()
        if diffusion_audio.is_cuda:
            diffusion_audio = diffusion_audio.cpu()
        if vae_mel.is_cuda:
            vae_mel = vae_mel.cpu()
        
        # è½¬æ¢ä¸º numpy
        if original_audio.dim() > 1:
            original_audio = original_audio.squeeze()
        if diffusion_audio.dim() > 1:
            diffusion_audio = diffusion_audio.squeeze()
        
        original_np = original_audio.numpy()
        diffusion_np = diffusion_audio.numpy()
        
        # è®¡ç®—é¢‘è°±å›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # åŸå§‹éŸ³é¢‘é¢‘è°±
        D_orig = librosa.amplitude_to_db(np.abs(librosa.stft(original_np)), ref=np.max)
        librosa.display.specshow(D_orig, y_axis='hz', x_axis='time', ax=axes[0, 0])
        axes[0, 0].set_title('åŸå§‹éŸ³é¢‘é¢‘è°±')
        axes[0, 0].set_ylabel('é¢‘ç‡ (Hz)')
        
        # Diffusion ç”ŸæˆéŸ³é¢‘é¢‘è°±
        D_diff = librosa.amplitude_to_db(np.abs(librosa.stft(diffusion_np)), ref=np.max)
        librosa.display.specshow(D_diff, y_axis='hz', x_axis='time', ax=axes[0, 1])
        axes[0, 1].set_title('Diffusion ç”ŸæˆéŸ³é¢‘é¢‘è°±')
        
        # VAE mel-spectrogram
        vae_mel_np = vae_mel.squeeze().numpy()
        if vae_mel_np.ndim == 3:
            vae_mel_np = vae_mel_np[0]  # å–ç¬¬ä¸€ä¸ªé€šé“
        
        im = axes[0, 2].imshow(vae_mel_np, aspect='auto', origin='lower')
        axes[0, 2].set_title('VAE é‡å»º Mel-spectrogram')
        axes[0, 2].set_ylabel('Mel bins')
        plt.colorbar(im, ax=axes[0, 2])
        
        # é¢‘ç‡èƒ½é‡åˆ†å¸ƒå¯¹æ¯”
        freqs_orig = np.mean(np.abs(librosa.stft(original_np)), axis=1)
        freqs_diff = np.mean(np.abs(librosa.stft(diffusion_np)), axis=1)
        
        freq_bins = librosa.fft_frequencies(sr=16000)
        axes[1, 0].plot(freq_bins, freqs_orig, label='åŸå§‹éŸ³é¢‘', alpha=0.7)
        axes[1, 0].plot(freq_bins, freqs_diff, label='Diffusion éŸ³é¢‘', alpha=0.7)
        axes[1, 0].set_xlabel('é¢‘ç‡ (Hz)')
        axes[1, 0].set_ylabel('èƒ½é‡')
        axes[1, 0].set_title('é¢‘ç‡èƒ½é‡åˆ†å¸ƒå¯¹æ¯”')
        axes[1, 0].legend()
        axes[1, 0].set_xlim(0, 8000)
        
        # é«˜é¢‘èƒ½é‡å¯¹æ¯”
        high_freq_mask = freq_bins > 4000
        high_freq_orig = np.mean(freqs_orig[high_freq_mask])
        high_freq_diff = np.mean(freqs_diff[high_freq_mask])
        
        methods = ['åŸå§‹éŸ³é¢‘', 'DiffusionéŸ³é¢‘']
        high_freq_energy = [high_freq_orig, high_freq_diff]
        
        axes[1, 1].bar(methods, high_freq_energy, alpha=0.7)
        axes[1, 1].set_title('é«˜é¢‘èƒ½é‡å¯¹æ¯” (>4kHz)')
        axes[1, 1].set_ylabel('å¹³å‡èƒ½é‡')
        
        # å…³é”®å·®å¼‚è¯´æ˜
        axes[1, 2].text(0.1, 0.8, 
                        "ğŸ” å…³é”®å·®å¼‚åˆ†æ:\n\n"
                        "VAE-only é‡å»º:\n"
                        "â€¢ åªæœ‰ encode â†’ decode\n"
                        "â€¢ æ²¡æœ‰å»å™ªè¿‡ç¨‹\n"
                        "â€¢ è¾“å‡º mel-spectrogram\n"
                        "â€¢ éœ€è¦é¢å¤– vocoder\n"
                        "â€¢ è´¨é‡å— VAE ç“¶é¢ˆé™åˆ¶\n\n"
                        "å®Œæ•´ Diffusion:\n"
                        "â€¢ åŒ…å« UNet å»å™ª\n"
                        "â€¢ æ–‡æœ¬æ¡ä»¶å¼•å¯¼\n"
                        "â€¢ å¤šæ­¥è¿­ä»£ä¼˜åŒ–\n"
                        "â€¢ ç›´æ¥è¾“å‡ºéŸ³é¢‘\n"
                        "â€¢ è´¨é‡æ›´é«˜",
                        transform=axes[1, 2].transAxes,
                        fontsize=10,
                        verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        axes[1, 2].axis('off')
        
        plt.tight_layout()
        plt.savefig(output_path / "vae_vs_diffusion_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ğŸ“Š é¢‘è°±åˆ†æå›¾ä¿å­˜è‡³: {output_path / 'vae_vs_diffusion_analysis.png'}")
        
        # æ•°å€¼å¯¹æ¯”
        print(f"\nğŸ“ˆ å®šé‡åˆ†æ:")
        print(f"   é«˜é¢‘èƒ½é‡ (>4kHz):")
        print(f"      åŸå§‹éŸ³é¢‘: {high_freq_orig:.4f}")
        print(f"      Diffusion: {high_freq_diff:.4f}")
        print(f"      æ¯”å€¼: {high_freq_diff/high_freq_orig:.2f}")
        
    def comprehensive_comparison(self, input_audio_path: str = "AudioLDM2_Music_output.wav"):
        """
        ç»¼åˆå¯¹æ¯”åˆ†æ
        
        Args:
            input_audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
        """
        print("\n" + "="*60)
        print("ğŸ¯ VAE-only vs å®Œæ•´ Diffusion ç»¼åˆå¯¹æ¯”")
        print("="*60)
        
        # åŠ è½½éŸ³é¢‘
        if not Path(input_audio_path).exists():
            print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_audio_path}")
            return
        
        audio, sr = torchaudio.load(input_audio_path)
        print(f"ğŸ“‚ åŠ è½½éŸ³é¢‘: {audio.shape}, {sr}Hz")
        
        # é‡é‡‡æ ·åˆ° 48kHz
        if sr != 48000:
            resampler = torchaudio.transforms.Resample(sr, 48000)
            audio = resampler(audio)
        
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # é™åˆ¶é•¿åº¦
        max_length = 48000 * 8  # 8 ç§’
        if audio.shape[-1] > max_length:
            audio = audio[..., :max_length]
        
        print(f"ğŸ“Š å¤„ç†åéŸ³é¢‘: {audio.shape}")
        
        # 1. VAE-only é‡å»º
        print(f"\n1ï¸âƒ£ VAE-only é‡å»ºæµ‹è¯•")
        print("-" * 30)
        try:
            vae_mel = self.vae_only_reconstruction(audio.squeeze(0))
            print("   âœ… VAE-only æˆåŠŸ")
        except Exception as e:
            print(f"   âŒ VAE-only å¤±è´¥: {e}")
            vae_mel = None
        
        # 2. å®Œæ•´ Diffusion é‡å»º
        print(f"\n2ï¸âƒ£ å®Œæ•´ Diffusion é‡å»ºæµ‹è¯•")
        print("-" * 30)
        try:
            diffusion_audio = self.full_diffusion_reconstruction(
                audio.squeeze(0), 
                "high quality instrumental music with rich harmonics"
            )
            print("   âœ… Diffusion æˆåŠŸ")
        except Exception as e:
            print(f"   âŒ Diffusion å¤±è´¥: {e}")
            diffusion_audio = None
        
        # 3. ä¿å­˜ç»“æœ
        print(f"\n3ï¸âƒ£ ä¿å­˜ç»“æœ")
        print("-" * 30)
        
        output_dir = Path("vae_vs_diffusion_comparison")
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘
        if audio.dim() == 2:
            audio_save = audio
        else:
            audio_save = audio.unsqueeze(0)
        
        torchaudio.save(str(output_dir / "original_input.wav"), 
                       audio_save.cpu(), 16000)
        print(f"   ğŸ’¾ åŸå§‹éŸ³é¢‘: {output_dir / 'original_input.wav'}")
          # ä¿å­˜ diffusion ç»“æœ
        if diffusion_audio is not None:
            # è½¬æ¢ä¸º tensor å¦‚æœæ˜¯ numpy
            if isinstance(diffusion_audio, np.ndarray):
                diffusion_audio = torch.tensor(diffusion_audio)
            
            if diffusion_audio.dim() == 1:
                diffusion_save = diffusion_audio.unsqueeze(0)
            else:
                diffusion_save = diffusion_audio
                
            torchaudio.save(str(output_dir / "diffusion_output.wav"), 
                           diffusion_save.cpu(), 16000)
            print(f"   ğŸ’¾ Diffusion éŸ³é¢‘: {output_dir / 'diffusion_output.wav'}")
        
        # 4. é¢‘è°±åˆ†æ
        if vae_mel is not None and diffusion_audio is not None:
            print(f"\n4ï¸âƒ£ é¢‘è°±åˆ†æ")
            print("-" * 30)
            self.analyze_spectrograms(
                audio.squeeze(0),
                vae_mel,
                diffusion_audio,
                str(output_dir)
            )
        
        # 5. æ€»ç»“åˆ†æ
        print(f"\nğŸ“‹ æ€»ç»“åˆ†æ")
        print("-" * 30)
        print("ğŸ” æ–¹æ³•å¯¹æ¯”:")
        print("   VAE-only (ä¹‹å‰æ‰€æœ‰è„šæœ¬çš„æ–¹æ³•):")
        print("      â€¢ æµç¨‹: audio â†’ mel â†’ encode â†’ decode â†’ mel")
        print("      â€¢ é—®é¢˜: åªæœ‰ VAE ç“¶é¢ˆï¼Œæ— å»å™ªä¼˜åŒ–")
        print("      â€¢ è¾“å‡º: mel-spectrogram (éœ€è¦ vocoder)")
        print("      â€¢ è´¨é‡: å—é™äº VAE å‹ç¼©æŸå¤±")
        print("")
        print("   å®Œæ•´ Diffusion (æœ¬æ¬¡å®ç°):")
        print("      â€¢ æµç¨‹: prompt â†’ noise â†’ UNetå»å™ª â†’ VAEè§£ç  â†’ audio")
        print("      â€¢ ä¼˜åŠ¿: åŒ…å«å»å™ªä¼˜åŒ–å’Œæ–‡æœ¬å¼•å¯¼")
        print("      â€¢ è¾“å‡º: ç›´æ¥éŸ³é¢‘")
        print("      â€¢ è´¨é‡: æ›´é«˜çš„æ„ŸçŸ¥è´¨é‡")
        print("")
        print("ğŸ¯ å…³é”®å‘ç°:")
        print("   â€¢ ä¹‹å‰æ‰€æœ‰æµ‹è¯•éƒ½ç¼ºå°‘äº†çœŸæ­£çš„ diffusion è¿‡ç¨‹")
        print("   â€¢ VAE-only çš„é«˜é¢‘ä¸¢å¤±æ˜¯ç»“æ„æ€§é—®é¢˜")
        print("   â€¢ å®Œæ•´ diffusion èƒ½ç”Ÿæˆæ›´è‡ªç„¶çš„éŸ³é¢‘")
        print("   â€¢ ä½† diffusion ç”Ÿæˆçš„æ˜¯æ–°éŸ³é¢‘ï¼Œä¸æ˜¯é‡å»º")
        
        print(f"\nâœ… å¯¹æ¯”åˆ†æå®Œæˆï¼æŸ¥çœ‹ {output_dir} ç›®å½•è·å–è¯¦ç»†ç»“æœ")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = DiffusionVsVAEAnalysis()
    analyzer.comprehensive_comparison()

if __name__ == "__main__":
    main()
