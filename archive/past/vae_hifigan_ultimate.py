"""
AudioLDM2 HiFiGAN ç»ˆæä¿®å¤ç‰ˆæœ¬
============================

åŸºäºvocoderåˆ†æçš„ç²¾ç¡®ç»´åº¦åŒ¹é…
å·²é›†æˆå…¼å®¹æ€§éŸ³é¢‘ä¿å­˜
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F
from diffusers import AudioLDM2Pipeline

# å°è¯•å¯¼å…¥ soundfile ä»¥è·å¾—æ›´å¥½çš„å…¼å®¹æ€§
try:
    import soundfile as sf
    SOUNDFILE_AVAILABLE = True
except ImportError:
    SOUNDFILE_AVAILABLE = False
    print("âš ï¸ soundfile ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ torchaudio ä¿å­˜ï¼ˆå¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼‰")


def save_audio_compatible(audio_data, filepath, sample_rate=16000):
    """
    ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ soundfile ä»¥è·å¾—æœ€å¤§å…¼å®¹æ€§
    """
    # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯æ­£ç¡®çš„æ ¼å¼
    if isinstance(audio_data, torch.Tensor):
        audio_data = audio_data.detach().cpu().numpy()
    
    # ç¡®ä¿æ˜¯ 1D æ•°ç»„
    if len(audio_data.shape) > 1:
        audio_data = audio_data.flatten()
    
    # æ¸…ç†æ•°æ®
    audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=0.0, neginf=0.0)
    
    # å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
    if np.max(np.abs(audio_data)) > 0:
        audio_data = audio_data / np.max(np.abs(audio_data))
    
    success = False
    
    if SOUNDFILE_AVAILABLE:
        try:
            # ä½¿ç”¨ soundfile ä¿å­˜ä¸º PCM_16 æ ¼å¼ï¼ˆæœ€é«˜å…¼å®¹æ€§ï¼‰
            sf.write(filepath, audio_data, sample_rate, subtype='PCM_16')
            print(f"   âœ… ä½¿ç”¨ soundfile (PCM_16) ä¿å­˜: {Path(filepath).name}")
            success = True
        except Exception as e:
            print(f"   âš ï¸ soundfile ä¿å­˜å¤±è´¥: {e}")
    
    if not success:
        try:
            # å›é€€åˆ° torchaudio
            audio_tensor = torch.from_numpy(audio_data).unsqueeze(0).float()
            torchaudio.save(filepath, audio_tensor, sample_rate)
            print(f"   âœ… ä½¿ç”¨ torchaudio ä¿å­˜: {Path(filepath).name}")
            success = True
        except Exception as e:
            print(f"   âŒ torchaudio ä¿å­˜ä¹Ÿå¤±è´¥: {e}")
    
    return success

def test_audioldm2_hifigan_final(audio_path, max_length=5):
    """
    æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ï¼šåŸºäºvocoderåˆ†æç»“æœçš„ç²¾ç¡®å®ç°
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ¯ AudioLDM2 HiFiGAN ç»ˆæä¿®å¤æµ‹è¯•")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # åŠ è½½AudioLDM2
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2-music",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=sample_rate, duration=max_length)
    print(f"ğŸ“Š éŸ³é¢‘: {len(audio)/sr:.2f}ç§’")
    
    # åˆ›å»ºmelé¢‘è°± (64ç»´ï¼Œä¸AudioLDM2åŒ¹é…)
    mel_spec = librosa.feature.melspectrogram(
        y=audio, sr=sample_rate, n_mels=64, hop_length=160, n_fft=1024
    )
    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
    
    # å½’ä¸€åŒ–
    mel_min, mel_max = mel_spec.min(), mel_spec.max()
    mel_spec = 2 * (mel_spec - mel_min) / (mel_max - mel_min) - 1
    
    # è½¬æ¢ä¸ºtensor
    mel_tensor = torch.from_numpy(mel_spec).to(device)
    if device == "cuda":
        mel_tensor = mel_tensor.half()
    
    mel_input = mel_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, 64, time]
    print(f"ğŸµ Melè¾“å…¥: {mel_input.shape}")
    
    # VAEå¤„ç†
    with torch.no_grad():
        # ç¡®ä¿å°ºå¯¸åŒ¹é…VAEè¦æ±‚
        if mel_input.shape[-1] % 4 != 0:
            pad_length = 4 - (mel_input.shape[-1] % 4)
            mel_input = F.pad(mel_input, (0, pad_length))
        
        # VAEç¼–ç è§£ç 
        latent = vae.encode(mel_input).latent_dist.sample()
        reconstructed_mel = vae.decode(latent).sample
        
        print(f"ğŸ”„ VAEé‡å»º: {mel_input.shape} â†’ {reconstructed_mel.shape}")
        
        # å…³é”®ä¿®å¤ï¼šæ­£ç¡®çš„HiFiGANè¾“å…¥æ ¼å¼
        print("ğŸ¤ å‡†å¤‡HiFiGANè¾“å…¥...")
        
        # ä» [1, 1, 64, time] è½¬æ¢ä¸º [1, time, 64]
        vocoder_input = reconstructed_mel.squeeze()  # [64, time]
        print(f"   æ­¥éª¤1 - squeeze: {vocoder_input.shape}")
        
        if vocoder_input.dim() == 3:  # å¦‚æœè¿˜æœ‰batchç»´åº¦
            vocoder_input = vocoder_input.squeeze(0)  # [1, 64, time] -> [64, time]
            print(f"   æ­¥éª¤2 - å†æ¬¡squeeze: {vocoder_input.shape}")
        
        vocoder_input = vocoder_input.transpose(0, 1)  # [64, time] -> [time, 64]
        print(f"   æ­¥éª¤3 - transpose: {vocoder_input.shape}")
        
        vocoder_input = vocoder_input.unsqueeze(0)  # [time, 64] -> [1, time, 64]
        print(f"   æ­¥éª¤4 - æœ€ç»ˆæ ¼å¼: {vocoder_input.shape}")
        
        # æ•°æ®ç±»å‹åŒ¹é…
        if next(vocoder.parameters()).dtype == torch.float16:
            vocoder_input = vocoder_input.half()
        else:
            vocoder_input = vocoder_input.float()
        
        print(f"   æ•°æ®ç±»å‹: {vocoder_input.dtype}")
        
        # ä½¿ç”¨AudioLDM2å†…ç½®HiFiGAN
        try:
            print("ğŸš€ è°ƒç”¨AudioLDM2 HiFiGAN...")
            audio_tensor = vocoder(vocoder_input)
            reconstructed_audio = audio_tensor.squeeze().cpu().numpy()
            vocoder_method = "AudioLDM2_HiFiGAN_SUCCESS"
            print(f"âœ… æˆåŠŸï¼è¾“å‡º: {len(reconstructed_audio)}æ ·æœ¬")
            
        except Exception as e:
            print(f"âŒ ä»ç„¶å¤±è´¥: {e}")
            print("ğŸ”„ ä½¿ç”¨Griffin-Lim...")
            
            # Griffin-Limé™çº§
            mel_np = reconstructed_mel.squeeze().cpu().numpy()
            mel_denorm = (mel_np + 1) / 2 * (mel_max - mel_min) + mel_min
            mel_power = librosa.db_to_power(mel_denorm)
            
            reconstructed_audio = librosa.feature.inverse.mel_to_audio(
                mel_power, sr=sample_rate, hop_length=160, n_fft=1024
            )
            vocoder_method = "Griffin_Lim_Fallback"
            print(f"âœ… Griffin-LimæˆåŠŸ: {len(reconstructed_audio)}æ ·æœ¬")
      # ä¿å­˜ç»“æœ (ä½¿ç”¨å…¼å®¹æ€§ä¿å­˜æ–¹æ³•)
    output_dir = Path("vae_hifigan_final_test")
    output_dir.mkdir(exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜éŸ³é¢‘
    original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
    reconstructed_path = output_dir / f"{input_name}_{vocoder_method}_{timestamp}.wav"
    
    # å½’ä¸€åŒ–éŸ³é¢‘
    audio_norm = audio / (np.max(np.abs(audio)) + 1e-8)
    
    # ç¡®ä¿é‡å»ºéŸ³é¢‘é•¿åº¦åŒ¹é…
    if len(reconstructed_audio) > len(audio):
        reconstructed_audio = reconstructed_audio[:len(audio)]
    elif len(reconstructed_audio) < len(audio):
        reconstructed_audio = np.pad(reconstructed_audio, (0, len(audio) - len(reconstructed_audio)))
    
    recon_norm = reconstructed_audio / (np.max(np.abs(reconstructed_audio)) + 1e-8)
    
    # ä½¿ç”¨å…¼å®¹æ€§ä¿å­˜å‡½æ•°
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    save_audio_compatible(audio_norm, original_path, sample_rate)
    save_audio_compatible(recon_norm, reconstructed_path, sample_rate)
    
    # è®¡ç®—æŒ‡æ ‡
    mse = np.mean((audio - recon_norm) ** 2)
    snr = 10 * np.log10(np.mean(audio ** 2) / (mse + 1e-10))
    correlation = np.corrcoef(audio, recon_norm)[0, 1] if len(audio) > 1 else 0
    
    # è¾“å‡ºç»“æœ
    print(f"\n{'='*60}")
    print(f"ğŸ¯ AudioLDM2 HiFiGAN ç»ˆææµ‹è¯•ç»“æœ")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    print(f"ğŸ“ é‡å»ºéŸ³é¢‘: {reconstructed_path}")
    print(f"ğŸ“Š SNR: {snr:.2f} dB")
    print(f"ğŸ“Š ç›¸å…³ç³»æ•°: {correlation:.4f}")
    print(f"ğŸ¤ é‡å»ºæ–¹æ³•: {vocoder_method}")
    
    # å…³é”®ç»“è®º
    if vocoder_method == "AudioLDM2_HiFiGAN_SUCCESS":
        print(f"\nğŸ‰ é‡å¤§çªç ´ï¼")
        print(f"âœ… æˆåŠŸä½¿ç”¨AudioLDM2å†…ç½®HiFiGAN")
        print(f"âœ… ç»•è¿‡äº†Griffin-Limçš„92%ä¿¡æ¯æŸå¤±ç“¶é¢ˆ")
        print(f"ğŸ“ˆ é¢„æœŸè´¨é‡æå‡æ˜¾è‘—")
        
        if snr > 5:
            print(f"ğŸ† é‡å»ºè´¨é‡ä¼˜ç§€ï¼")
        elif snr > 0:
            print(f"âœ… é‡å»ºè´¨é‡è‰¯å¥½")
        else:
            print(f"âš ï¸ ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä½†å·²æ˜¯é‡å¤§è¿›æ­¥")
    else:
        print(f"\nâš ï¸ HiFiGANé›†æˆä»æœ‰æŠ€æœ¯éšœç¢")
        print(f"ğŸ“Š å½“å‰ä½¿ç”¨: {vocoder_method}")
        print(f"ğŸ”¬ éœ€è¦æ›´æ·±å…¥çš„AudioLDM2å†…éƒ¨ç ”ç©¶")
    
    return {
        'snr': snr,
        'correlation': correlation,
        'vocoder_method': vocoder_method,
        'original_path': str(original_path),
        'reconstructed_path': str(reconstructed_path)
    }

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        audio_files = list(Path('.').glob('*.wav'))
        if audio_files:
            print("æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶:")
            for i, file in enumerate(audio_files[:3], 1):
                print(f"{i}. {file.name}")
            
            try:
                choice = int(input("é€‰æ‹©æ–‡ä»¶: "))
                audio_path = str(audio_files[choice-1])
            except:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
    else:
        audio_path = sys.argv[1]
    
    print(f"ğŸš€ å¼€å§‹AudioLDM2 HiFiGANç»ˆææµ‹è¯•")
    
    try:
        result = test_audioldm2_hifigan_final(audio_path)
        
        print(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print(f"   æ–¹æ³•: {result['vocoder_method']}")
        print(f"   SNR: {result['snr']:.2f}dB")
        print(f"   ç›¸å…³æ€§: {result['correlation']:.4f}")
        
        if result['vocoder_method'] == "AudioLDM2_HiFiGAN_SUCCESS":
            print(f"\nğŸŠ æ­å–œï¼å·²çªç ´HiFiGANé›†æˆæŠ€æœ¯ç“¶é¢ˆï¼")
        else:
            print(f"\nğŸ” ä»åœ¨æ¢ç´¢HiFiGANé›†æˆçš„æœ€ä½³æ–¹æ¡ˆ...")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
