"""
ä¿®å¤Vocoderç»´åº¦é—®é¢˜çš„VAEæµ‹è¯•
è§£å†³ "expected input[1, 504, 64] to have 64 channels, but got 504 channels" é”™è¯¯

é—®é¢˜åˆ†æï¼š
- VocoderæœŸæœ›è¾“å…¥: [batch, channels, time] = [1, 64, time_steps]
- å½“å‰è¾“å…¥: [1, time_steps, 64] - ç»´åº¦é¡ºåºé”™è¯¯

è§£å†³æ–¹æ¡ˆï¼š
1. æ­£ç¡®è½¬ç½®mel-spectrogramç»´åº¦
2. ä½¿ç”¨é€‚å½“çš„vocoderé…ç½®
3. æ·»åŠ ç»´åº¦æ£€æŸ¥å’Œä¿®å¤
"""

import torch
import librosa
import numpy as np
import os
import sys
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F

from diffusers import AudioLDM2Pipeline


def calculate_metrics(original, reconstructed):
    """è®¡ç®—é‡å»ºè´¨é‡æŒ‡æ ‡"""
    if len(original) != len(reconstructed):
        min_len = min(len(original), len(reconstructed))
        original = original[:min_len]
        reconstructed = reconstructed[:min_len]
    
    # SNRè®¡ç®—
    noise = reconstructed - original
    signal_power = np.mean(original ** 2)
    noise_power = np.mean(noise ** 2)
    snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
    
    # ç›¸å…³ç³»æ•°
    correlation = np.corrcoef(original, reconstructed)[0, 1] if len(original) > 1 else 0
    
    return snr, correlation


def audio_to_mel_fixed(audio, sample_rate=16000, n_mels=64, n_fft=1024, hop_length=160):
    """
    éŸ³é¢‘è½¬mel-spectrogramï¼Œç¡®ä¿æ­£ç¡®çš„ç»´åº¦è¾“å‡º
    """
    # è®¡ç®—mel-spectrogram
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sample_rate,
        n_mels=n_mels,
        n_fft=n_fft,
        hop_length=hop_length,
        window='hann',
        center=True,
        pad_mode='reflect',
        power=2.0
    )
    
    # è½¬æ¢åˆ°å¯¹æ•°åŸŸ
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    
    # å½’ä¸€åŒ–åˆ°[-1, 1]
    mel_spec_norm = 2.0 * (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min()) - 1.0
    
    print(f"   Mel-spec shape: {mel_spec_norm.shape}")
    print(f"   Mel-spec range: [{mel_spec_norm.min():.3f}, {mel_spec_norm.max():.3f}]")
    
    return mel_spec_norm


def mel_to_audio_vocoder_fixed(mel_spec, vocoder, device):
    """
    ä½¿ç”¨vocoderè½¬æ¢mel-spectrogramåˆ°éŸ³é¢‘ï¼Œä¿®å¤ç»´åº¦é—®é¢˜
    """
    print(f"ğŸ¤ ä½¿ç”¨ä¿®å¤çš„vocoder...")
    
    # ç¡®ä¿mel_specæ˜¯numpyæ•°ç»„
    if isinstance(mel_spec, torch.Tensor):
        mel_spec = mel_spec.cpu().numpy()
    
    # ç¡®ä¿æ­£ç¡®çš„ç»´åº¦é¡ºåº: [channels, time] -> [batch, channels, time]
    if mel_spec.ndim == 2:
        # ä» [n_mels, time] è½¬æ¢ä¸º [1, n_mels, time]
        mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).float().to(device)
    else:
        raise ValueError(f"Unexpected mel_spec dimensions: {mel_spec.shape}")
    
    print(f"   ä¿®å¤åmel tensor shape: {mel_tensor.shape}")
    print(f"   æœŸæœ›vocoderè¾“å…¥: [batch, channels={vocoder.config.model_in_dim}, time]")
    
    # æ£€æŸ¥é€šé“æ•°æ˜¯å¦åŒ¹é…
    expected_channels = vocoder.config.model_in_dim
    actual_channels = mel_tensor.shape[1]
    
    if actual_channels != expected_channels:
        print(f"   âš ï¸ é€šé“æ•°ä¸åŒ¹é…: å®é™…{actual_channels}, æœŸæœ›{expected_channels}")
        # å°è¯•è°ƒæ•´é€šé“æ•°
        if actual_channels < expected_channels:
            # é€šè¿‡é‡å¤æ‰©å±•é€šé“
            repeat_factor = expected_channels // actual_channels
            mel_tensor = mel_tensor.repeat(1, repeat_factor, 1)
            if mel_tensor.shape[1] < expected_channels:
                # å¦‚æœè¿˜ä¸å¤Ÿï¼Œè¡¥é›¶
                padding = expected_channels - mel_tensor.shape[1]
                mel_tensor = F.pad(mel_tensor, (0, 0, 0, padding))
        else:
            # è£å‰ªå¤šä½™é€šé“
            mel_tensor = mel_tensor[:, :expected_channels, :]
        
        print(f"   è°ƒæ•´åshape: {mel_tensor.shape}")
    
    try:
        with torch.no_grad():
            # ä½¿ç”¨vocoderç”ŸæˆéŸ³é¢‘
            audio_tensor = vocoder(mel_tensor)
            
            if isinstance(audio_tensor, tuple):
                audio_tensor = audio_tensor[0]
            
            # è½¬æ¢ä¸ºnumpy
            audio = audio_tensor.squeeze().cpu().numpy()
            print(f"   âœ… VocoderæˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {audio.shape}")
            return audio
            
    except Exception as e:
        print(f"   âŒ Vocoderå¤±è´¥: {e}")
        return None


def mel_to_audio_griffinlim(mel_spec, sample_rate=16000, n_fft=1024, hop_length=160):
    """Griffin-Limé‡å»ºéŸ³é¢‘"""
    # åå½’ä¸€åŒ–
    mel_spec_denorm = (mel_spec + 1.0) / 2.0
    mel_spec_denorm = mel_spec_denorm * 80.0 - 80.0  # å‡è®¾åŸå§‹èŒƒå›´æ˜¯-80åˆ°0 dB
    
    # è½¬æ¢å›åŠŸç‡åŸŸ
    mel_spec_power = librosa.db_to_power(mel_spec_denorm, ref=1.0)
    
    # ä½¿ç”¨Griffin-Limé‡å»º
    audio = librosa.feature.inverse.mel_to_audio(
        mel_spec_power,
        sr=sample_rate,
        n_fft=n_fft,
        hop_length=hop_length,
        window='hann',
        center=True,
        pad_mode='reflect',
        n_iter=32
    )
    
    return audio


def test_vae_reconstruction_fixed(audio_path, model_id="cvssp/audioldm2-music", max_length=5):
    """
    ä¿®å¤ç‰ˆæœ¬çš„VAEéŸ³é¢‘é‡å»ºæµ‹è¯•
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")
    
    if not os.path.exists(audio_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ {audio_path}")
        return
    
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½ AudioLDM2 æ¨¡å‹: {model_id}")
    pipeline = AudioLDM2Pipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    # æ‰“å°vocoderé…ç½®ä¿¡æ¯
    print(f"ğŸ”§ Vocoderé…ç½®:")
    print(f"   ç±»å‹: {type(vocoder).__name__}")
    print(f"   è¾“å…¥ç»´åº¦: {vocoder.config.model_in_dim}")
    print(f"   é‡‡æ ·ç‡: {vocoder.config.sampling_rate}")
    print(f"   ä¸Šé‡‡æ ·ç‡: {vocoder.config.upsample_rates}")
    
    # åŠ è½½éŸ³é¢‘
    print(f"ğŸ“ æ­£åœ¨åŠ è½½éŸ³é¢‘: {audio_path}")
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    
    if len(audio) > max_length * sample_rate:
        audio = audio[:int(max_length * sample_rate)]
        print(f"âœ‚ï¸ éŸ³é¢‘å·²è£å‰ªåˆ° {max_length} ç§’")
    
    print(f"ğŸ“Š éŸ³é¢‘ä¿¡æ¯: é•¿åº¦={len(audio)/sample_rate:.2f}ç§’, æ ·æœ¬æ•°={len(audio)}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "vae_fixed_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    audio_save = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio
    torchaudio.save(original_path, torch.from_numpy(audio_save).unsqueeze(0), sample_rate)
    print(f"ğŸ’¾ åŸå§‹éŸ³é¢‘ä¿å­˜: {original_path}")
    
    # æµ‹è¯•æ­¥éª¤
    results = {}
    
    print(f"\\nğŸ”¬ å¼€å§‹VAEé‡å»ºæµ‹è¯•...")
    
    # æ­¥éª¤1: éŸ³é¢‘ -> Mel-spectrogram
    print(f"\\nğŸ“Š æ­¥éª¤1: éŸ³é¢‘ -> Mel-spectrogram")
    start_time = time.time()
    mel_spec = audio_to_mel_fixed(audio, sample_rate)
    mel_time = time.time() - start_time
    print(f"   âœ… Mel-specç”Ÿæˆå®Œæˆ ({mel_time:.2f}ç§’)")
    
    # æ­¥éª¤2: Mel -> VAEæ½œåœ¨ç©ºé—´
    print(f"\\nğŸ§  æ­¥éª¤2: Mel -> VAEæ½œåœ¨ç©ºé—´")
    start_time = time.time()
      # å‡†å¤‡VAEè¾“å…¥ - ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
    mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).unsqueeze(0)
    
    # ç¡®ä¿æ•°æ®ç±»å‹ä¸æ¨¡å‹åŒ¹é…
    if next(vae.parameters()).dtype == torch.float16:
        mel_tensor = mel_tensor.half()
    else:
        mel_tensor = mel_tensor.float()
    
    mel_tensor = mel_tensor.to(device)
    print(f"   VAEè¾“å…¥å½¢çŠ¶: {mel_tensor.shape}, æ•°æ®ç±»å‹: {mel_tensor.dtype}")
    
    with torch.no_grad():
        # VAEç¼–ç 
        latent = vae.encode(mel_tensor).latent_dist.sample()
        print(f"   æ½œåœ¨å‘é‡å½¢çŠ¶: {latent.shape}")
        
        # VAEè§£ç 
        decoded = vae.decode(latent).sample
        print(f"   VAEè§£ç å½¢çŠ¶: {decoded.shape}")
    
    vae_time = time.time() - start_time
    print(f"   âœ… VAEç¼–ç /è§£ç å®Œæˆ ({vae_time:.2f}ç§’)")
    
    # è½¬æ¢è§£ç ç»“æœä¸ºnumpy
    decoded_mel = decoded.squeeze().cpu().numpy()
    print(f"   è§£ç melå½¢çŠ¶: {decoded_mel.shape}")
    
    # æ­¥éª¤3a: ä½¿ç”¨ä¿®å¤çš„Vocoderé‡å»º
    print(f"\\nğŸ¤ æ­¥éª¤3a: ä½¿ç”¨ä¿®å¤çš„Vocoderé‡å»º")
    start_time = time.time()
    
    vocoder_audio = mel_to_audio_vocoder_fixed(decoded_mel, vocoder, device)
    
    if vocoder_audio is not None:
        vocoder_time = time.time() - start_time
        print(f"   âœ… Vocoderé‡å»ºå®Œæˆ ({vocoder_time:.2f}ç§’)")
        
        # è®¡ç®—æŒ‡æ ‡
        snr_vocoder, corr_vocoder = calculate_metrics(audio, vocoder_audio)
        
        # ä¿å­˜ç»“æœ
        vocoder_path = os.path.join(output_dir, f"{input_name}_vocoder_fixed_{timestamp}.wav")
        audio_norm = vocoder_audio / np.max(np.abs(vocoder_audio)) if np.max(np.abs(vocoder_audio)) > 0 else vocoder_audio
        torchaudio.save(vocoder_path, torch.from_numpy(audio_norm).unsqueeze(0), sample_rate)
        
        results['vocoder'] = {
            'path': vocoder_path,
            'snr': snr_vocoder,
            'correlation': corr_vocoder,
            'time': vocoder_time
        }
        
        print(f"   ğŸ“ˆ SNR: {snr_vocoder:.2f} dB")
        print(f"   ğŸ”— ç›¸å…³ç³»æ•°: {corr_vocoder:.4f}")
        print(f"   ğŸ’¾ ä¿å­˜: {vocoder_path}")
    else:
        print(f"   âŒ Vocoderé‡å»ºå¤±è´¥")
    
    # æ­¥éª¤3b: ä½¿ç”¨Griffin-Limé‡å»ºä½œä¸ºå¯¹æ¯”
    print(f"\\nğŸµ æ­¥éª¤3b: ä½¿ç”¨Griffin-Limé‡å»º")
    start_time = time.time()
    
    griffinlim_audio = mel_to_audio_griffinlim(decoded_mel, sample_rate)
    griffinlim_time = time.time() - start_time
    
    # è®¡ç®—æŒ‡æ ‡
    snr_gl, corr_gl = calculate_metrics(audio, griffinlim_audio)
    
    # ä¿å­˜ç»“æœ
    gl_path = os.path.join(output_dir, f"{input_name}_griffinlim_{timestamp}.wav")
    audio_norm = griffinlim_audio / np.max(np.abs(griffinlim_audio)) if np.max(np.abs(griffinlim_audio)) > 0 else griffinlim_audio
    torchaudio.save(gl_path, torch.from_numpy(audio_norm).unsqueeze(0), sample_rate)
    
    results['griffinlim'] = {
        'path': gl_path,
        'snr': snr_gl,
        'correlation': corr_gl,
        'time': griffinlim_time
    }
    
    print(f"   âœ… Griffin-Limé‡å»ºå®Œæˆ ({griffinlim_time:.2f}ç§’)")
    print(f"   ğŸ“ˆ SNR: {snr_gl:.2f} dB")
    print(f"   ğŸ”— ç›¸å…³ç³»æ•°: {corr_gl:.4f}")
    print(f"   ğŸ’¾ ä¿å­˜: {gl_path}")
    
    # æ‰“å°æ€»ç»“
    print(f"\\n{'='*60}")
    print(f"ğŸ¯ VAEé‡å»ºæµ‹è¯•ç»“æœæ€»ç»“")
    print(f"{'='*60}")
    print(f"ğŸ“ åŸå§‹éŸ³é¢‘: {original_path}")
    
    if 'vocoder' in results:
        vocoder_res = results['vocoder']
        print(f"\\nğŸ¤ ä¿®å¤Vocoderæ–¹æ³•:")
        print(f"   ğŸ“„ æ–‡ä»¶: {vocoder_res['path']}")
        print(f"   ğŸ“ˆ SNR: {vocoder_res['snr']:.2f} dB")
        print(f"   ğŸ”— ç›¸å…³ç³»æ•°: {vocoder_res['correlation']:.4f}")
        print(f"   â±ï¸ å¤„ç†æ—¶é—´: {vocoder_res['time']:.2f}ç§’")
    
    gl_res = results['griffinlim']
    print(f"\\nğŸµ Griffin-Limæ–¹æ³•:")
    print(f"   ğŸ“„ æ–‡ä»¶: {gl_res['path']}")
    print(f"   ğŸ“ˆ SNR: {gl_res['snr']:.2f} dB")
    print(f"   ğŸ”— ç›¸å…³ç³»æ•°: {gl_res['correlation']:.4f}")
    print(f"   â±ï¸ å¤„ç†æ—¶é—´: {gl_res['time']:.2f}ç§’")
    
    if 'vocoder' in results:
        snr_diff = results['vocoder']['snr'] - results['griffinlim']['snr']
        print(f"\\nğŸš€ æ”¹è¿›æ•ˆæœ:")
        print(f"   ğŸ“ˆ SNRæ”¹è¿›: {snr_diff:+.2f} dB")
        print(f"   ğŸ† æ›´å¥½æ–¹æ³•: {'Vocoder' if snr_diff > 0 else 'Griffin-Lim'}")
    
    print(f"\\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨ {output_dir}/ ç›®å½•")
    print(f"ğŸ§ å»ºè®®æ’­æ”¾éŸ³é¢‘æ–‡ä»¶æ¥ä¸»è§‚è¯„ä¼°è´¨é‡å·®å¼‚")
    print(f"\\nâœ… ä¿®å¤æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        audio_path = sys.argv[1]
    else:
        audio_path = "AudioLDM2_Music_output.wav"
    
    print(f"ğŸš€ å¼€å§‹ä¿®å¤ç‰ˆVAEæµ‹è¯•: {audio_path}")
    test_vae_reconstruction_fixed(audio_path)
