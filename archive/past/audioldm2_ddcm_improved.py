#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆAudioLDM2 DDCM - æ›´å¼ºçš„è¾“å…¥ç›¸å…³æ€§
é€šè¿‡ä»¥ä¸‹æ–¹å¼æé«˜ç›¸å…³æ€§ï¼š
1. æ›´å¤§çš„ç æœ¬å¤§å°
2. æ›´ä¿å®ˆçš„é‡åŒ–ç­–ç•¥
3. æ”¹è¿›çš„diffusionå¼•å¯¼
4. å¤šé˜¶æ®µé‡å»º
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from diffusers import AudioLDM2Pipeline, DDPMScheduler
import torchaudio
from pathlib import Path
import json
import time
import soundfile as sf
import librosa
import warnings
warnings.filterwarnings("ignore")

class ImprovedAudioDDCMCodebook(nn.Module):
    """
    æ”¹è¿›çš„éŸ³é¢‘DDCMç æœ¬
    """
    
    def __init__(self, 
                 codebook_size: int = 1024,  # å¢å¤§ç æœ¬
                 latent_shape: Tuple[int, int, int] = (8, 250, 16),
                 temperature: float = 0.1):  # æ·»åŠ æ¸©åº¦å‚æ•°
        super().__init__()
        
        self.codebook_size = codebook_size
        self.latent_shape = latent_shape
        self.latent_dim = np.prod(latent_shape)
        self.temperature = temperature
        
        # åˆ›å»ºç æœ¬ï¼šä½¿ç”¨æ›´å°çš„æ ‡å‡†å·®åˆå§‹åŒ–
        self.register_buffer("codebook", torch.randn(codebook_size, *latent_shape) * 0.5)
        self.register_buffer("usage_count", torch.zeros(codebook_size))
        
        print(f"ğŸ”§ æ”¹è¿›Audio DDCMç æœ¬åˆå§‹åŒ–:")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {codebook_size}")
        print(f"   ğŸ“ Latentå½¢çŠ¶: {latent_shape}")
        print(f"   ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°: {temperature}")
    
    def soft_quantize_latent(self, latent: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è½¯é‡åŒ–ç­–ç•¥ï¼šä¿æŒæ›´å¤šåŸå§‹ä¿¡æ¯
        """
        batch_size = latent.shape[0]
        
        # å±•å¹³è®¡ç®—è·ç¦»
        latent_flat = latent.view(batch_size, -1).float()
        codebook_flat = self.codebook.view(self.codebook_size, -1).float()
        
        # è®¡ç®—L2è·ç¦»
        distances = torch.cdist(latent_flat, codebook_flat, p=2)
        
        # æ‰¾åˆ°æœ€è¿‘çš„å‡ ä¸ªç æœ¬å‘é‡
        k = min(5, self.codebook_size)  # å–æœ€è¿‘çš„5ä¸ª
        top_k_distances, top_k_indices = torch.topk(distances, k, dim=1, largest=False)
        
        # ä½¿ç”¨æ¸©åº¦ç¼©æ”¾çš„softmaxæƒé‡
        weights = F.softmax(-top_k_distances / self.temperature, dim=1)
        
        # åŠ æƒç»„åˆç æœ¬å‘é‡
        quantized = torch.zeros_like(latent)
        for i in range(batch_size):
            weighted_sum = torch.zeros_like(self.codebook[0])
            for j in range(k):
                idx = top_k_indices[i, j]
                weight = weights[i, j]
                weighted_sum += weight * self.codebook[idx]
            quantized[i] = weighted_sum
        
        # æ›´æ–°ä½¿ç”¨è®¡æ•°
        with torch.no_grad():
            for i in range(batch_size):
                for j in range(k):
                    idx = top_k_indices[i, j]
                    self.usage_count[idx] += weights[i, j].item()
        
        # è¿”å›ä¸»è¦ç´¢å¼•ï¼ˆæœ€è¿‘çš„é‚£ä¸ªï¼‰
        main_indices = top_k_indices[:, 0]
        main_distances = top_k_distances[:, 0]
        
        return quantized, main_indices, main_distances
    
    def hard_quantize_latent(self, latent: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ç¡¬é‡åŒ–ç­–ç•¥ï¼ˆåŸå§‹æ–¹æ³•ï¼‰
        """
        batch_size = latent.shape[0]
        
        latent_flat = latent.view(batch_size, -1).float()
        codebook_flat = self.codebook.view(self.codebook_size, -1).float()
        
        distances = torch.cdist(latent_flat, codebook_flat, p=2)
        min_distances, indices = torch.min(distances, dim=1)
        quantized = self.codebook[indices].to(latent.dtype)
        
        with torch.no_grad():
            for idx in indices:
                self.usage_count[idx] += 1
        
        return quantized, indices, min_distances

class ImprovedAudioLDM2_DDCM:
    """
    æ”¹è¿›çš„åŸºäºè¾“å…¥éŸ³é¢‘çš„AudioLDM2 DDCMç®¡é“
    """
    
    def __init__(self, 
                 model_name: str = "cvssp/audioldm2-music",
                 codebook_size: int = 512):
        """
        åˆå§‹åŒ–æ”¹è¿›çš„DDCMç®¡é“
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸµ åˆå§‹åŒ–æ”¹è¿›çš„åŸºäºè¾“å…¥éŸ³é¢‘çš„DDCMç®¡é“")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {codebook_size}")
        
        # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        # åˆ›å»ºæ”¹è¿›çš„DDCMç æœ¬
        self.ddcm_codebook = ImprovedAudioDDCMCodebook(
            codebook_size=codebook_size,
            latent_shape=(8, 250, 16),
            temperature=0.1
        ).to(self.device)
        
        print(f"âœ… æ”¹è¿›çš„DDCMç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    def process_input_audio_improved(self, audio_path: str, prompt: str = "high quality music") -> Dict:
        """
        æ”¹è¿›çš„è¾“å…¥éŸ³é¢‘å¤„ç†æµç¨‹
        """
        print(f"ğŸµ æ”¹è¿›DDCMè¾“å…¥éŸ³é¢‘å¤„ç†: {Path(audio_path).name}")
        print(f"   ğŸ“ é‡å»ºæç¤º: {prompt}")
        
        # 1. ç¼–ç è¾“å…¥éŸ³é¢‘
        input_latent = self._encode_input_audio(audio_path)
        
        # 2. ä¸¤ç§é‡åŒ–ç­–ç•¥å¯¹æ¯”
        soft_quantized, soft_indices, soft_distances = self.ddcm_codebook.soft_quantize_latent(input_latent)
        hard_quantized, hard_indices, hard_distances = self.ddcm_codebook.hard_quantize_latent(input_latent)
        
        print(f"   ğŸ“Š é‡åŒ–ç»“æœå¯¹æ¯”:")
        print(f"   - è½¯é‡åŒ–å¹³å‡è·ç¦»: {soft_distances.mean():.4f}")
        print(f"   - ç¡¬é‡åŒ–å¹³å‡è·ç¦»: {hard_distances.mean():.4f}")
        
        # 3. å¤šç§é‡å»ºæ–¹æ³•
        results = {}
        
        # åŸå§‹VAEé‡å»º
        results['original_vae'] = self._reconstruct_with_vae(input_latent, "Original_VAE")
        
        # è½¯é‡åŒ–VAEé‡å»º
        results['soft_quantized_vae'] = self._reconstruct_with_vae(soft_quantized, "Soft_Quantized_VAE")
        
        # ç¡¬é‡åŒ–VAEé‡å»º
        results['hard_quantized_vae'] = self._reconstruct_with_vae(hard_quantized, "Hard_Quantized_VAE")
        
        # æ”¹è¿›çš„DDCM diffusionï¼ˆä½¿ç”¨è½¯é‡åŒ–ï¼‰
        results['improved_ddcm_diffusion'] = self._reconstruct_with_improved_ddcm_diffusion(
            soft_quantized, input_latent, prompt, "Improved_DDCM_Diffusion"
        )
        
        # æ··åˆé‡å»ºï¼ˆè½¯é‡åŒ– + åŸå§‹çš„åŠ æƒç»„åˆï¼‰
        alpha = 0.7  # è½¯é‡åŒ–æƒé‡
        mixed_latent = alpha * soft_quantized + (1 - alpha) * input_latent
        results['mixed_reconstruction'] = self._reconstruct_with_vae(mixed_latent, "Mixed_Reconstruction")
        
        # 4. åŠ è½½åŸå§‹éŸ³é¢‘
        original_audio, sr = torchaudio.load(audio_path)
        if sr != 16000:
            resampler = torchaudio.transforms.Resample(sr, 16000)
            original_audio = resampler(original_audio)
        
        if original_audio.shape[0] > 1:
            original_audio = original_audio.mean(dim=0, keepdim=True)
        
        original_audio = original_audio.squeeze().numpy()
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘
        output_dir = Path("improved_ddcm_output")
        output_dir.mkdir(exist_ok=True)
        timestamp = int(time.time())
        
        original_path = output_dir / f"original_{timestamp}.wav"
        sf.write(str(original_path), original_audio, 16000)
        
        # 5. è´¨é‡åˆ†æ
        analysis = self._analyze_reconstruction_quality(original_audio, results)
        
        # 6. æ•´åˆç»“æœ
        final_result = {
            "input_file": audio_path,
            "prompt": prompt,
            "soft_quantization_distance": soft_distances.mean().item(),
            "hard_quantization_distance": hard_distances.mean().item(),
            "original_audio_path": str(original_path),
            "reconstructions": results,
            "quality_analysis": analysis,
            "codebook_usage": {
                "used_codes": (self.ddcm_codebook.usage_count > 0).sum().item(),
                "total_codes": self.ddcm_codebook.codebook_size,
            }
        }
        
        # 7. æ˜¾ç¤ºç»“æœ
        self._display_improved_results(final_result)
        
        return final_result
    
    def _encode_input_audio(self, audio_path: str) -> torch.Tensor:
        """ç¼–ç è¾“å…¥éŸ³é¢‘"""
        print(f"   ğŸ”„ ç¼–ç è¾“å…¥éŸ³é¢‘...")
        
        audio, sr = torchaudio.load(audio_path)
        if sr != 48000:
            resampler = torchaudio.transforms.Resample(sr, 48000)
            audio = resampler(audio)
        
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        max_length = 48000 * 10
        if audio.shape[-1] > max_length:
            audio = audio[..., :max_length]
        
        with torch.no_grad():
            audio_np = audio.squeeze(0).numpy()
            
            inputs = self.pipeline.feature_extractor(
                audio_np,
                sampling_rate=48000,
                return_tensors="pt"
            )
            
            mel_features = inputs["input_features"].to(self.device)
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode()
            latent = latent * self.pipeline.vae.config.scaling_factor
            
            print(f"   âœ… ç¼–ç å®Œæˆ: {latent.shape}")
            return latent
    
    def _reconstruct_with_vae(self, latent: torch.Tensor, method_name: str) -> Dict:
        """ä½¿ç”¨VAEé‡å»ºéŸ³é¢‘"""
        print(f"   ğŸ”§ {method_name}é‡å»º...")
        
        start_time = time.time()
        
        with torch.no_grad():
            latent_for_decode = latent / self.pipeline.vae.config.scaling_factor
            vae_dtype = next(self.pipeline.vae.parameters()).dtype
            latent_for_decode = latent_for_decode.to(vae_dtype)
            
            mel_spectrogram = self.pipeline.vae.decode(latent_for_decode).sample
            audio_tensor = self.pipeline.mel_spectrogram_to_waveform(mel_spectrogram)
            audio = audio_tensor.squeeze().cpu().numpy()
        
        reconstruction_time = time.time() - start_time
        
        output_dir = Path("improved_ddcm_output")
        output_dir.mkdir(exist_ok=True)
        timestamp = int(time.time())
        
        output_path = output_dir / f"{method_name}_{timestamp}.wav"
        sf.write(str(output_path), audio, 16000)
        
        return {
            "method": method_name,
            "audio": audio,
            "output_path": str(output_path),
            "reconstruction_time": reconstruction_time
        }
    
    def _reconstruct_with_improved_ddcm_diffusion(self, 
                                                quantized_latent: torch.Tensor, 
                                                original_latent: torch.Tensor,
                                                prompt: str, 
                                                method_name: str) -> Dict:
        """æ”¹è¿›çš„DDCM guided diffusioné‡å»º"""
        print(f"   ğŸ¯ {method_name}é‡å»º...")
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨æ··åˆlatentä½œä¸ºå¼•å¯¼
            guidance_strength = 0.8  # å¼•å¯¼å¼ºåº¦
            guided_latent = guidance_strength * quantized_latent + (1 - guidance_strength) * original_latent
            
            with torch.no_grad():
                result = self.pipeline(
                    prompt=prompt,
                    num_inference_steps=20,  # å¢åŠ æ­¥æ•°
                    guidance_scale=7.5,
                    audio_length_in_s=10.0,
                    latents=guided_latent
                )
                audio = result.audios[0]
                
        except Exception as e:
            print(f"   âš ï¸ æ”¹è¿›DDCM diffusionå¤±è´¥ï¼Œå›é€€åˆ°VAE: {e}")
            return self._reconstruct_with_vae(quantized_latent, f"{method_name}_Fallback")
        
        reconstruction_time = time.time() - start_time
        
        output_dir = Path("improved_ddcm_output")
        output_dir.mkdir(exist_ok=True)
        timestamp = int(time.time())
        
        output_path = output_dir / f"{method_name}_{timestamp}.wav"
        sf.write(str(output_path), audio, 16000)
        
        return {
            "method": method_name,
            "audio": audio,
            "output_path": str(output_path),
            "reconstruction_time": reconstruction_time
        }
    
    def _analyze_reconstruction_quality(self, original_audio: np.ndarray, results: Dict) -> Dict:
        """åˆ†æé‡å»ºè´¨é‡"""
        print(f"   ğŸ“Š è´¨é‡åˆ†æ...")
        
        analysis = {}
        
        for method, result in results.items():
            recon_audio = result["audio"]
            
            min_len = min(len(original_audio), len(recon_audio))
            orig = original_audio[:min_len]
            recon = recon_audio[:min_len]
            
            # åŸºæœ¬æŒ‡æ ‡
            mse = np.mean((orig - recon) ** 2)
            snr = 10 * np.log10(np.mean(orig ** 2) / (mse + 1e-10))
            correlation = np.corrcoef(orig, recon)[0, 1] if min_len > 1 else 0
            mae = np.mean(np.abs(orig - recon))
            
            # é¢‘è°±ç›¸å…³æ€§
            if min_len > 1024:
                orig_fft = np.abs(np.fft.fft(orig[:1024]))
                recon_fft = np.abs(np.fft.fft(recon[:1024]))
                spectral_correlation = np.corrcoef(orig_fft, recon_fft)[0, 1]
            else:
                spectral_correlation = 0
            
            # MFCCç›¸å…³æ€§
            if min_len > 2048:
                orig_mfcc = librosa.feature.mfcc(y=orig, sr=16000, n_mfcc=13)
                recon_mfcc = librosa.feature.mfcc(y=recon, sr=16000, n_mfcc=13)
                mfcc_correlation = np.corrcoef(orig_mfcc.flatten(), recon_mfcc.flatten())[0, 1]
            else:
                mfcc_correlation = 0
            
            # ç»¼åˆç›¸ä¼¼æ€§åˆ†æ•°
            similarity_score = (
                correlation * 0.3 +
                spectral_correlation * 0.3 +
                mfcc_correlation * 0.2 +
                (snr + 20) / 40 * 0.2  # å½’ä¸€åŒ–SNR
            )
            
            analysis[method] = {
                "snr": snr,
                "correlation": correlation,
                "spectral_correlation": spectral_correlation,
                "mfcc_correlation": mfcc_correlation,
                "mse": mse,
                "mae": mae,
                "similarity_score": similarity_score,
                "reconstruction_time": result["reconstruction_time"]
            }
        
        return analysis
    
    def _display_improved_results(self, result: Dict):
        """æ˜¾ç¤ºæ”¹è¿›çš„ç»“æœ"""
        print(f"\n{'='*70}")
        print(f"ğŸ¯ æ”¹è¿›DDCMåŸºäºè¾“å…¥éŸ³é¢‘çš„å¤„ç†ç»“æœ")
        print(f"{'='*70}")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {result['input_file']}")
        print(f"ğŸ“ é‡å»ºæç¤º: {result['prompt']}")
        print(f"ğŸ“š ç æœ¬ä½¿ç”¨: {result['codebook_usage']['used_codes']}/{result['codebook_usage']['total_codes']}")
        print(f"ğŸ”„ è½¯é‡åŒ–è·ç¦»: {result['soft_quantization_distance']:.4f}")
        print(f"ğŸ”„ ç¡¬é‡åŒ–è·ç¦»: {result['hard_quantization_distance']:.4f}")
        
        print(f"\nğŸ“Š é‡å»ºæ–¹æ³•å¯¹æ¯”:")
        print(f"{'æ–¹æ³•':<25} {'SNR(dB)':<10} {'æ³¢å½¢ç›¸å…³':<10} {'é¢‘è°±ç›¸å…³':<10} {'MFCCç›¸å…³':<10} {'ç»¼åˆåˆ†æ•°':<10}")
        print("-" * 85)
        
        for method, analysis in result['quality_analysis'].items():
            print(f"{method:<25} {analysis['snr']:<10.2f} {analysis['correlation']:<10.4f} "
                  f"{analysis['spectral_correlation']:<10.4f} {analysis['mfcc_correlation']:<10.4f} "
                  f"{analysis['similarity_score']:<10.4f}")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   åŸå§‹: {result['original_audio_path']}")
        for method, recon in result['reconstructions'].items():
            print(f"   {method}: {recon['output_path']}")
        
        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
        best_method = max(result['quality_analysis'].items(), 
                         key=lambda x: x[1]['similarity_score'])
        
        print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]}")
        print(f"   ç»¼åˆç›¸ä¼¼æ€§åˆ†æ•°: {best_method[1]['similarity_score']:.4f}")
        
        # åˆ†ææ”¹è¿›æ•ˆæœ
        methods = result['quality_analysis']
        if 'improved_ddcm_diffusion' in methods:
            score = methods['improved_ddcm_diffusion']['similarity_score']
            print(f"\nğŸ¯ æ”¹è¿›DDCMåˆ†æ:")
            if score > 0.6:
                print(f"   ğŸ‰ æ”¹è¿›DDCMè¡¨ç°ä¼˜ç§€ï¼ç›¸ä¼¼æ€§åˆ†æ•°: {score:.4f}")
            elif score > 0.4:
                print(f"   âœ… æ”¹è¿›DDCMè¡¨ç°è‰¯å¥½ï¼ç›¸ä¼¼æ€§åˆ†æ•°: {score:.4f}")
            elif score > 0.2:
                print(f"   âš ï¸ æ”¹è¿›DDCMæœ‰ä¸€å®šç›¸å…³æ€§ï¼Œç›¸ä¼¼æ€§åˆ†æ•°: {score:.4f}")
            else:
                print(f"   âŒ æ”¹è¿›DDCMç›¸å…³æ€§ä»ç„¶è¾ƒä½ï¼Œç›¸ä¼¼æ€§åˆ†æ•°: {score:.4f}")

def demo_improved_ddcm():
    """æ¼”ç¤ºæ”¹è¿›çš„DDCM"""
    print("ğŸ¯ æ”¹è¿›ç‰ˆåŸºäºè¾“å…¥éŸ³é¢‘çš„DDCMæ¼”ç¤º")
    print("=" * 50)
    
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        return
    
    # ä½¿ç”¨æ›´å¤§çš„ç æœ¬
    ddcm_pipeline = ImprovedAudioLDM2_DDCM(
        model_name="cvssp/audioldm2-music",
        codebook_size=512  # å¢å¤§ç æœ¬
    )
    
    result = ddcm_pipeline.process_input_audio_improved(
        audio_path=input_file,
        prompt="high quality instrumental music with rich harmonics and detailed textures"
    )
    
    print(f"\nâœ… æ”¹è¿›ç‰ˆDDCMæ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ’¡ é€šè¿‡è½¯é‡åŒ–ã€æ··åˆé‡å»ºç­‰ç­–ç•¥æé«˜äº†ç›¸å…³æ€§")

if __name__ == "__main__":
    demo_improved_ddcm()
