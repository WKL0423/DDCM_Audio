"""
è§£å†³Vocoderç»´åº¦é—®é¢˜çš„æœ€ç»ˆç‰ˆæœ¬

æ ¹æ®é”™è¯¯åˆ†æï¼š
- æœŸæœ›è¾“å…¥: [1, 500, 64] (batch, time, channels)
- å®é™…è¾“å…¥: [1, 64, 500] (batch, channels, time)
- è§£å†³æ–¹æ¡ˆ: è½¬ç½®æœ€åä¸¤ä¸ªç»´åº¦

è¿™ä¸ªç‰ˆæœ¬ä¸“é—¨è§£å†³vocoderçš„ç»´åº¦åŒ¹é…é—®é¢˜ã€‚
"""

import torch
import librosa
import numpy as np
import os
import time
from pathlib import Path
import torchaudio

from diffusers import AudioLDM2Pipeline


def calculate_metrics(original, reconstructed):
    """è®¡ç®—é‡å»ºè´¨é‡æŒ‡æ ‡"""
    if len(original) != len(reconstructed):
        min_len = min(len(original), len(reconstructed))
        original = original[:min_len]
        reconstructed = reconstructed[:min_len]
    
    # SNRè®¡ç®—
    noise = reconstructed - original
    signal_power = np.mean(original ** 2)
    noise_power = np.mean(noise ** 2)
    snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
    
    # ç›¸å…³ç³»æ•°
    correlation = np.corrcoef(original, reconstructed)[0, 1] if len(original) > 1 else 0
    
    return snr, correlation


def mel_to_audio_vocoder_corrected(mel_spec, vocoder, device):
    """
    ä¿®æ­£ç»´åº¦çš„vocoderéŸ³é¢‘é‡å»º
    """
    print(f"ğŸ¤ ä½¿ç”¨ä¿®æ­£ç»´åº¦çš„vocoder...")
    
    # ç¡®ä¿æ˜¯numpyæ•°ç»„å’Œfloat32
    if isinstance(mel_spec, torch.Tensor):
        mel_spec = mel_spec.cpu().numpy()
    mel_spec = mel_spec.astype(np.float32)
    
    print(f"   åŸå§‹melå½¢çŠ¶: {mel_spec.shape}")
    
    # è½¬æ¢ä¸ºtensor
    mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).float().to(device)
    print(f"   æ·»åŠ batchç»´åº¦å: {mel_tensor.shape}")
    
    # å…³é”®ä¿®æ­£ï¼šè½¬ç½®æœ€åä¸¤ä¸ªç»´åº¦
    # ä» [batch, channels, time] è½¬æ¢ä¸º [batch, time, channels]
    mel_tensor_transposed = mel_tensor.transpose(-2, -1)
    print(f"   è½¬ç½®åç»´åº¦: {mel_tensor_transposed.shape}")
    
    try:
        with torch.no_grad():
            # å°è¯•vocoder
            audio_tensor = vocoder(mel_tensor_transposed)
            
            if isinstance(audio_tensor, tuple):
                audio_tensor = audio_tensor[0]
            
            audio = audio_tensor.squeeze().cpu().numpy()
            print(f"   âœ… VocoderæˆåŠŸ! éŸ³é¢‘å½¢çŠ¶: {audio.shape}")
            return audio, "success"
            
    except Exception as e:
        print(f"   âŒ è½¬ç½®åä»å¤±è´¥: {e}")
        
        # å°è¯•å…¶ä»–å¯èƒ½çš„ç»´åº¦ç»„åˆ
        try:
            print(f"   å°è¯•å…¶ä»–ç»´åº¦å®‰æ’...")
            # å°è¯• [batch, time, channels] ä½†ç¡®ä¿channels=64
            if mel_tensor.shape[-1] == 64:  # å¦‚æœæœ€åä¸€ç»´æ˜¯64
                mel_for_vocoder = mel_tensor.transpose(-2, -1)
            elif mel_tensor.shape[-2] == 64:  # å¦‚æœå€’æ•°ç¬¬äºŒç»´æ˜¯64
                mel_for_vocoder = mel_tensor
            else:
                print(f"   æ‰¾ä¸åˆ°64é€šé“ç»´åº¦")
                return None, f"dimension_error: {mel_tensor.shape}"
                
            print(f"   æœ€ç»ˆå°è¯•ç»´åº¦: {mel_for_vocoder.shape}")
            
            with torch.no_grad():
                audio_tensor = vocoder(mel_for_vocoder)
                if isinstance(audio_tensor, tuple):
                    audio_tensor = audio_tensor[0]
                audio = audio_tensor.squeeze().cpu().numpy()
                print(f"   âœ… å¤‡é€‰æ–¹æ¡ˆæˆåŠŸ! éŸ³é¢‘å½¢çŠ¶: {audio.shape}")
                return audio, "alternative_success"
                
        except Exception as e2:
            print(f"   âŒ æ‰€æœ‰vocoderå°è¯•éƒ½å¤±è´¥: {e2}")
            return None, f"all_failed: {e} | {e2}"


def mel_to_audio_griffinlim_safe(mel_spec, sample_rate=16000):
    """
    å®‰å…¨çš„Griffin-Limé‡å»º
    """
    try:
        # ç¡®ä¿æ˜¯float32
        mel_spec = mel_spec.astype(np.float32)
        
        # åå½’ä¸€åŒ–ï¼šä»[-1,1] -> [min_db, 0]
        mel_spec_denorm = (mel_spec + 1.0) / 2.0
        mel_spec_denorm = mel_spec_denorm * 80.0 - 80.0
        
        # è½¬æ¢åˆ°åŠŸç‡åŸŸ
        mel_spec_power = librosa.db_to_power(mel_spec_denorm, ref=1.0)
        
        # Griffin-Limé‡å»º
        audio = librosa.feature.inverse.mel_to_audio(
            mel_spec_power,
            sr=sample_rate,
            n_fft=1024,
            hop_length=160,
            window='hann',
            center=True,
            pad_mode='reflect',
            n_iter=32
        )
        
        return audio, "success"
        
    except Exception as e:
        return None, f"griffinlim_failed: {e}"


def final_vae_test(audio_path, model_id="cvssp/audioldm2-music", max_length=5):
    """
    æœ€ç»ˆçš„VAEæµ‹è¯•ï¼Œä¸“æ³¨äºè§£å†³vocoderç»´åº¦é—®é¢˜
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ¯ è®¾å¤‡: {device}")
    
    if not os.path.exists(audio_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {audio_path}")
        return
    
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_id}")
    
    # ä½¿ç”¨float32é¿å…ç±»å‹é—®é¢˜
    pipeline = AudioLDM2Pipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float32
    ).to(device)
    
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    print(f"ğŸ”§ Vocoderä¿¡æ¯:")
    print(f"   ç±»å‹: {type(vocoder).__name__}")
    if hasattr(vocoder, 'config'):
        print(f"   è¾“å…¥ç»´åº¦: {vocoder.config.model_in_dim}")
        print(f"   é‡‡æ ·ç‡: {vocoder.config.sampling_rate}")
    
    # åŠ è½½éŸ³é¢‘
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘: {audio_path}")
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    
    if len(audio) > max_length * sample_rate:
        audio = audio[:int(max_length * sample_rate)]
        print(f"âœ‚ï¸ è£å‰ªåˆ° {max_length} ç§’")
    
    print(f"ğŸ“Š éŸ³é¢‘ä¿¡æ¯: {len(audio)/sample_rate:.2f}ç§’")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "vae_final_test"
    os.makedirs(output_dir, exist_ok=True)
    
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    audio_save = audio / (np.max(np.abs(audio)) + 1e-8)
    torchaudio.save(original_path, torch.from_numpy(audio_save).unsqueeze(0), sample_rate)
    print(f"ğŸ’¾ åŸå§‹éŸ³é¢‘: {original_path}")
    
    print(f"\\nğŸ”¬ å¼€å§‹å®Œæ•´VAEæµç¨‹æµ‹è¯•...")
    
    # 1. éŸ³é¢‘ -> Mel
    print(f"\\n1ï¸âƒ£ éŸ³é¢‘è½¬Mel-spectrogram")
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sample_rate,
        n_mels=64,
        n_fft=1024,
        hop_length=160,
        power=2.0
    )
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_spec_norm = 2.0 * (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min()) - 1.0
    mel_spec_norm = mel_spec_norm.astype(np.float32)
    
    print(f"   âœ… Melå½¢çŠ¶: {mel_spec_norm.shape}")
    
    # 2. VAEç¼–ç /è§£ç 
    print(f"\\n2ï¸âƒ£ VAEç¼–ç è§£ç ")
    mel_tensor = torch.from_numpy(mel_spec_norm).unsqueeze(0).unsqueeze(0).float().to(device)
    print(f"   VAEè¾“å…¥: {mel_tensor.shape}")
    
    with torch.no_grad():
        latent = vae.encode(mel_tensor).latent_dist.sample()
        decoded = vae.decode(latent).sample
        
    decoded_mel = decoded.squeeze().cpu().float().numpy()
    print(f"   âœ… VAEè¾“å‡º: {decoded_mel.shape}")
    
    # 3. å¤šç§é‡å»ºæ–¹æ³•æµ‹è¯•
    print(f"\\n3ï¸âƒ£ éŸ³é¢‘é‡å»ºå¯¹æ¯”æµ‹è¯•")
    
    results = []
    
    # æ–¹æ³•A: ä¿®æ­£ç»´åº¦çš„Vocoder
    print(f"\\nğŸ¤ æ–¹æ³•A: ä¿®æ­£ç»´åº¦Vocoder")
    start_time = time.time()
    vocoder_audio, vocoder_status = mel_to_audio_vocoder_corrected(decoded_mel, vocoder, device)
    vocoder_time = time.time() - start_time
    
    if vocoder_audio is not None:
        snr_v, corr_v = calculate_metrics(audio, vocoder_audio)
        
        vocoder_path = os.path.join(output_dir, f"{input_name}_vocoder_corrected_{timestamp}.wav")
        audio_norm = vocoder_audio / (np.max(np.abs(vocoder_audio)) + 1e-8)
        torchaudio.save(vocoder_path, torch.from_numpy(audio_norm).unsqueeze(0), sample_rate)
        
        results.append({
            'method': 'Vocoderä¿®æ­£',
            'path': vocoder_path,
            'snr': snr_v,
            'correlation': corr_v,
            'time': vocoder_time,
            'status': vocoder_status
        })
        
        print(f"   âœ… æˆåŠŸ! SNR: {snr_v:.2f}dB, ç›¸å…³: {corr_v:.4f}")
    else:
        print(f"   âŒ å¤±è´¥: {vocoder_status}")
    
    # æ–¹æ³•B: Griffin-Lim
    print(f"\\nğŸµ æ–¹æ³•B: Griffin-Lim")
    start_time = time.time()
    gl_audio, gl_status = mel_to_audio_griffinlim_safe(decoded_mel, sample_rate)
    gl_time = time.time() - start_time
    
    if gl_audio is not None:
        snr_gl, corr_gl = calculate_metrics(audio, gl_audio)
        
        gl_path = os.path.join(output_dir, f"{input_name}_griffinlim_{timestamp}.wav")
        audio_norm = gl_audio / (np.max(np.abs(gl_audio)) + 1e-8)
        torchaudio.save(gl_path, torch.from_numpy(audio_norm).unsqueeze(0), sample_rate)
        
        results.append({
            'method': 'Griffin-Lim',
            'path': gl_path,
            'snr': snr_gl,
            'correlation': corr_gl,
            'time': gl_time,
            'status': gl_status
        })
        
        print(f"   âœ… æˆåŠŸ! SNR: {snr_gl:.2f}dB, ç›¸å…³: {corr_gl:.4f}")
    else:
        print(f"   âŒ å¤±è´¥: {gl_status}")
    
    # æ€»ç»“ç»“æœ
    print(f"\\n{'='*60}")
    print(f"ğŸ¯ æœ€ç»ˆVAEé‡å»ºæµ‹è¯•ç»“æœ")
    print(f"{'='*60}")
    
    if results:
        # æŒ‰SNRæ’åº
        results.sort(key=lambda x: x['snr'], reverse=True)
        
        print(f"\\nğŸ† é‡å»ºè´¨é‡æ’å:")
        for i, result in enumerate(results, 1):
            print(f"   #{i} {result['method']}:")
            print(f"       ğŸ“ˆ SNR: {result['snr']:.2f} dB")
            print(f"       ğŸ”— ç›¸å…³ç³»æ•°: {result['correlation']:.4f}")
            print(f"       â±ï¸ å¤„ç†æ—¶é—´: {result['time']:.2f}ç§’")
            print(f"       ğŸ“„ æ–‡ä»¶: {result['path']}")
            print(f"       âœ… çŠ¶æ€: {result['status']}")
            print()
        
        best_result = results[0]
        print(f"ğŸš€ æœ€ä½³ç»“æœ:")
        print(f"   ğŸ† æœ€ä¼˜æ–¹æ³•: {best_result['method']}")
        print(f"   ğŸ“ˆ æœ€é«˜SNR: {best_result['snr']:.2f} dB")
        print(f"   ğŸ”— ç›¸å…³ç³»æ•°: {best_result['correlation']:.4f}")
        
        if len(results) > 1:
            improvement = best_result['snr'] - results[-1]['snr']
            print(f"   ğŸ“Š æ–¹æ³•é—´å·®å¼‚: {improvement:.2f} dB")
        
        # æ£€æŸ¥vocoderæ˜¯å¦æˆåŠŸ
        vocoder_success = any(r['method'] == 'Vocoderä¿®æ­£' for r in results)
        if vocoder_success:
            print(f"\\nğŸ‰ Vocoderç»´åº¦é—®é¢˜å·²è§£å†³ï¼")
        else:
            print(f"\\nâš ï¸ Vocoderä»æœ‰é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨Griffin-Lim")
            
    else:
        print(f"\\nâŒ æ‰€æœ‰é‡å»ºæ–¹æ³•éƒ½å¤±è´¥äº†")
    
    print(f"\\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}/")
    print(f"ğŸ§ å»ºè®®æ’­æ”¾éŸ³é¢‘æ–‡ä»¶è¿›è¡Œä¸»è§‚è´¨é‡è¯„ä¼°")
    print(f"\\nâœ… æœ€ç»ˆæµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        audio_path = sys.argv[1]
    else:
        audio_path = "AudioLDM2_Music_output.wav"
    
    print(f"ğŸš€ å¼€å§‹æœ€ç»ˆVAEæµ‹è¯•: {audio_path}")
    print(f"ğŸ¯ ç›®æ ‡: è§£å†³vocoderç»´åº¦é—®é¢˜ï¼Œå®ç°é«˜è´¨é‡éŸ³é¢‘é‡å»º")
    final_vae_test(audio_path)
