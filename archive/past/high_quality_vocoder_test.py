"""
é«˜è´¨é‡Vocoderæ”¹è¿›æ–¹æ¡ˆå®æ–½
ä¸“é—¨è§£å†³Griffin-Limçš„92.1%ä¿¡æ¯æŸå¤±é—®é¢˜

ä¸»è¦æ”¹è¿›ï¼š
1. é›†æˆå¤šç§é«˜è´¨é‡vocoder
2. ä¼˜åŒ–mel-spectrogramå‚æ•°é…ç½®
3. æ·»åŠ åå¤„ç†å¢å¼º
4. æä¾›è¯¦ç»†çš„æ€§èƒ½å¯¹æ¯”
"""

import torch
import librosa
import numpy as np
import os
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F
from scipy.signal import butter, filtfilt

from diffusers import AudioLDM2Pipeline


def load_high_quality_vocoders():
    """
    åŠ è½½å¤šç§é«˜è´¨é‡vocoderè¿›è¡Œå¯¹æ¯”
    """
    vocoders = {}
    
    # 1. AudioLDM2å†…ç½®vocoderï¼ˆå·²ä¿®æ­£ç»´åº¦é—®é¢˜ï¼‰
    try:
        pipeline = AudioLDM2Pipeline.from_pretrained("cvssp/audioldm2-music", torch_dtype=torch.float32)
        vocoders['audioldm2'] = pipeline.vocoder
        print("âœ… AudioLDM2 SpeechT5HifiGanåŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ AudioLDM2 vocoderåŠ è½½å¤±è´¥: {e}")
    
    # 2. å°è¯•åŠ è½½å…¶ä»–é«˜è´¨é‡vocoder
    try:
        from transformers import SpeechT5HifiGan
        hifigan = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
        vocoders['hifigan'] = hifigan
        print("âœ… Microsoft SpeechT5 HifiGanåŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ SpeechT5 HifiGanåŠ è½½å¤±è´¥: {e}")
    
    # 3. å¯ä»¥æ·»åŠ æ›´å¤švocoder...
    # å¦‚WaveGlow, Parallel WaveGANç­‰
    
    return vocoders


def optimize_mel_parameters(audio, sr=16000):
    """
    é’ˆå¯¹é«˜è´¨é‡é‡å»ºä¼˜åŒ–mel-spectrogramå‚æ•°
    """
    # å¤šç§melé…ç½®ç”¨äºæµ‹è¯•
    configs = [
        # æ ‡å‡†é…ç½®
        {'n_mels': 64, 'n_fft': 1024, 'hop_length': 160, 'name': 'standard'},
        # é«˜åˆ†è¾¨ç‡é…ç½®
        {'n_mels': 80, 'n_fft': 2048, 'hop_length': 128, 'name': 'high_res'},
        # å¹³è¡¡é…ç½®
        {'n_mels': 64, 'n_fft': 1536, 'hop_length': 144, 'name': 'balanced'},
    ]
    
    mel_variants = {}
    
    for config in configs:
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sr,
            n_mels=config['n_mels'],
            n_fft=config['n_fft'],
            hop_length=config['hop_length'],
            window='hann',
            center=True,
            pad_mode='reflect',
            power=2.0,
            fmin=0,
            fmax=sr//2
        )
        
        mel_variants[config['name']] = {
            'spec': mel_spec,
            'config': config
        }
    
    return mel_variants


def advanced_post_processing(audio, sr=16000):
    """
    é«˜çº§éŸ³é¢‘åå¤„ç†
    """
    processed_variants = {}
    
    # 1. åŸºç¡€åå¤„ç†
    try:
        # å»å™ªæ»¤æ³¢
        nyquist = sr // 2
        cutoff = min(7500, nyquist * 0.85)
        b, a = butter(4, cutoff / nyquist, btype='low')
        audio_filtered = filtfilt(b, a, audio)
        
        # è½»å¾®åŠ¨æ€èŒƒå›´å‹ç¼©
        audio_compressed = np.sign(audio_filtered) * np.power(np.abs(audio_filtered), 0.85)
        
        processed_variants['basic'] = audio_compressed
        
    except Exception as e:
        print(f"åŸºç¡€åå¤„ç†å¤±è´¥: {e}")
        processed_variants['basic'] = audio
    
    # 2. é¢‘åŸŸå¢å¼º
    try:
        # STFTåŸŸå¤„ç†
        stft = librosa.stft(audio, n_fft=1024, hop_length=160)
        magnitude = np.abs(stft)
        phase = np.angle(stft)
        
        # è°±å‡æ³•å»å™ª
        noise_floor = np.percentile(magnitude, 10, axis=1, keepdims=True)
        magnitude_cleaned = np.maximum(magnitude - 0.1 * noise_floor, 0.1 * magnitude)
        
        # é‡å»ºéŸ³é¢‘
        stft_cleaned = magnitude_cleaned * np.exp(1j * phase)
        audio_enhanced = librosa.istft(stft_cleaned, hop_length=160)
        
        processed_variants['spectral'] = audio_enhanced
        
    except Exception as e:
        print(f"é¢‘åŸŸå¢å¼ºå¤±è´¥: {e}")
        processed_variants['spectral'] = audio
    
    # 3. ç›¸ä½ä¼˜åŒ–
    try:
        # æœ€å°ç›¸ä½é‡å»º
        stft = librosa.stft(audio, n_fft=1024, hop_length=160)
        magnitude = np.abs(stft)
        
        # ä½¿ç”¨æœ€å°ç›¸ä½
        log_magnitude = np.log(magnitude + 1e-8)
        cepstrum = np.fft.irfft(log_magnitude, axis=0)
        
        # æ„é€ æœ€å°ç›¸ä½
        cepstrum_min = cepstrum.copy()
        cepstrum_min[1:cepstrum.shape[0]//2] *= 2
        cepstrum_min[cepstrum.shape[0]//2+1:] = 0
        
        magnitude_min_phase = np.exp(np.fft.rfft(cepstrum_min, axis=0))
        audio_min_phase = librosa.istft(magnitude_min_phase, hop_length=160)
        
        processed_variants['min_phase'] = audio_min_phase
        
    except Exception as e:
        print(f"ç›¸ä½ä¼˜åŒ–å¤±è´¥: {e}")
        processed_variants['min_phase'] = audio
    
    return processed_variants


def comprehensive_vocoder_test(audio_path):
    """
    ç»¼åˆvocoderæµ‹è¯•å’Œå¯¹æ¯”
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ å¯åŠ¨ç»¼åˆvocoderè´¨é‡æå‡æµ‹è¯•")
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000, mono=True)
    audio = audio[:5*16000]  # 5ç§’
    
    print(f"ğŸ“Š æµ‹è¯•éŸ³é¢‘: {len(audio)/16000:.1f}ç§’")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "high_quality_vocoder_test"
    os.makedirs(output_dir, exist_ok=True)
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘
    orig_path = os.path.join(output_dir, f"{input_name}_original.wav")
    torchaudio.save(orig_path, torch.from_numpy(audio / (np.max(np.abs(audio)) + 1e-8)).unsqueeze(0), 16000)
    
    # åŠ è½½VAE
    pipeline = AudioLDM2Pipeline.from_pretrained("cvssp/audioldm2-music", torch_dtype=torch.float32).to(device)
    vae = pipeline.vae
    
    # åŠ è½½vocoders
    print(f"\\nğŸ¤ åŠ è½½é«˜è´¨é‡vocoders...")
    vocoders = load_high_quality_vocoders()
    
    # ä¼˜åŒ–melå‚æ•°
    print(f"\\nğŸ“Š ä¼˜åŒ–mel-spectrogramå‚æ•°...")
    mel_variants = optimize_mel_parameters(audio)
    
    results = []
    
    # æµ‹è¯•æ‰€æœ‰vocoderå’Œmelé…ç½®ç»„åˆ
    for mel_name, mel_data in mel_variants.items():
        print(f"\\nğŸ”¬ æµ‹è¯•melé…ç½®: {mel_name}")
        
        mel_spec = mel_data['spec']
        config = mel_data['config']
        
        # æ ‡å‡†åŒ–mel-spectrogram
        mel_db = librosa.power_to_db(mel_spec, ref=np.max)
        mel_norm = 2.0 * (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min()) - 1.0
        
        # è°ƒæ•´åˆ°VAEæœŸæœ›çš„64 mel bins
        if config['n_mels'] != 64:
            mel_resized = F.interpolate(
                torch.from_numpy(mel_norm).unsqueeze(0).unsqueeze(0),
                size=(64, mel_norm.shape[1]),
                mode='bilinear', align_corners=False
            ).squeeze().numpy()
        else:
            mel_resized = mel_norm
        
        # VAEç¼–ç /è§£ç 
        mel_tensor = torch.from_numpy(mel_resized).unsqueeze(0).unsqueeze(0).float().to(device)
        
        with torch.no_grad():
            latent = vae.encode(mel_tensor).latent_dist.sample()
            reconstructed_mel_tensor = vae.decode(latent).sample
        
        reconstructed_mel = reconstructed_mel_tensor.squeeze().cpu().numpy()
        
        # æµ‹è¯•ä¸åŒçš„vocoder
        for vocoder_name, vocoder in vocoders.items():
            print(f"   ğŸµ ä½¿ç”¨vocoder: {vocoder_name}")
            
            try:
                if vocoder_name == 'audioldm2':
                    # ä½¿ç”¨ä¿®æ­£çš„AudioLDM2 vocoder
                    mel_for_vocoder = torch.from_numpy(reconstructed_mel).unsqueeze(0).float().to(device)
                    mel_transposed = mel_for_vocoder.transpose(-2, -1)
                    
                    with torch.no_grad():
                        audio_vocoder = vocoder(mel_transposed).squeeze().cpu().numpy()
                
                elif vocoder_name == 'hifigan':
                    # ä½¿ç”¨SpeechT5 HifiGan
                    # éœ€è¦è°ƒæ•´è¾“å…¥æ ¼å¼
                    mel_for_hifigan = torch.from_numpy(reconstructed_mel).unsqueeze(0).float()
                    
                    with torch.no_grad():
                        audio_vocoder = vocoder(mel_for_hifigan).squeeze().cpu().numpy()
                
                else:
                    print(f"   âš ï¸ æœªå®ç°çš„vocoder: {vocoder_name}")
                    continue
                
                # é•¿åº¦å¯¹é½
                min_len = min(len(audio), len(audio_vocoder))
                audio_aligned = audio[:min_len]
                vocoder_aligned = audio_vocoder[:min_len]
                
                # åå¤„ç†æµ‹è¯•
                post_processed = advanced_post_processing(vocoder_aligned)
                
                for post_name, post_audio in post_processed.items():
                    method_name = f"{mel_name}_{vocoder_name}_{post_name}"
                    
                    # é•¿åº¦å¯¹é½
                    if len(post_audio) > len(audio_aligned):
                        post_audio = post_audio[:len(audio_aligned)]
                    elif len(post_audio) < len(audio_aligned):
                        post_audio = np.pad(post_audio, (0, len(audio_aligned) - len(post_audio)))
                    
                    # è®¡ç®—æŒ‡æ ‡
                    snr = 10 * np.log10(np.mean(audio_aligned**2) / (np.mean((audio_aligned - post_audio)**2) + 1e-10))
                    corr = np.corrcoef(audio_aligned, post_audio)[0,1] if len(audio_aligned) > 1 else 0
                    rmse = np.sqrt(np.mean((audio_aligned - post_audio)**2))
                    
                    # ä¿å­˜éŸ³é¢‘
                    save_path = os.path.join(output_dir, f"{input_name}_{method_name}_{timestamp}.wav")
                    audio_norm = post_audio / (np.max(np.abs(post_audio)) + 1e-8)
                    torchaudio.save(save_path, torch.from_numpy(audio_norm).unsqueeze(0), 16000)
                    
                    results.append({
                        'method': method_name,
                        'mel_config': mel_name,
                        'vocoder': vocoder_name,
                        'post_process': post_name,
                        'snr': snr,
                        'correlation': corr,
                        'rmse': rmse,
                        'path': save_path
                    })
                    
                    print(f"       {post_name}: SNR={snr:.2f}dB, ç›¸å…³={corr:.4f}")
                
            except Exception as e:
                print(f"   âŒ {vocoder_name} å¤±è´¥: {e}")
                continue
    
    # æ·»åŠ åŸºçº¿å¯¹æ¯”ï¼šç›´æ¥Griffin-Lim
    print(f"\\nğŸµ åŸºçº¿å¯¹æ¯”: ç›´æ¥Griffin-Lim")
    for mel_name, mel_data in mel_variants.items():
        mel_spec = mel_data['spec']
        config = mel_data['config']
        
        try:
            # ç›´æ¥Griffin-Limé‡å»º
            audio_gl = librosa.feature.inverse.mel_to_audio(
                mel_spec,
                sr=16000,
                n_fft=config['n_fft'],
                hop_length=config['hop_length'],
                n_iter=64
            )
            
            min_len = min(len(audio), len(audio_gl))
            snr_gl = 10 * np.log10(np.mean(audio[:min_len]**2) / (np.mean((audio[:min_len] - audio_gl[:min_len])**2) + 1e-10))
            corr_gl = np.corrcoef(audio[:min_len], audio_gl[:min_len])[0,1]
            
            gl_path = os.path.join(output_dir, f"{input_name}_baseline_{mel_name}_gl.wav")
            torchaudio.save(gl_path, torch.from_numpy(audio_gl[:min_len] / (np.max(np.abs(audio_gl[:min_len])) + 1e-8)).unsqueeze(0), 16000)
            
            results.append({
                'method': f"baseline_{mel_name}_griffin_lim",
                'mel_config': mel_name,
                'vocoder': 'griffin_lim',
                'post_process': 'none',
                'snr': snr_gl,
                'correlation': corr_gl,
                'rmse': np.sqrt(np.mean((audio[:min_len] - audio_gl[:min_len])**2)),
                'path': gl_path
            })
            
            print(f"   {mel_name} Griffin-Lim: SNR={snr_gl:.2f}dB, ç›¸å…³={corr_gl:.4f}")
            
        except Exception as e:
            print(f"   âŒ {mel_name} Griffin-Limå¤±è´¥: {e}")
    
    # ç»“æœåˆ†æ
    print(f"\\n{'='*70}")
    print(f"ğŸ¯ é«˜è´¨é‡Vocoderæµ‹è¯•ç»“æœåˆ†æ")
    print(f"{'='*70}")
    
    if results:
        # æŒ‰SNRæ’åº
        results.sort(key=lambda x: x['snr'], reverse=True)
        
        print(f"\\nğŸ† è´¨é‡æ’å (å‰10å):")
        for i, result in enumerate(results[:10], 1):
            improvement = result['snr'] - 0.02  # ä¸ä¹‹å‰åŸºçº¿å¯¹æ¯”
            print(f"   #{i} {result['method'][:50]}...")
            print(f"       ğŸ“ˆ SNR: {result['snr']:.2f}dB (+{improvement:.2f})")
            print(f"       ğŸ”— ç›¸å…³æ€§: {result['correlation']:.4f}")
            print(f"       ğŸ“ æ–‡ä»¶: {result['path']}")
            print()
        
        # æœ€ä½³ç»“æœåˆ†æ
        best = results[0]
        best_improvement = best['snr'] - 0.02
        
        print(f"ğŸš€ æœ€ä½³æ”¹è¿›æ•ˆæœ:")
        print(f"   ğŸ† æœ€ä¼˜ç»„åˆ: {best['mel_config']} + {best['vocoder']} + {best['post_process']}")
        print(f"   ğŸ“ˆ SNRæå‡: {best_improvement:+.2f} dB")
        print(f"   ğŸ”— æœ€é«˜ç›¸å…³æ€§: {best['correlation']:.4f}")
        
        if best_improvement > 5:
            print(f"   âœ… æ˜¾è‘—æ”¹å–„ï¼Griffin-Limç“¶é¢ˆåŸºæœ¬è§£å†³")
        elif best_improvement > 2:
            print(f"   âš ï¸ æœ‰æ˜æ˜¾æ”¹å–„ï¼Œä½†ä»æœ‰ä¼˜åŒ–ç©ºé—´")
        else:
            print(f"   âŒ æ”¹å–„æœ‰é™ï¼Œéœ€è¦æ¢ç´¢å…¶ä»–æ–¹æ¡ˆ")
        
        # æ–¹æ³•æ•ˆæœåˆ†æ
        print(f"\\nğŸ“Š ä¸åŒæ–¹æ³•æ•ˆæœåˆ†æ:")
        
        # æŒ‰vocoderåˆ†ç»„
        vocoder_performance = {}
        for result in results:
            vocoder = result['vocoder']
            if vocoder not in vocoder_performance:
                vocoder_performance[vocoder] = []
            vocoder_performance[vocoder].append(result['snr'])
        
        for vocoder, snrs in vocoder_performance.items():
            avg_snr = np.mean(snrs)
            max_snr = np.max(snrs)
            print(f"   ğŸ¤ {vocoder}: å¹³å‡{avg_snr:.2f}dB, æœ€ä½³{max_snr:.2f}dB")
        
    else:
        print(f"\\nâŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
    
    print(f"\\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}/")
    print(f"ğŸ§ å¼ºçƒˆå»ºè®®æ’­æ”¾æœ€ä½³ç»“æœè¿›è¡Œä¸»è§‚è¯„ä¼°")
    print(f"\\nâœ… ç»¼åˆvocoderæµ‹è¯•å®Œæˆï¼")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    audio_path = sys.argv[1] if len(sys.argv) > 1 else "AudioLDM2_Music_output.wav"
    
    print(f"ğŸ¯ é«˜è´¨é‡Vocoderæ”¹è¿›æ–¹æ¡ˆæµ‹è¯•")
    print(f"=" * 60)
    print(f"ç›®æ ‡: è§£å†³Griffin-Limçš„92.1%ä¿¡æ¯æŸå¤±é—®é¢˜")
    print(f"æ–¹æ³•: é›†æˆé«˜è´¨é‡vocoder + ä¼˜åŒ–å‚æ•° + åå¤„ç†å¢å¼º")
    
    results = comprehensive_vocoder_test(audio_path)
    
    if results and len(results) > 0:
        best_snr = max(results, key=lambda x: x['snr'])['snr']
        baseline_snr = 0.02  # ä¹‹å‰çš„åŸºçº¿
        total_improvement = best_snr - baseline_snr
        
        print(f"\\nğŸ‰ æµ‹è¯•å®Œæˆæ€»ç»“:")
        print(f"   ğŸ“ˆ æœ€å¤§SNRæå‡: {total_improvement:+.2f} dB")
        print(f"   ğŸ¯ Griffin-Limç“¶é¢ˆè§£å†³ç¨‹åº¦: {min(100, max(0, total_improvement/10*100)):.1f}%")
        
        if total_improvement > 8:
            print(f"   ğŸ‰ é‡å¤§çªç ´ï¼è´¨é‡æ˜¾è‘—æå‡")
        elif total_improvement > 3:
            print(f"   âœ… æ˜æ˜¾æ”¹å–„ï¼Œæ¥è¿‘å®ç”¨æ°´å¹³")
        else:
            print(f"   âš ï¸ éƒ¨åˆ†æ”¹å–„ï¼Œéœ€è¦ç»§ç»­ä¼˜åŒ–")


if __name__ == "__main__":
    main()
