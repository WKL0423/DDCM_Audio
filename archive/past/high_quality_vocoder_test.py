"""
È´òË¥®ÈáèVocoderÊîπËøõÊñπÊ°àÂÆûÊñΩ
‰∏ìÈó®Ëß£ÂÜ≥Griffin-LimÁöÑ92.1%‰ø°ÊÅØÊçüÂ§±ÈóÆÈ¢ò

‰∏ªË¶ÅÊîπËøõÔºö
1. ÈõÜÊàêÂ§öÁßçÈ´òË¥®Èáèvocoder
2. ‰ºòÂåñmel-spectrogramÂèÇÊï∞ÈÖçÁΩÆ
3. Ê∑ªÂä†ÂêéÂ§ÑÁêÜÂ¢ûÂº∫
4. Êèê‰æõËØ¶ÁªÜÁöÑÊÄßËÉΩÂØπÊØî
"""

import torch
import librosa
import numpy as np
import os
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F
from scipy.signal import butter, filtfilt

from diffusers import AudioLDM2Pipeline


def load_high_quality_vocoders():
    """
    Âä†ËΩΩÂ§öÁßçÈ´òË¥®ÈáèvocoderËøõË°åÂØπÊØî
    """
    vocoders = {}
    
    # 1. AudioLDM2ÂÜÖÁΩÆvocoderÔºàÂ∑≤‰øÆÊ≠£Áª¥Â∫¶ÈóÆÈ¢òÔºâ
    try:
        pipeline = AudioLDM2Pipeline.from_pretrained("cvssp/audioldm2-music", torch_dtype=torch.float32)
        vocoders['audioldm2'] = pipeline.vocoder
        print("‚úÖ AudioLDM2 SpeechT5HifiGanÂä†ËΩΩÊàêÂäü")
    except Exception as e:
        print(f"‚ùå AudioLDM2 vocoderÂä†ËΩΩÂ§±Ë¥•: {e}")
    
    # 2. Â∞ùËØïÂä†ËΩΩÂÖ∂‰ªñÈ´òË¥®Èáèvocoder
    try:
        from transformers import SpeechT5HifiGan
        hifigan = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
        vocoders['hifigan'] = hifigan
        print("‚úÖ Microsoft SpeechT5 HifiGanÂä†ËΩΩÊàêÂäü")
    except Exception as e:
        print(f"‚ùå SpeechT5 HifiGanÂä†ËΩΩÂ§±Ë¥•: {e}")
    
    # 3. ÂèØ‰ª•Ê∑ªÂä†Êõ¥Â§övocoder...
    # Â¶ÇWaveGlow, Parallel WaveGANÁ≠â
    
    return vocoders


def optimize_mel_parameters(audio, sr=16000):
    """
    ÈíàÂØπÈ´òË¥®ÈáèÈáçÂª∫‰ºòÂåñmel-spectrogramÂèÇÊï∞
    """
    # Â§öÁßçmelÈÖçÁΩÆÁî®‰∫éÊµãËØï
    configs = [
        # Ê†áÂáÜÈÖçÁΩÆ
        {'n_mels': 64, 'n_fft': 1024, 'hop_length': 160, 'name': 'standard'},
        # È´òÂàÜËæ®ÁéáÈÖçÁΩÆ
        {'n_mels': 80, 'n_fft': 2048, 'hop_length': 128, 'name': 'high_res'},
        # Âπ≥Ë°°ÈÖçÁΩÆ
        {'n_mels': 64, 'n_fft': 1536, 'hop_length': 144, 'name': 'balanced'},
    ]
    
    mel_variants = {}
    
    for config in configs:
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=sr,
            n_mels=config['n_mels'],
            n_fft=config['n_fft'],
            hop_length=config['hop_length'],
            window='hann',
            center=True,
            pad_mode='reflect',
            power=2.0,
            fmin=0,
            fmax=sr//2
        )
        
        mel_variants[config['name']] = {
            'spec': mel_spec,
            'config': config
        }
    
    return mel_variants


def advanced_post_processing(audio, sr=16000):
    """
    È´òÁ∫ßÈü≥È¢ëÂêéÂ§ÑÁêÜ
    """
    processed_variants = {}
    
    # 1. Âü∫Á°ÄÂêéÂ§ÑÁêÜ
    try:
        # ÂéªÂô™Êª§Ê≥¢
        nyquist = sr // 2
        cutoff = min(7500, nyquist * 0.85)
        b, a = butter(4, cutoff / nyquist, btype='low')
        audio_filtered = filtfilt(b, a, audio)
        
        # ËΩªÂæÆÂä®ÊÄÅËåÉÂõ¥ÂéãÁº©
        audio_compressed = np.sign(audio_filtered) * np.power(np.abs(audio_filtered), 0.85)
        
        processed_variants['basic'] = audio_compressed
        
    except Exception as e:
        print(f"Âü∫Á°ÄÂêéÂ§ÑÁêÜÂ§±Ë¥•: {e}")
        processed_variants['basic'] = audio
    
    # 2. È¢ëÂüüÂ¢ûÂº∫
    try:
        # STFTÂüüÂ§ÑÁêÜ
        stft = librosa.stft(audio, n_fft=1024, hop_length=160)
        magnitude = np.abs(stft)
        phase = np.angle(stft)
        
        # Ë∞±ÂáèÊ≥ïÂéªÂô™
        noise_floor = np.percentile(magnitude, 10, axis=1, keepdims=True)
        magnitude_cleaned = np.maximum(magnitude - 0.1 * noise_floor, 0.1 * magnitude)
        
        # ÈáçÂª∫Èü≥È¢ë
        stft_cleaned = magnitude_cleaned * np.exp(1j * phase)
        audio_enhanced = librosa.istft(stft_cleaned, hop_length=160)
        
        processed_variants['spectral'] = audio_enhanced
        
    except Exception as e:
        print(f"È¢ëÂüüÂ¢ûÂº∫Â§±Ë¥•: {e}")
        processed_variants['spectral'] = audio
    
    # 3. Áõ∏‰Ωç‰ºòÂåñ
    try:
        # ÊúÄÂ∞èÁõ∏‰ΩçÈáçÂª∫
        stft = librosa.stft(audio, n_fft=1024, hop_length=160)
        magnitude = np.abs(stft)
        
        # ‰ΩøÁî®ÊúÄÂ∞èÁõ∏‰Ωç
        log_magnitude = np.log(magnitude + 1e-8)
        cepstrum = np.fft.irfft(log_magnitude, axis=0)
        
        # ÊûÑÈÄ†ÊúÄÂ∞èÁõ∏‰Ωç
        cepstrum_min = cepstrum.copy()
        cepstrum_min[1:cepstrum.shape[0]//2] *= 2
        cepstrum_min[cepstrum.shape[0]//2+1:] = 0
        
        magnitude_min_phase = np.exp(np.fft.rfft(cepstrum_min, axis=0))
        audio_min_phase = librosa.istft(magnitude_min_phase, hop_length=160)
        
        processed_variants['min_phase'] = audio_min_phase
        
    except Exception as e:
        print(f"Áõ∏‰Ωç‰ºòÂåñÂ§±Ë¥•: {e}")
        processed_variants['min_phase'] = audio
    
    return processed_variants


def comprehensive_vocoder_test(audio_path):
    """
    ÁªºÂêàvocoderÊµãËØïÂíåÂØπÊØî
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üöÄ ÂêØÂä®ÁªºÂêàvocoderË¥®ÈáèÊèêÂçáÊµãËØï")
    
    # Âä†ËΩΩÈü≥È¢ë
    audio, sr = librosa.load(audio_path, sr=16000, mono=True)
    audio = audio[:5*16000]  # 5Áßí
    
    print(f"üìä ÊµãËØïÈü≥È¢ë: {len(audio)/16000:.1f}Áßí")
    
    # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
    output_dir = "high_quality_vocoder_test"
    os.makedirs(output_dir, exist_ok=True)
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    # ‰øùÂ≠òÂéüÂßãÈü≥È¢ë
    orig_path = os.path.join(output_dir, f"{input_name}_original.wav")
    torchaudio.save(orig_path, torch.from_numpy(audio / (np.max(np.abs(audio)) + 1e-8)).unsqueeze(0), 16000)
    
    # Âä†ËΩΩVAE
    pipeline = AudioLDM2Pipeline.from_pretrained("cvssp/audioldm2-music", torch_dtype=torch.float32).to(device)
    vae = pipeline.vae
    
    # Âä†ËΩΩvocoders
    print(f"\\nüé§ Âä†ËΩΩÈ´òË¥®Èáèvocoders...")
    vocoders = load_high_quality_vocoders()
    
    # ‰ºòÂåñmelÂèÇÊï∞
    print(f"\\nüìä ‰ºòÂåñmel-spectrogramÂèÇÊï∞...")
    mel_variants = optimize_mel_parameters(audio)
    
    results = []
    
    # ÊµãËØïÊâÄÊúâvocoderÂíåmelÈÖçÁΩÆÁªÑÂêà
    for mel_name, mel_data in mel_variants.items():
        print(f"\\nüî¨ ÊµãËØïmelÈÖçÁΩÆ: {mel_name}")
        
        mel_spec = mel_data['spec']
        config = mel_data['config']
        
        # Ê†áÂáÜÂåñmel-spectrogram
        mel_db = librosa.power_to_db(mel_spec, ref=np.max)
        mel_norm = 2.0 * (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min()) - 1.0
        
        # Ë∞ÉÊï¥Âà∞VAEÊúüÊúõÁöÑ64 mel bins
        if config['n_mels'] != 64:
            mel_resized = F.interpolate(
                torch.from_numpy(mel_norm).unsqueeze(0).unsqueeze(0),
                size=(64, mel_norm.shape[1]),
                mode='bilinear', align_corners=False
            ).squeeze().numpy()
        else:
            mel_resized = mel_norm
        
        # VAEÁºñÁ†Å/Ëß£Á†Å
        mel_tensor = torch.from_numpy(mel_resized).unsqueeze(0).unsqueeze(0).float().to(device)
        
        with torch.no_grad():
            latent = vae.encode(mel_tensor).latent_dist.sample()
            reconstructed_mel_tensor = vae.decode(latent).sample
        
        reconstructed_mel = reconstructed_mel_tensor.squeeze().cpu().numpy()
        
        # ÊµãËØï‰∏çÂêåÁöÑvocoder
        for vocoder_name, vocoder in vocoders.items():
            print(f"   üéµ ‰ΩøÁî®vocoder: {vocoder_name}")
            
            try:
                if vocoder_name == 'audioldm2':
                    # ‰ΩøÁî®‰øÆÊ≠£ÁöÑAudioLDM2 vocoder
                    mel_for_vocoder = torch.from_numpy(reconstructed_mel).unsqueeze(0).float().to(device)
                    mel_transposed = mel_for_vocoder.transpose(-2, -1)
                    
                    with torch.no_grad():
                        audio_vocoder = vocoder(mel_transposed).squeeze().cpu().numpy()
                
                elif vocoder_name == 'hifigan':
                    # ‰ΩøÁî®SpeechT5 HifiGan
                    # ÈúÄË¶ÅË∞ÉÊï¥ËæìÂÖ•Ê†ºÂºè
                    mel_for_hifigan = torch.from_numpy(reconstructed_mel).unsqueeze(0).float()
                    
                    with torch.no_grad():
                        audio_vocoder = vocoder(mel_for_hifigan).squeeze().cpu().numpy()
                
                else:
                    print(f"   ‚ö†Ô∏è Êú™ÂÆûÁé∞ÁöÑvocoder: {vocoder_name}")
                    continue
                
                # ÈïøÂ∫¶ÂØπÈΩê
                min_len = min(len(audio), len(audio_vocoder))
                audio_aligned = audio[:min_len]
                vocoder_aligned = audio_vocoder[:min_len]
                
                # ÂêéÂ§ÑÁêÜÊµãËØï
                post_processed = advanced_post_processing(vocoder_aligned)
                
                for post_name, post_audio in post_processed.items():
                    method_name = f"{mel_name}_{vocoder_name}_{post_name}"
                    
                    # ÈïøÂ∫¶ÂØπÈΩê
                    if len(post_audio) > len(audio_aligned):
                        post_audio = post_audio[:len(audio_aligned)]
                    elif len(post_audio) < len(audio_aligned):
                        post_audio = np.pad(post_audio, (0, len(audio_aligned) - len(post_audio)))
                    
                    # ËÆ°ÁÆóÊåáÊ†á
                    snr = 10 * np.log10(np.mean(audio_aligned**2) / (np.mean((audio_aligned - post_audio)**2) + 1e-10))
                    corr = np.corrcoef(audio_aligned, post_audio)[0,1] if len(audio_aligned) > 1 else 0
                    rmse = np.sqrt(np.mean((audio_aligned - post_audio)**2))
                    
                    # ‰øùÂ≠òÈü≥È¢ë
                    save_path = os.path.join(output_dir, f"{input_name}_{method_name}_{timestamp}.wav")
                    audio_norm = post_audio / (np.max(np.abs(post_audio)) + 1e-8)
                    torchaudio.save(save_path, torch.from_numpy(audio_norm).unsqueeze(0), 16000)
                    
                    results.append({
                        'method': method_name,
                        'mel_config': mel_name,
                        'vocoder': vocoder_name,
                        'post_process': post_name,
                        'snr': snr,
                        'correlation': corr,
                        'rmse': rmse,
                        'path': save_path
                    })
                    
                    print(f"       {post_name}: SNR={snr:.2f}dB, Áõ∏ÂÖ≥={corr:.4f}")
                
            except Exception as e:
                print(f"   ‚ùå {vocoder_name} Â§±Ë¥•: {e}")
                continue
    
    # Ê∑ªÂä†Âü∫Á∫øÂØπÊØîÔºöÁõ¥Êé•Griffin-Lim
    print(f"\\nüéµ Âü∫Á∫øÂØπÊØî: Áõ¥Êé•Griffin-Lim")
    for mel_name, mel_data in mel_variants.items():
        mel_spec = mel_data['spec']
        config = mel_data['config']
        
        try:
            # Áõ¥Êé•Griffin-LimÈáçÂª∫
            audio_gl = librosa.feature.inverse.mel_to_audio(
                mel_spec,
                sr=16000,
                n_fft=config['n_fft'],
                hop_length=config['hop_length'],
                n_iter=64
            )
            
            min_len = min(len(audio), len(audio_gl))
            snr_gl = 10 * np.log10(np.mean(audio[:min_len]**2) / (np.mean((audio[:min_len] - audio_gl[:min_len])**2) + 1e-10))
            corr_gl = np.corrcoef(audio[:min_len], audio_gl[:min_len])[0,1]
            
            gl_path = os.path.join(output_dir, f"{input_name}_baseline_{mel_name}_gl.wav")
            torchaudio.save(gl_path, torch.from_numpy(audio_gl[:min_len] / (np.max(np.abs(audio_gl[:min_len])) + 1e-8)).unsqueeze(0), 16000)
            
            results.append({
                'method': f"baseline_{mel_name}_griffin_lim",
                'mel_config': mel_name,
                'vocoder': 'griffin_lim',
                'post_process': 'none',
                'snr': snr_gl,
                'correlation': corr_gl,
                'rmse': np.sqrt(np.mean((audio[:min_len] - audio_gl[:min_len])**2)),
                'path': gl_path
            })
            
            print(f"   {mel_name} Griffin-Lim: SNR={snr_gl:.2f}dB, Áõ∏ÂÖ≥={corr_gl:.4f}")
            
        except Exception as e:
            print(f"   ‚ùå {mel_name} Griffin-LimÂ§±Ë¥•: {e}")
    
    # ÁªìÊûúÂàÜÊûê
    print(f"\\n{'='*70}")
    print(f"üéØ È´òË¥®ÈáèVocoderÊµãËØïÁªìÊûúÂàÜÊûê")
    print(f"{'='*70}")
    
    if results:
        # ÊåâSNRÊéíÂ∫è
        results.sort(key=lambda x: x['snr'], reverse=True)
        
        print(f"\\nüèÜ Ë¥®ÈáèÊéíÂêç (Ââç10Âêç):")
        for i, result in enumerate(results[:10], 1):
            improvement = result['snr'] - 0.02  # ‰∏é‰πãÂâçÂü∫Á∫øÂØπÊØî
            print(f"   #{i} {result['method'][:50]}...")
            print(f"       üìà SNR: {result['snr']:.2f}dB (+{improvement:.2f})")
            print(f"       üîó Áõ∏ÂÖ≥ÊÄß: {result['correlation']:.4f}")
            print(f"       üìÅ Êñá‰ª∂: {result['path']}")
            print()
        
        # ÊúÄ‰Ω≥ÁªìÊûúÂàÜÊûê
        best = results[0]
        best_improvement = best['snr'] - 0.02
        
        print(f"üöÄ ÊúÄ‰Ω≥ÊîπËøõÊïàÊûú:")
        print(f"   üèÜ ÊúÄ‰ºòÁªÑÂêà: {best['mel_config']} + {best['vocoder']} + {best['post_process']}")
        print(f"   üìà SNRÊèêÂçá: {best_improvement:+.2f} dB")
        print(f"   üîó ÊúÄÈ´òÁõ∏ÂÖ≥ÊÄß: {best['correlation']:.4f}")
        
        if best_improvement > 5:
            print(f"   ‚úÖ ÊòæËëóÊîπÂñÑÔºÅGriffin-LimÁì∂È¢àÂü∫Êú¨Ëß£ÂÜ≥")
        elif best_improvement > 2:
            print(f"   ‚ö†Ô∏è ÊúâÊòéÊòæÊîπÂñÑÔºå‰ΩÜ‰ªçÊúâ‰ºòÂåñÁ©∫Èó¥")
        else:
            print(f"   ‚ùå ÊîπÂñÑÊúâÈôêÔºåÈúÄË¶ÅÊé¢Á¥¢ÂÖ∂‰ªñÊñπÊ°à")
        
        # ÊñπÊ≥ïÊïàÊûúÂàÜÊûê
        print(f"\\nüìä ‰∏çÂêåÊñπÊ≥ïÊïàÊûúÂàÜÊûê:")
        
        # ÊåâvocoderÂàÜÁªÑ
        vocoder_performance = {}
        for result in results:
            vocoder = result['vocoder']
            if vocoder not in vocoder_performance:
                vocoder_performance[vocoder] = []
            vocoder_performance[vocoder].append(result['snr'])
        
        for vocoder, snrs in vocoder_performance.items():
            avg_snr = np.mean(snrs)
            max_snr = np.max(snrs)
            print(f"   üé§ {vocoder}: Âπ≥Âùá{avg_snr:.2f}dB, ÊúÄ‰Ω≥{max_snr:.2f}dB")
        
    else:
        print(f"\\n‚ùå Ê≤°ÊúâÊàêÂäüÁöÑÊµãËØïÁªìÊûú")
    
    print(f"\\nüìÅ ÊâÄÊúâÁªìÊûú‰øùÂ≠òÂú®: {output_dir}/")
    print(f"üéß Âº∫ÁÉàÂª∫ËÆÆÊí≠ÊîæÊúÄ‰Ω≥ÁªìÊûúËøõË°å‰∏ªËßÇËØÑ‰º∞")
    print(f"\\n‚úÖ ÁªºÂêàvocoderÊµãËØïÂÆåÊàêÔºÅ")
    
    return results


def main():
    """‰∏ªÂáΩÊï∞"""
    import sys
    
    audio_path = sys.argv[1] if len(sys.argv) > 1 else "AudioLDM2_Music_output.wav"
    
    print(f"üéØ È´òË¥®ÈáèVocoderÊîπËøõÊñπÊ°àÊµãËØï")
    print(f"=" * 60)
    print(f"ÁõÆÊ†á: Ëß£ÂÜ≥Griffin-LimÁöÑ92.1%‰ø°ÊÅØÊçüÂ§±ÈóÆÈ¢ò")
    print(f"ÊñπÊ≥ï: ÈõÜÊàêÈ´òË¥®Èáèvocoder + ‰ºòÂåñÂèÇÊï∞ + ÂêéÂ§ÑÁêÜÂ¢ûÂº∫")
    
    results = comprehensive_vocoder_test(audio_path)
    
    if results and len(results) > 0:
        best_snr = max(results, key=lambda x: x['snr'])['snr']
        baseline_snr = 0.02  # ‰πãÂâçÁöÑÂü∫Á∫ø
        total_improvement = best_snr - baseline_snr
        
        print(f"\\nüéâ ÊµãËØïÂÆåÊàêÊÄªÁªì:")
        print(f"   üìà ÊúÄÂ§ßSNRÊèêÂçá: {total_improvement:+.2f} dB")
        print(f"   üéØ Griffin-LimÁì∂È¢àËß£ÂÜ≥Á®ãÂ∫¶: {min(100, max(0, total_improvement/10*100)):.1f}%")
        
        if total_improvement > 8:
            print(f"   üéâ ÈáçÂ§ßÁ™ÅÁ†¥ÔºÅË¥®ÈáèÊòæËëóÊèêÂçá")
        elif total_improvement > 3:
            print(f"   ‚úÖ ÊòéÊòæÊîπÂñÑÔºåÊé•ËøëÂÆûÁî®Ê∞¥Âπ≥")
        else:
            print(f"   ‚ö†Ô∏è ÈÉ®ÂàÜÊîπÂñÑÔºåÈúÄË¶ÅÁªßÁª≠‰ºòÂåñ")


if __name__ == "__main__":
    main()
