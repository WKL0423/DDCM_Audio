#!/usr/bin/env python3
"""
Step 2: ç®€åŒ–çš„VAE + Latentå¢å¼ºé‡å»º
ä½¿ç”¨ç®€å•çš„latentç©ºé—´å¢å¼ºæŠ€æœ¯ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„diffusionè¿‡ç¨‹
ç›®æ ‡ï¼šè§£å†³VAEé«˜é¢‘æŸå¤±é—®é¢˜ï¼Œæå‡éŸ³é¢‘é‡å»ºè´¨é‡

æµç¨‹ï¼š
1. VAEç¼–ç éŸ³é¢‘åˆ°latentç©ºé—´
2. ä½¿ç”¨ç®€å•çš„latentå¢å¼ºæŠ€æœ¯ï¼ˆé«˜é¢‘boosting, å™ªå£°æ³¨å…¥ç­‰ï¼‰
3. VAEè§£ç å¢å¼ºåçš„latentåˆ°éŸ³é¢‘
4. å¯¹æ¯”åˆ†æVAE-only vs VAE+å¢å¼ºçš„æ•ˆæœ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import librosa
from pathlib import Path
import time
from diffusers import AudioLDM2Pipeline
from typing import Dict, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

class SimpleLatentEnhancer:
    """
    ç®€å•çš„Latentå¢å¼ºå™¨
    åœ¨VAEçš„latentç©ºé—´ä¸­ä½¿ç”¨ç®€å•æŠ€æœ¯æ¥å¢å¼ºç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯é«˜é¢‘ä¿¡æ¯
    """
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """åˆå§‹åŒ–ç®€å•Latentå¢å¼ºå™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸš€ åˆå§‹åŒ–ç®€å•Latentå¢å¼ºå™¨")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        print(f"   ğŸ¤– æ¨¡å‹: {model_name}")
        
        # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        print(f"âœ… ç®€å•Latentå¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š VAE scaling_factor: {self.pipeline.vae.config.scaling_factor}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("simple_latent_enhanced")
        self.output_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def enhance_reconstruction(self, audio_path: str, 
                            enhancement_method: str = "frequency_boost",
                            boost_factor: float = 1.5,
                            noise_level: float = 0.1) -> Dict:
        """
        æ‰§è¡Œç®€å•Latentå¢å¼ºé‡å»º
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            enhancement_method: å¢å¼ºæ–¹æ³• ('frequency_boost', 'noise_injection', 'hybrid')
            boost_factor: å¢å¼ºç³»æ•°
            noise_level: å™ªå£°æ°´å¹³
            
        Returns:
            åŒ…å«é‡å»ºç»“æœå’Œåˆ†æçš„å­—å…¸
        """
        print(f"\nğŸ”„ å¼€å§‹ç®€å•Latentå¢å¼ºé‡å»º: {Path(audio_path).name}")
        print(f"   ğŸ¯ å¢å¼ºæ–¹æ³•: {enhancement_method}")
        print(f"   ğŸ“ˆ å¢å¼ºç³»æ•°: {boost_factor}")
        print(f"   ğŸ”Š å™ªå£°æ°´å¹³: {noise_level}")
        
        # 1. åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘
        original_audio, processed_audio = self._load_and_preprocess_audio(audio_path)
        
        # 2. VAEç¼–ç åˆ°latentç©ºé—´
        latent = self._encode_audio(processed_audio)
        
        # 3. åˆ›å»ºå¯¹æ¯”ï¼šæ™®é€šVAEé‡å»º
        vae_only_audio = self._decode_audio(latent.clone())
        
        # 4. ç®€å•å¢å¼ºlatent
        enhanced_latent = self._enhance_latent_simple(
            latent,
            method=enhancement_method,
            boost_factor=boost_factor,
            noise_level=noise_level
        )
        
        # 5. VAEè§£ç å¢å¼ºåçš„latent
        enhanced_audio = self._decode_audio(enhanced_latent)
        
        # 6. ä¿å­˜ç»“æœ
        timestamp = int(time.time())
        paths = self._save_audio_results(
            original_audio, vae_only_audio, enhanced_audio, timestamp
        )
        
        # 7. è´¨é‡å¯¹æ¯”åˆ†æ
        quality_comparison = self._compare_quality(
            original_audio, vae_only_audio, enhanced_audio
        )
        
        # 8. é¢‘ç‡å¯¹æ¯”åˆ†æ
        frequency_comparison = self._compare_frequency_content(
            original_audio, vae_only_audio, enhanced_audio
        )
        
        # 9. å¯è§†åŒ–åˆ†æ
        self._create_comparison_visualizations(
            original_audio, vae_only_audio, enhanced_audio, timestamp
        )
        
        # 10. æ•´åˆç»“æœ
        result = {
            "input_file": audio_path,
            "timestamp": timestamp,
            "parameters": {
                "enhancement_method": enhancement_method,
                "boost_factor": boost_factor,
                "noise_level": noise_level
            },
            "paths": paths,
            "quality_comparison": quality_comparison,
            "frequency_comparison": frequency_comparison,
            "latent_shapes": {
                "original": latent.shape,
                "enhanced": enhanced_latent.shape
            },
            "processing_device": str(self.device)
        }
        
        # 11. æ˜¾ç¤ºç»“æœ
        self._display_comparison_results(result)
        
        return result
    
    def _load_and_preprocess_audio(self, audio_path: str) -> Tuple[np.ndarray, torch.Tensor]:
        """åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘"""
        print(f"   ğŸ“‚ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
        
        # åŠ è½½åŸå§‹éŸ³é¢‘
        original_audio, sr = torchaudio.load(audio_path)
        
        # è½¬æ¢ä¸ºå•å£°é“
        if original_audio.shape[0] > 1:
            original_audio = original_audio.mean(dim=0, keepdim=True)
        
        # é‡é‡‡æ ·åˆ°48kHz
        if sr != 48000:
            print(f"   ğŸ”„ é‡é‡‡æ ·: {sr}Hz -> 48000Hz")
            resampler = torchaudio.transforms.Resample(sr, 48000)
            processed_audio = resampler(original_audio)
        else:
            processed_audio = original_audio.clone()
        
        # é™åˆ¶é•¿åº¦åˆ°10ç§’
        max_length = 48000 * 10
        if processed_audio.shape[-1] > max_length:
            print(f"   âœ‚ï¸ æˆªå–å‰10ç§’")
            processed_audio = processed_audio[..., :max_length]
        
        # è½¬æ¢ä¸ºnumpyç”¨äºæœ€ç»ˆè¾“å‡º
        original_audio_np = original_audio.squeeze().numpy()
        
        print(f"   âœ… éŸ³é¢‘é¢„å¤„ç†å®Œæˆ: {processed_audio.shape}")
        
        return original_audio_np, processed_audio
    
    def _encode_audio(self, audio: torch.Tensor) -> torch.Tensor:
        """ä½¿ç”¨VAEç¼–ç éŸ³é¢‘åˆ°latentç©ºé—´"""
        print(f"   ğŸ”— VAEç¼–ç ...")
        
        with torch.no_grad():
            # è½¬æ¢ä¸ºnumpyå¹¶ä½¿ç”¨feature extractor
            audio_np = audio.squeeze().numpy()
            
            # ä½¿ç”¨ClapFeatureExtractorå¤„ç†éŸ³é¢‘
            inputs = self.pipeline.feature_extractor(
                audio_np,
                sampling_rate=48000,
                return_tensors="pt"
            )
            
            # è·å–melé¢‘è°±ç‰¹å¾
            mel_features = inputs["input_features"].to(self.device)
            
            # ç¡®ä¿æ˜¯4Då¼ é‡
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            # åŒ¹é…VAEçš„æ•°æ®ç±»å‹
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            # VAEç¼–ç 
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode()
            
            # åº”ç”¨scaling factor
            latent = latent * self.pipeline.vae.config.scaling_factor
            
            print(f"   âœ… ç¼–ç å®Œæˆ: {mel_features.shape} -> {latent.shape}")
            
            return latent
    
    def _enhance_latent_simple(self,
                              latent: torch.Tensor,
                              method: str = "frequency_boost",
                              boost_factor: float = 1.5,
                              noise_level: float = 0.1) -> torch.Tensor:
        """
        ä½¿ç”¨ç®€å•æŠ€æœ¯å¢å¼ºlatentè¡¨ç¤º
        """
        print(f"   ğŸ¨ ç®€å•Latentå¢å¼º: {method}")
        
        with torch.no_grad():
            enhanced_latent = latent.clone()
            
            if method == "frequency_boost":
                enhanced_latent = self._apply_frequency_boost(enhanced_latent, boost_factor)
            
            elif method == "noise_injection":
                enhanced_latent = self._apply_noise_injection(enhanced_latent, noise_level)
            
            elif method == "hybrid":
                # ç»„åˆå¤šç§æŠ€æœ¯
                enhanced_latent = self._apply_frequency_boost(enhanced_latent, boost_factor)
                enhanced_latent = self._apply_noise_injection(enhanced_latent, noise_level * 0.5)
                enhanced_latent = self._apply_contrast_enhancement(enhanced_latent, 1.2)
            
            elif method == "contrast_enhancement":
                enhanced_latent = self._apply_contrast_enhancement(enhanced_latent, boost_factor)
            
            print(f"   âœ… Latentå¢å¼ºå®Œæˆ")
            
            return enhanced_latent
    
    def _apply_frequency_boost(self, latent: torch.Tensor, boost_factor: float) -> torch.Tensor:
        """
        åº”ç”¨é¢‘ç‡å¢å¼ºï¼šè¯†åˆ«å¹¶å¢å¼ºé«˜é¢‘ç‰¹å¾
        """
        print(f"   ğŸ¼ åº”ç”¨é¢‘ç‡å¢å¼º: {boost_factor}x")
        
        # ä½¿ç”¨é«˜é€šæ»¤æ³¢å™¨è¯†åˆ«é«˜é¢‘ç‰¹å¾
        # å®šä¹‰è¾¹ç¼˜æ£€æµ‹æ ¸ï¼ˆLaplacianï¼‰
        laplacian_kernel = torch.tensor([
            [[[-1, -1, -1],
              [-1,  8, -1],
              [-1, -1, -1]]]
        ], dtype=latent.dtype, device=latent.device)
        
        enhanced_latent = latent.clone()
        
        # å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«åº”ç”¨é«˜é¢‘æ£€æµ‹å’Œå¢å¼º
        for c in range(latent.shape[1]):
            channel_latent = latent[:, c:c+1, :, :]  # [B, 1, H, W]
            
            # åº”ç”¨é«˜é¢‘æ£€æµ‹
            high_freq_response = F.conv2d(
                channel_latent,
                laplacian_kernel,
                padding=1
            )
            
            # å¢å¼ºé«˜é¢‘éƒ¨åˆ†
            enhanced_channel = channel_latent + high_freq_response * (boost_factor - 1.0)
            enhanced_latent[:, c:c+1, :, :] = enhanced_channel
        
        return enhanced_latent
    
    def _apply_noise_injection(self, latent: torch.Tensor, noise_level: float) -> torch.Tensor:
        """
        åº”ç”¨å™ªå£°æ³¨å…¥ï¼šæ·»åŠ ç»“æ„åŒ–å™ªå£°æ¥å¢å¼ºç»†èŠ‚
        """
        print(f"   ğŸ”Š åº”ç”¨å™ªå£°æ³¨å…¥: {noise_level}")
        
        # ç”Ÿæˆç»“æ„åŒ–å™ªå£°ï¼ˆä¸æ˜¯å®Œå…¨éšæœºï¼‰
        noise = torch.randn_like(latent) * noise_level
        
        # ä½¿ç”¨ä½é€šæ»¤æ³¢å™¨ä½¿å™ªå£°æ›´ç»“æ„åŒ–
        gaussian_kernel = torch.ones(1, 1, 3, 3, dtype=latent.dtype, device=latent.device) / 9
        
        structured_noise = torch.zeros_like(noise)
        for c in range(noise.shape[1]):
            channel_noise = noise[:, c:c+1, :, :]
            filtered_noise = F.conv2d(channel_noise, gaussian_kernel, padding=1)
            structured_noise[:, c:c+1, :, :] = filtered_noise
        
        # è‡ªé€‚åº”å™ªå£°å¼ºåº¦ï¼ˆåŸºäºlatentçš„å±€éƒ¨æ–¹å·®ï¼‰
        local_std = torch.std(latent, dim=[2, 3], keepdim=True)
        adaptive_noise = structured_noise * local_std.clamp(min=0.1)
        
        enhanced_latent = latent + adaptive_noise
        
        return enhanced_latent
    
    def _apply_contrast_enhancement(self, latent: torch.Tensor, contrast_factor: float) -> torch.Tensor:
        """
        åº”ç”¨å¯¹æ¯”åº¦å¢å¼ºï¼šå¢å¼ºlatentçš„åŠ¨æ€èŒƒå›´
        """
        print(f"   ğŸ“ˆ åº”ç”¨å¯¹æ¯”åº¦å¢å¼º: {contrast_factor}x")
        
        # è®¡ç®—æ¯ä¸ªé€šé“çš„å‡å€¼
        channel_means = torch.mean(latent, dim=[2, 3], keepdim=True)
        
        # å¢å¼ºä¸å‡å€¼çš„å·®å¼‚
        enhanced_latent = channel_means + (latent - channel_means) * contrast_factor
        
        return enhanced_latent
    
    def _decode_audio(self, latent: torch.Tensor) -> np.ndarray:
        """ä½¿ç”¨VAEè§£ç latentåˆ°éŸ³é¢‘"""
        print(f"   ğŸ”„ VAEè§£ç ...")
        
        with torch.no_grad():
            # åå‘scaling
            latent_for_decode = latent / self.pipeline.vae.config.scaling_factor
            
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            vae_dtype = next(self.pipeline.vae.parameters()).dtype
            latent_for_decode = latent_for_decode.to(vae_dtype)
            
            # VAEè§£ç åˆ°melé¢‘è°±
            mel_spectrogram = self.pipeline.vae.decode(latent_for_decode).sample
            
            # ä½¿ç”¨vocoderè½¬æ¢ä¸ºéŸ³é¢‘æ³¢å½¢
            audio_tensor = self.pipeline.mel_spectrogram_to_waveform(mel_spectrogram)
            audio_np = audio_tensor.squeeze().cpu().numpy()
            
            print(f"   âœ… è§£ç å®Œæˆ: {latent.shape} -> éŸ³é¢‘é•¿åº¦ {len(audio_np)}")
            
            return audio_np
    
    def _save_audio_results(self, 
                          original: np.ndarray,
                          vae_only: np.ndarray,
                          enhanced: np.ndarray,
                          timestamp: int) -> Dict[str, str]:
        """ä¿å­˜éŸ³é¢‘ç»“æœ"""
        print(f"   ğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...")
        
        paths = {}
        
        # ä¿å­˜åŸå§‹éŸ³é¢‘ï¼ˆé‡é‡‡æ ·åˆ°16kHzï¼‰
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        original_path = self.output_dir / f"original_{timestamp}.wav"
        sf.write(str(original_path), original_16k, 16000)
        paths["original"] = str(original_path)
        
        # ä¿å­˜VAE-onlyé‡å»ºéŸ³é¢‘
        vae_only_path = self.output_dir / f"vae_only_{timestamp}.wav"
        sf.write(str(vae_only_path), vae_only, 16000)
        paths["vae_only"] = str(vae_only_path)
        
        # ä¿å­˜å¢å¼ºéŸ³é¢‘
        enhanced_path = self.output_dir / f"latent_enhanced_{timestamp}.wav"
        sf.write(str(enhanced_path), enhanced, 16000)
        paths["enhanced"] = str(enhanced_path)
        
        print(f"   âœ… éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜")
        
        return paths
    
    def _compare_quality(self, 
                        original: np.ndarray,
                        vae_only: np.ndarray,
                        enhanced: np.ndarray) -> Dict:
        """å¯¹æ¯”é‡å»ºè´¨é‡"""
        print(f"   ğŸ“Š è´¨é‡å¯¹æ¯”åˆ†æ...")
        
        # é‡é‡‡æ ·åŸå§‹éŸ³é¢‘åˆ°16kHz
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        def calculate_metrics(orig, recon):
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            min_len = min(len(orig), len(recon))
            o = orig[:min_len]
            r = recon[:min_len]
            
            # åŸºæœ¬æŒ‡æ ‡
            mse = np.mean((o - r) ** 2)
            mae = np.mean(np.abs(o - r))
            
            # SNR
            signal_power = np.mean(o ** 2)
            noise_power = mse
            snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
            
            # ç›¸å…³æ€§
            correlation = np.corrcoef(o, r)[0, 1] if min_len > 1 else 0.0
            if np.isnan(correlation):
                correlation = 0.0
            
            return {
                "snr_db": snr,
                "correlation": correlation,
                "mse": mse,
                "mae": mae,
                "signal_power": signal_power
            }
        
        # è®¡ç®—VAE-onlyæŒ‡æ ‡
        vae_metrics = calculate_metrics(original_16k, vae_only)
        
        # è®¡ç®—å¢å¼ºç‰ˆæŒ‡æ ‡
        enhanced_metrics = calculate_metrics(original_16k, enhanced)
        
        # è®¡ç®—æ”¹è¿›é‡
        improvements = {
            "snr_improvement": enhanced_metrics["snr_db"] - vae_metrics["snr_db"],
            "correlation_improvement": enhanced_metrics["correlation"] - vae_metrics["correlation"],
            "mse_improvement": vae_metrics["mse"] - enhanced_metrics["mse"],
            "mae_improvement": vae_metrics["mae"] - enhanced_metrics["mae"]
        }
        
        comparison = {
            "vae_only": vae_metrics,
            "enhanced": enhanced_metrics,
            "improvements": improvements
        }
        
        print(f"   âœ… è´¨é‡å¯¹æ¯”åˆ†æå®Œæˆ")
        
        return comparison
    
    def _compare_frequency_content(self,
                                 original: np.ndarray,
                                 vae_only: np.ndarray,
                                 enhanced: np.ndarray) -> Dict:
        """å¯¹æ¯”é¢‘ç‡å†…å®¹"""
        print(f"   ğŸ¼ é¢‘ç‡å¯¹æ¯”åˆ†æ...")
        
        # é‡é‡‡æ ·åŸå§‹éŸ³é¢‘åˆ°16kHz
        original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
        
        def analyze_frequency_bands(orig, recon):
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            min_len = min(len(orig), len(recon))
            o = orig[:min_len]
            r = recon[:min_len]
            
            # FFTåˆ†æ
            if min_len >= 8192:
                n_fft = 8192
            else:
                n_fft = 2 ** int(np.log2(min_len))
            
            orig_fft = np.abs(np.fft.fft(o[:n_fft]))[:n_fft//2]
            recon_fft = np.abs(np.fft.fft(r[:n_fft]))[:n_fft//2]
            
            freqs = np.fft.fftfreq(n_fft, 1/16000)[:n_fft//2]
            
            # é¢‘æ®µå®šä¹‰
            low_mask = freqs < 500
            mid_mask = (freqs >= 500) & (freqs < 4000)
            high_mask = freqs >= 4000
            
            # è®¡ç®—å„é¢‘æ®µèƒ½é‡ä¿æŒç‡
            low_retention = np.sum(recon_fft[low_mask]) / (np.sum(orig_fft[low_mask]) + 1e-10)
            mid_retention = np.sum(recon_fft[mid_mask]) / (np.sum(orig_fft[mid_mask]) + 1e-10)
            high_retention = np.sum(recon_fft[high_mask]) / (np.sum(orig_fft[high_mask]) + 1e-10)
            
            # æ€»ä½“é¢‘è°±ç›¸å…³æ€§
            freq_corr = np.corrcoef(orig_fft, recon_fft)[0, 1]
            if np.isnan(freq_corr):
                freq_corr = 0.0
            
            return {
                "low_freq_retention": low_retention,
                "mid_freq_retention": mid_retention,
                "high_freq_retention": high_retention,
                "frequency_correlation": freq_corr
            }
        
        # åˆ†æVAE-onlyé¢‘ç‡ç‰¹æ€§
        vae_freq = analyze_frequency_bands(original_16k, vae_only)
        
        # åˆ†æå¢å¼ºç‰ˆé¢‘ç‡ç‰¹æ€§
        enhanced_freq = analyze_frequency_bands(original_16k, enhanced)
        
        # è®¡ç®—é¢‘ç‡æ”¹è¿›
        freq_improvements = {
            "low_freq_improvement": enhanced_freq["low_freq_retention"] - vae_freq["low_freq_retention"],
            "mid_freq_improvement": enhanced_freq["mid_freq_retention"] - vae_freq["mid_freq_retention"],
            "high_freq_improvement": enhanced_freq["high_freq_retention"] - vae_freq["high_freq_retention"],
            "frequency_correlation_improvement": enhanced_freq["frequency_correlation"] - vae_freq["frequency_correlation"]
        }
        
        comparison = {
            "vae_only": vae_freq,
            "enhanced": enhanced_freq,
            "improvements": freq_improvements
        }
        
        print(f"   âœ… é¢‘ç‡å¯¹æ¯”åˆ†æå®Œæˆ")
        
        return comparison
    
    def _create_comparison_visualizations(self,
                                        original: np.ndarray,
                                        vae_only: np.ndarray,
                                        enhanced: np.ndarray,
                                        timestamp: int):
        """åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–"""
        print(f"   ğŸ“ˆ ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–...")
        
        try:
            # é‡é‡‡æ ·åŸå§‹éŸ³é¢‘åˆ°16kHz
            original_16k = librosa.resample(original, orig_sr=48000, target_sr=16000)
            
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            min_len = min(len(original_16k), len(vae_only), len(enhanced))
            orig = original_16k[:min_len]
            vae = vae_only[:min_len]
            enh = enhanced[:min_len]
            
            # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            fig.suptitle('VAE vs Latentå¢å¼º éŸ³é¢‘é‡å»ºå¯¹æ¯”', fontsize=16, fontweight='bold')
            
            # 1. æ—¶åŸŸæ³¢å½¢å¯¹æ¯”
            time_samples = min(16000, min_len)  # æ˜¾ç¤ºå‰1ç§’
            axes[0, 0].plot(orig[:time_samples], label='åŸå§‹éŸ³é¢‘', alpha=0.8, linewidth=0.8)
            axes[0, 0].plot(vae[:time_samples], label='VAE-only', alpha=0.8, linewidth=0.8)
            axes[0, 0].plot(enh[:time_samples], label='Latentå¢å¼º', alpha=0.8, linewidth=0.8)
            axes[0, 0].set_title('æ—¶åŸŸæ³¢å½¢å¯¹æ¯”ï¼ˆå‰1ç§’ï¼‰')
            axes[0, 0].set_xlabel('é‡‡æ ·ç‚¹')
            axes[0, 0].set_ylabel('æŒ¯å¹…')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. é¢‘è°±å¯¹æ¯”
            if min_len >= 8192:
                n_fft = 8192
                orig_fft = np.abs(np.fft.fft(orig[:n_fft]))[:n_fft//2]
                vae_fft = np.abs(np.fft.fft(vae[:n_fft]))[:n_fft//2]
                enh_fft = np.abs(np.fft.fft(enh[:n_fft]))[:n_fft//2]
                freqs = np.fft.fftfreq(n_fft, 1/16000)[:n_fft//2]
                
                axes[0, 1].loglog(freqs[1:], orig_fft[1:], label='åŸå§‹éŸ³é¢‘', alpha=0.8, linewidth=1.5)
                axes[0, 1].loglog(freqs[1:], vae_fft[1:], label='VAE-only', alpha=0.8, linewidth=1.5)
                axes[0, 1].loglog(freqs[1:], enh_fft[1:], label='Latentå¢å¼º', alpha=0.8, linewidth=1.5)
                axes[0, 1].set_title('é¢‘è°±å¯¹æ¯”')
                axes[0, 1].set_xlabel('é¢‘ç‡ (Hz)')
                axes[0, 1].set_ylabel('å¹…åº¦')
                axes[0, 1].legend()
                axes[0, 1].grid(True, alpha=0.3)
                
                # 6. é¢‘æ®µèƒ½é‡å¯¹æ¯”
                low_mask = freqs < 500
                mid_mask = (freqs >= 500) & (freqs < 4000)
                high_mask = freqs >= 4000
                
                bands = ['Low\n(<500Hz)', 'Mid\n(500Hz-4kHz)', 'High\n(>4kHz)']
                orig_energies = [
                    np.sum(orig_fft[low_mask]),
                    np.sum(orig_fft[mid_mask]),
                    np.sum(orig_fft[high_mask])
                ]
                vae_energies = [
                    np.sum(vae_fft[low_mask]),
                    np.sum(vae_fft[mid_mask]),
                    np.sum(vae_fft[high_mask])
                ]
                enh_energies = [
                    np.sum(enh_fft[low_mask]),
                    np.sum(enh_fft[mid_mask]),
                    np.sum(enh_fft[high_mask])
                ]
                
                x = np.arange(len(bands))
                width = 0.25
                
                axes[0, 2].bar(x - width, orig_energies, width, label='åŸå§‹', alpha=0.8)
                axes[0, 2].bar(x, vae_energies, width, label='VAE-only', alpha=0.8)
                axes[0, 2].bar(x + width, enh_energies, width, label='Latentå¢å¼º', alpha=0.8)
                
                axes[0, 2].set_title('é¢‘æ®µèƒ½é‡å¯¹æ¯”')
                axes[0, 2].set_xlabel('é¢‘æ®µ')
                axes[0, 2].set_ylabel('èƒ½é‡')
                axes[0, 2].set_xticks(x)
                axes[0, 2].set_xticklabels(bands)
                axes[0, 2].legend()
                axes[0, 2].grid(True, alpha=0.3)
            
            # 3. Melé¢‘è°±å›¾ - åŸå§‹
            orig_mel = librosa.feature.melspectrogram(y=orig, sr=16000, n_mels=128)
            orig_mel_db = librosa.power_to_db(orig_mel, ref=np.max)
            im1 = axes[1, 0].imshow(orig_mel_db, aspect='auto', origin='lower', cmap='viridis')
            axes[1, 0].set_title('åŸå§‹éŸ³é¢‘Melé¢‘è°±')
            axes[1, 0].set_xlabel('æ—¶é—´å¸§')
            axes[1, 0].set_ylabel('Melé¢‘ç‡')
            plt.colorbar(im1, ax=axes[1, 0])
            
            # 4. Melé¢‘è°±å›¾ - VAE only
            vae_mel = librosa.feature.melspectrogram(y=vae, sr=16000, n_mels=128)
            vae_mel_db = librosa.power_to_db(vae_mel, ref=np.max)
            im2 = axes[1, 1].imshow(vae_mel_db, aspect='auto', origin='lower', cmap='viridis')
            axes[1, 1].set_title('VAE-onlyé‡å»ºMelé¢‘è°±')
            axes[1, 1].set_xlabel('æ—¶é—´å¸§')
            axes[1, 1].set_ylabel('Melé¢‘ç‡')
            plt.colorbar(im2, ax=axes[1, 1])
            
            # 5. Melé¢‘è°±å›¾ - å¢å¼ºç‰ˆ
            enh_mel = librosa.feature.melspectrogram(y=enh, sr=16000, n_mels=128)
            enh_mel_db = librosa.power_to_db(enh_mel, ref=np.max)
            im3 = axes[1, 2].imshow(enh_mel_db, aspect='auto', origin='lower', cmap='viridis')
            axes[1, 2].set_title('Latentå¢å¼ºMelé¢‘è°±')
            axes[1, 2].set_xlabel('æ—¶é—´å¸§')
            axes[1, 2].set_ylabel('Melé¢‘ç‡')
            plt.colorbar(im3, ax=axes[1, 2])
            
            # ä¿å­˜å›¾è¡¨
            plt.tight_layout()
            plot_path = self.output_dir / f"comparison_analysis_{timestamp}.png"
            plt.savefig(str(plot_path), dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"   âœ… å¯¹æ¯”å¯è§†åŒ–å·²ä¿å­˜: {plot_path}")
            
        except Exception as e:
            print(f"   âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    def _display_comparison_results(self, result: Dict):
        """æ˜¾ç¤ºå¯¹æ¯”ç»“æœ"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ VAE vs Latentå¢å¼º é‡å»ºå¯¹æ¯”ç»“æœ")
        print(f"{'='*80}")
        
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {result['input_file']}")
        print(f"ğŸ“± å¤„ç†è®¾å¤‡: {result['processing_device']}")
        print(f"ğŸ“ Latentå½¢çŠ¶: {result['latent_shapes']['original']}")
        
        params = result['parameters']
        print(f"\nâš™ï¸ å¢å¼ºå‚æ•°:")
        print(f"   ğŸ¯ å¢å¼ºæ–¹æ³•: {params['enhancement_method']}")
        print(f"   ğŸ“ˆ å¢å¼ºç³»æ•°: {params['boost_factor']}")
        print(f"   ğŸ”Š å™ªå£°æ°´å¹³: {params['noise_level']}")
        
        print(f"\nğŸ“Š è´¨é‡å¯¹æ¯”:")
        quality = result['quality_comparison']
        
        vae_metrics = quality['vae_only']
        enh_metrics = quality['enhanced']
        improvements = quality['improvements']
        
        print(f"   ğŸµ ä¿¡å™ªæ¯” (SNR):")
        print(f"      VAE-only: {vae_metrics['snr_db']:.2f} dB")
        print(f"      Latentå¢å¼º: {enh_metrics['snr_db']:.2f} dB")
        print(f"      æ”¹è¿›: {improvements['snr_improvement']:+.2f} dB")
        
        print(f"   ğŸ”— ç›¸å…³æ€§:")
        print(f"      VAE-only: {vae_metrics['correlation']:.4f}")
        print(f"      Latentå¢å¼º: {enh_metrics['correlation']:.4f}")
        print(f"      æ”¹è¿›: {improvements['correlation_improvement']:+.4f}")
        
        print(f"\nğŸ¼ é¢‘ç‡å¯¹æ¯”:")
        freq = result['frequency_comparison']
        
        vae_freq = freq['vae_only']
        enh_freq = freq['enhanced']
        freq_improvements = freq['improvements']
        
        print(f"   ğŸ¶ ä½é¢‘ä¿æŒç‡ (<500Hz):")
        print(f"      VAE-only: {vae_freq['low_freq_retention']:.3f}")
        print(f"      Latentå¢å¼º: {enh_freq['low_freq_retention']:.3f}")
        print(f"      æ”¹è¿›: {freq_improvements['low_freq_improvement']:+.3f}")
        
        print(f"   ğŸµ ä¸­é¢‘ä¿æŒç‡ (500Hz-4kHz):")
        print(f"      VAE-only: {vae_freq['mid_freq_retention']:.3f}")
        print(f"      Latentå¢å¼º: {enh_freq['mid_freq_retention']:.3f}")
        print(f"      æ”¹è¿›: {freq_improvements['mid_freq_improvement']:+.3f}")
        
        print(f"   ğŸ¼ é«˜é¢‘ä¿æŒç‡ (>4kHz):")
        print(f"      VAE-only: {vae_freq['high_freq_retention']:.3f}")
        print(f"      Latentå¢å¼º: {enh_freq['high_freq_retention']:.3f}")
        print(f"      æ”¹è¿›: {freq_improvements['high_freq_improvement']:+.3f}")
        
        # æ•ˆæœè¯„ä¼°
        print(f"\nğŸ¯ æ•ˆæœè¯„ä¼°:")
        
        if improvements['snr_improvement'] > 1.0:
            print(f"   âœ… æ˜¾è‘—çš„è´¨é‡æ”¹è¿›ï¼SNRæå‡ {improvements['snr_improvement']:.1f}dB")
        elif improvements['snr_improvement'] > 0.5:
            print(f"   ğŸŸ¢ æ˜æ˜¾çš„è´¨é‡æ”¹è¿›ï¼ŒSNRæå‡ {improvements['snr_improvement']:.1f}dB")
        elif improvements['snr_improvement'] > 0:
            print(f"   ğŸ”¶ è½»å¾®çš„è´¨é‡æ”¹è¿›ï¼ŒSNRæå‡ {improvements['snr_improvement']:.1f}dB")
        else:
            print(f"   âŒ è´¨é‡ä¸‹é™ï¼ŒSNRä¸‹é™ {-improvements['snr_improvement']:.1f}dB")
        
        if freq_improvements['high_freq_improvement'] > 0.1:
            print(f"   ğŸ¼ é«˜é¢‘æ˜¾è‘—å¢å¼ºï¼æå‡ {freq_improvements['high_freq_improvement']*100:.1f}%")
        elif freq_improvements['high_freq_improvement'] > 0.05:
            print(f"   ğŸµ é«˜é¢‘æ˜æ˜¾å¢å¼ºï¼Œæå‡ {freq_improvements['high_freq_improvement']*100:.1f}%")
        elif freq_improvements['high_freq_improvement'] > 0:
            print(f"   ğŸ”¸ é«˜é¢‘è½»å¾®å¢å¼ºï¼Œæå‡ {freq_improvements['high_freq_improvement']*100:.1f}%")
        else:
            print(f"   âš ï¸ é«˜é¢‘æœªæ”¹å–„æˆ–ä¸‹é™")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        for name, path in result['paths'].items():
            print(f"   {name}: {path}")

def demo_simple_latent_enhancement():
    """æ¼”ç¤ºç®€å•Latentå¢å¼º"""
    print("ğŸš€ Step 2: ç®€å•Latentå¢å¼ºé‡å»º")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        print("è¯·ç¡®ä¿AudioLDM2_Music_output.wavåœ¨å½“å‰ç›®å½•ä¸­")
        return
    
    # åˆå§‹åŒ–ç®€å•Latentå¢å¼ºå™¨
    enhancer = SimpleLatentEnhancer()
    
    # æµ‹è¯•ä¸åŒçš„å¢å¼ºæ–¹æ³•
    test_configs = [
        {
            "name": "é¢‘ç‡å¢å¼º",
            "method": "frequency_boost",
            "boost_factor": 1.5,
            "noise_level": 0.0
        },
        {
            "name": "å™ªå£°æ³¨å…¥",
            "method": "noise_injection",
            "boost_factor": 1.0,
            "noise_level": 0.1
        },
        {
            "name": "å¯¹æ¯”åº¦å¢å¼º",
            "method": "contrast_enhancement",
            "boost_factor": 1.3,
            "noise_level": 0.0
        },
        {
            "name": "æ··åˆå¢å¼º",
            "method": "hybrid",
            "boost_factor": 1.4,
            "noise_level": 0.08
        }
    ]
    
    results = []
    
    for config in test_configs:
        print(f"\nğŸ”¬ æµ‹è¯•é…ç½®: {config['name']}")
        
        result = enhancer.enhance_reconstruction(
            input_file,
            enhancement_method=config['method'],
            boost_factor=config['boost_factor'],
            noise_level=config['noise_level']
        )
        
        result['config_name'] = config['name']
        results.append(result)
    
    # å¯¹æ¯”æ‰€æœ‰é…ç½®çš„ç»“æœ
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æ‰€æœ‰å¢å¼ºæ–¹æ³•å¯¹æ¯”æ€»ç»“")
    print(f"{'='*80}")
    
    best_snr = -float('inf')
    best_high_freq = -float('inf')
    best_overall = None
    
    for i, result in enumerate(results):
        config_name = result['config_name']
        improvements = result['quality_comparison']['improvements']
        freq_improvements = result['frequency_comparison']['improvements']
        
        print(f"\n{i+1}. {config_name}:")
        print(f"   ğŸ“ˆ SNRæ”¹è¿›: {improvements['snr_improvement']:+.2f} dB")
        print(f"   ğŸ¼ é«˜é¢‘æ”¹è¿›: {freq_improvements['high_freq_improvement']*100:+.1f}%")
        print(f"   ğŸ”— ç›¸å…³æ€§æ”¹è¿›: {improvements['correlation_improvement']:+.4f}")
        
        # ç»¼åˆè¯„åˆ†
        overall_score = improvements['snr_improvement'] + freq_improvements['high_freq_improvement'] * 10
        
        if overall_score > (best_snr + best_high_freq * 10):
            best_snr = improvements['snr_improvement']
            best_high_freq = freq_improvements['high_freq_improvement']
            best_overall = config_name
    
    print(f"\nğŸ† æœ€ä½³å¢å¼ºæ–¹æ³•: {best_overall}")
    print(f"   ğŸ“ˆ SNRæ”¹è¿›: {best_snr:+.2f} dB")
    print(f"   ğŸ¼ é«˜é¢‘æ”¹è¿›: {best_high_freq*100:+.1f}%")
    
    print(f"\nâœ… ç®€å•Latentå¢å¼ºæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š æŸ¥çœ‹è¾“å‡ºç›®å½•: simple_latent_enhanced/")
    print(f"ğŸµ å¯¹æ¯”ä¸åŒå¢å¼ºæ–¹æ³•çš„éŸ³é¢‘æ•ˆæœ")
    
    print(f"\nğŸ’¡ æ€»ç»“ï¼š")
    print(f"   ğŸ“Š æˆ‘ä»¬æµ‹è¯•äº†å¤šç§ç®€å•çš„latentå¢å¼ºæŠ€æœ¯")
    print(f"   ğŸ¯ è¿™äº›æ–¹æ³•æ¯”å®Œæ•´çš„diffusionè¿‡ç¨‹æ›´ç®€å•ã€æ›´å¯æ§")
    print(f"   ğŸ” å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©æœ€é€‚åˆçš„å¢å¼ºæ–¹æ³•")
    print(f"   ğŸš€ ä¸‹ä¸€æ­¥å¯ä»¥è€ƒè™‘ç»„åˆå¤šç§æ–¹æ³•æˆ–ä¼˜åŒ–å‚æ•°")

if __name__ == "__main__":
    demo_simple_latent_enhancement()
