"""
è°ƒè¯• AudioLDM2 çš„å®é™… latent ç»´åº¦
"""

import torch
import torchaudio
from diffusers import AudioLDM2Pipeline
import warnings
warnings.filterwarnings("ignore")

def debug_audioldm2_latent_dimensions():
    """è°ƒè¯• AudioLDM2 çš„å®é™…æ½œåœ¨ç©ºé—´ç»´åº¦"""
    
    print("ğŸ” è°ƒè¯• AudioLDM2 æ½œåœ¨ç©ºé—´ç»´åº¦")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # åŠ è½½ pipeline
    pipeline = AudioLDM2Pipeline.from_pretrained(
        "cvssp/audioldm2-music",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    ).to(device)
    
    # åŠ è½½æµ‹è¯•éŸ³é¢‘
    audio, sr = torchaudio.load("AudioLDM2_Music_output.wav")
    print(f"ğŸ“Š åŸå§‹éŸ³é¢‘: {audio.shape}, {sr}Hz")
    
    # é‡é‡‡æ ·åˆ° 48kHz
    if sr != 48000:
        resampler = torchaudio.transforms.Resample(sr, 48000)
        audio = resampler(audio)
    
    if audio.shape[0] > 1:
        audio = audio.mean(dim=0, keepdim=True)
    
    print(f"ğŸ“Š å¤„ç†åéŸ³é¢‘: {audio.shape}")
    
    # ç¼–ç ä¸º latent
    with torch.no_grad():
        if audio.is_cuda:
            audio = audio.cpu()
        
        audio_numpy = audio.squeeze(0).numpy()
        
        # ClapFeatureExtractor
        inputs = pipeline.feature_extractor(
            audio_numpy,
            sampling_rate=48000,
            return_tensors="pt"
        )
        
        mel_features = inputs["input_features"].to(device)
        if mel_features.dim() == 3:
            mel_features = mel_features.unsqueeze(1)
        
        if device == "cuda":
            mel_features = mel_features.half()
        
        print(f"ğŸ“Š Mel features: {mel_features.shape}")
        
        # VAE ç¼–ç 
        latent_dist = pipeline.vae.encode(mel_features)
        latent = latent_dist.latent_dist.mode() if hasattr(latent_dist.latent_dist, 'mode') else latent_dist.latent_dist.sample()
        
        print(f"ğŸ“Š Latent: {latent.shape}")
        print(f"ğŸ“Š Latent å…ƒç´ æ€»æ•°: {latent.numel()}")
        
        # åˆ†ææ¯ä¸ªç»´åº¦
        if latent.dim() == 4:
            batch, channels, height, width = latent.shape
            print(f"ğŸ“Š Latent ç»´åº¦åˆ†æ:")
            print(f"   Batch: {batch}")
            print(f"   Channels: {channels}")
            print(f"   Height (æ—¶é—´): {height}")
            print(f"   Width (é¢‘ç‡): {width}")
        
        # æµ‹è¯•ä¸åŒéŸ³é¢‘é•¿åº¦
        test_lengths = [1, 5, 10]  # ç§’
        print(f"\nğŸ§ª æµ‹è¯•ä¸åŒéŸ³é¢‘é•¿åº¦çš„ latent ç»´åº¦:")
        
        for length in test_lengths:
            test_samples = int(48000 * length)
            if test_samples <= audio.shape[-1]:
                test_audio = audio[..., :test_samples]
                test_audio_numpy = test_audio.squeeze(0).numpy()
                
                test_inputs = pipeline.feature_extractor(
                    test_audio_numpy,
                    sampling_rate=48000,
                    return_tensors="pt"
                )
                
                test_mel = test_inputs["input_features"].to(device)
                if test_mel.dim() == 3:
                    test_mel = test_mel.unsqueeze(1)
                
                if device == "cuda":
                    test_mel = test_mel.half()
                
                test_latent_dist = pipeline.vae.encode(test_mel)
                test_latent = test_latent_dist.latent_dist.mode() if hasattr(test_latent_dist.latent_dist, 'mode') else test_latent_dist.latent_dist.sample()
                
                print(f"   {length}s éŸ³é¢‘ -> Latent: {test_latent.shape}")

if __name__ == "__main__":
    debug_audioldm2_latent_dimensions()
