"""
AudioLDM2 DDCM (Denoising Diffusion Codebook Model) ÂÆûÁé∞
Âü∫‰∫é DDCM ËÆ∫ÊñáÊÄùÊÉ≥Ôºå‰∏∫ AudioLDM2 ÂàõÂª∫Á†ÅÊú¨ÂåñÁöÑ diffusion ËøáÁ®ã
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from diffusers import AudioLDM2Pipeline
import torchaudio
from pathlib import Path
import json
import pickle
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings("ignore")

class AudioCodebook(nn.Module):
    """
    Èü≥È¢ëÊâ©Êï£Á†ÅÊú¨
    ‰∏∫ AudioLDM2 ÁöÑ latent space ËÆæËÆ°ÁöÑÁ†ÅÊú¨ÁªìÊûÑ
    """
    
    def __init__(self, 
                 codebook_size: int = 1024,
                 latent_dim: int = 8,
                 latent_height: int = 250, 
                 latent_width: int = 16,
                 init_method: str = "gaussian"):
        """
        ÂàùÂßãÂåñÈü≥È¢ëÁ†ÅÊú¨
        
        Args:
            codebook_size: Á†ÅÊú¨Â§ßÂ∞è
            latent_dim: ÊΩúÂú®Á©∫Èó¥Áª¥Â∫¶ (AudioLDM2 VAE latent channels)
            latent_height: ÊΩúÂú®Á©∫Èó¥È´òÂ∫¶ (Êó∂Èó¥Áª¥Â∫¶)
            latent_width: ÊΩúÂú®Á©∫Èó¥ÂÆΩÂ∫¶ (È¢ëÁéáÁª¥Â∫¶)
            init_method: ÂàùÂßãÂåñÊñπÊ≥ï ("gaussian", "uniform", "kmeans")
        """
        super().__init__()
        
        self.codebook_size = codebook_size
        self.latent_dim = latent_dim
        self.latent_height = latent_height
        self.latent_width = latent_width
        self.latent_shape = (latent_dim, latent_height, latent_width)
        
        # Á†ÅÊú¨ÂêëÈáèÔºö[codebook_size, latent_dim, latent_height, latent_width]
        if init_method == "gaussian":
            self.codebook = nn.Parameter(
                torch.randn(codebook_size, latent_dim, latent_height, latent_width)
            )
        elif init_method == "uniform":
            self.codebook = nn.Parameter(
                torch.rand(codebook_size, latent_dim, latent_height, latent_width) * 2 - 1
            )
        else:
            # Âª∂ËøüÂàùÂßãÂåñÔºåÁî®‰∫é k-means
            self.register_parameter('codebook', None)
        
        self.usage_count = torch.zeros(codebook_size)
        
    def initialize_with_kmeans(self, sample_latents: torch.Tensor, n_samples: int = 10000):
        """
        ‰ΩøÁî® k-means ÂàùÂßãÂåñÁ†ÅÊú¨
        
        Args:
            sample_latents: Ê†∑Êú¨ÊΩúÂú®Ë°®Á§∫ [n_samples, latent_dim, latent_height, latent_width]
            n_samples: Áî®‰∫éËÅöÁ±ªÁöÑÊ†∑Êú¨Êï∞Èáè
        """
        print(f"üîß ‰ΩøÁî® K-means ÂàùÂßãÂåñÁ†ÅÊú¨ (Ê†∑Êú¨Êï∞: {min(n_samples, len(sample_latents))})")
        
        # ÈöèÊú∫ÈÄâÊã©Ê†∑Êú¨
        if len(sample_latents) > n_samples:
            indices = torch.randperm(len(sample_latents))[:n_samples]
            sample_latents = sample_latents[indices]
        
        # Â±ïÂπ≥Ê†∑Êú¨
        flattened = sample_latents.view(len(sample_latents), -1).cpu().numpy()
        
        # K-means ËÅöÁ±ª
        kmeans = KMeans(n_clusters=self.codebook_size, random_state=42, n_init=10)
        kmeans.fit(flattened)
        
        # ÈáçÂ°ë‰∏∫Á†ÅÊú¨ÂΩ¢Áä∂
        centroids = torch.tensor(kmeans.cluster_centers_).float()
        centroids = centroids.view(self.codebook_size, *self.latent_shape)        
        self.codebook = nn.Parameter(centroids)
        print(f"‚úÖ K-means Á†ÅÊú¨ÂàùÂßãÂåñÂÆåÊàê")
    
    def find_nearest_codes(self, latents: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ‰∏∫ÁªôÂÆöÁöÑÊΩúÂú®Ë°®Á§∫ÊâæÂà∞ÊúÄËøëÁöÑÁ†ÅÊú¨ÂêëÈáè
        
        Args:
            latents: ÊΩúÂú®Ë°®Á§∫ [batch_size, latent_dim, latent_height, latent_width]
            
        Returns:
            indices: Á†ÅÊú¨Á¥¢Âºï [batch_size]
            quantized: ÈáèÂåñÂêéÁöÑÊΩúÂú®Ë°®Á§∫ [batch_size, latent_dim, latent_height, latent_width]
            distances: Ë∑ùÁ¶ª [batch_size]
        """
        batch_size = latents.shape[0]
        
        # Á°Æ‰øùÊï∞ÊçÆÁ±ªÂûã‰∏ÄËá¥
        latents = latents.float()
        codebook_float = self.codebook.float()
        
        # Â±ïÂπ≥Áî®‰∫éËÆ°ÁÆóË∑ùÁ¶ª
        latents_flat = latents.view(batch_size, -1)  # [batch, latent_dim*height*width]
        codebook_flat = codebook_float.view(self.codebook_size, -1)  # [codebook_size, latent_dim*height*width]
        
        # ËÆ°ÁÆó L2 Ë∑ùÁ¶ª
        distances = torch.cdist(latents_flat, codebook_flat, p=2)  # [batch, codebook_size]
        
        # ÊâæÂà∞ÊúÄËøëÁöÑÁ†ÅÊú¨ÂêëÈáè
        min_distances, indices = torch.min(distances, dim=1)  # [batch]
        
        # Ëé∑ÂèñÈáèÂåñÂêéÁöÑÂêëÈáè
        quantized = codebook_float[indices]  # [batch, latent_dim, latent_height, latent_width]
        
        # Êõ¥Êñ∞‰ΩøÁî®ËÆ°Êï∞
        with torch.no_grad():
            for idx in indices:
                self.usage_count[idx] += 1
        
        return indices, quantized, min_distances
    
    def get_code_by_index(self, indices: torch.Tensor) -> torch.Tensor:
        """
        Ê†πÊçÆÁ¥¢ÂºïËé∑ÂèñÁ†ÅÊú¨ÂêëÈáè
        
        Args:
            indices: Á†ÅÊú¨Á¥¢Âºï [batch_size]
            
        Returns:
            codes: Á†ÅÊú¨ÂêëÈáè [batch_size, latent_dim, latent_height, latent_width]
        """
        return self.codebook[indices]
    
    def get_usage_stats(self) -> Dict:
        """Ëé∑ÂèñÁ†ÅÊú¨‰ΩøÁî®ÁªüËÆ°"""
        used_codes = (self.usage_count > 0).sum().item()
        total_usage = self.usage_count.sum().item()
        
        return {
            "used_codes": used_codes,
            "total_codes": self.codebook_size,
            "usage_rate": used_codes / self.codebook_size,
            "total_usage": total_usage,
            "avg_usage": total_usage / max(used_codes, 1)
        }

class AudioLDM2_DDCM(nn.Module):
    """
    AudioLDM2 DDCM ‰∏ªÁ±ª
    ÈõÜÊàêÁ†ÅÊú¨ÂåñÊâ©Êï£Ê®°ÂûãÂà∞ AudioLDM2 pipeline
    """
    
    def __init__(self, 
                 model_name: str = "cvssp/audioldm2-music",
                 codebook_size: int = 1024,
                 compression_level: str = "medium"):
        """
        ÂàùÂßãÂåñ AudioLDM2 DDCM
        
        Args:
            model_name: AudioLDM2 Ê®°ÂûãÂêçÁß∞
            codebook_size: Á†ÅÊú¨Â§ßÂ∞è
            compression_level: ÂéãÁº©Á∫ßÂà´ ("low", "medium", "high", "extreme")
        """
        super().__init__()
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"üéµ ÂàùÂßãÂåñ AudioLDM2 DDCM...")
        print(f"   üì± ËÆæÂ§á: {self.device}")
        print(f"   üìö Á†ÅÊú¨Â§ßÂ∞è: {codebook_size}")
        print(f"   üóúÔ∏è ÂéãÁº©Á∫ßÂà´: {compression_level}")
        
        # Âä†ËΩΩÂü∫Á°Ä AudioLDM2 pipeline
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
          # Ëé∑Âèñ VAE latent Á©∫Èó¥Áª¥Â∫¶
        # AudioLDM2 VAE latent shape: [batch, 8, 250, 16] for audio segments
        self.latent_channels = self.pipeline.vae.config.latent_channels  # 8
        
        # Ê†πÊçÆÂéãÁº©Á∫ßÂà´Ë∞ÉÊï¥Á†ÅÊú¨ÈÖçÁΩÆ
        compression_configs = {
            "low": {"codebook_size": codebook_size * 4, "selection_strategy": "nearest"},
            "medium": {"codebook_size": codebook_size, "selection_strategy": "nearest"},  
            "high": {"codebook_size": codebook_size // 2, "selection_strategy": "nearest"},
            "extreme": {"codebook_size": codebook_size // 4, "selection_strategy": "probabilistic"}
        }
        
        config = compression_configs[compression_level]
        self.compression_level = compression_level
        self.selection_strategy = config["selection_strategy"]
        
        # ÂàõÂª∫Á†ÅÊú¨ (‰ΩøÁî®ÂÆûÈôÖÁöÑ latent Áª¥Â∫¶)
        self.codebook = AudioCodebook(
            codebook_size=config["codebook_size"],
            latent_dim=self.latent_channels,
            latent_height=250,  # AudioLDM2 ÂÆûÈôÖÊó∂Èó¥Áª¥Â∫¶
            latent_width=16,    # AudioLDM2 ÂÆûÈôÖÈ¢ëÁéáÁª¥Â∫¶
            init_method="gaussian"  # ÂÖàÁî®È´òÊñØÂàùÂßãÂåñÔºåÂêéÁª≠ÂèØÁî® k-means
        ).to(self.device)
        
        # ÂéãÁº©ÂéÜÂè≤
        self.compression_history = []
        
        print(f"‚úÖ AudioLDM2 DDCM ÂàùÂßãÂåñÂÆåÊàê")
    
    def prepare_codebook_with_dataset(self, audio_files: List[str], max_samples: int = 1000):
        """
        ‰ΩøÁî®Èü≥È¢ëÊï∞ÊçÆÈõÜÂáÜÂ§áÁ†ÅÊú¨
        
        Args:
            audio_files: Èü≥È¢ëÊñá‰ª∂Ë∑ØÂæÑÂàóË°®
            max_samples: ÊúÄÂ§ßÊ†∑Êú¨Êï∞Èáè
        """
        print(f"üìö ‰ΩøÁî®Êï∞ÊçÆÈõÜÂáÜÂ§áÁ†ÅÊú¨ ({len(audio_files)} Êñá‰ª∂, ÊúÄÂ§ßÊ†∑Êú¨: {max_samples})")
        
        sample_latents = []
        processed = 0
        
        for audio_file in audio_files:
            if processed >= max_samples:
                break
                
            try:
                # Âä†ËΩΩÈü≥È¢ë
                audio, sr = torchaudio.load(audio_file)
                
                # È¢ÑÂ§ÑÁêÜ
                if sr != 48000:
                    resampler = torchaudio.transforms.Resample(sr, 48000)
                    audio = resampler(audio)
                
                if audio.shape[0] > 1:
                    audio = audio.mean(dim=0, keepdim=True)
                
                # ÈôêÂà∂ÈïøÂ∫¶
                max_length = 48000 * 10  # 10Áßí
                if audio.shape[-1] > max_length:
                    audio = audio[..., :max_length]
                
                # ÁºñÁ†Å‰∏∫ latent
                latent = self._encode_audio_to_latent(audio.squeeze(0))
                sample_latents.append(latent)
                
                processed += 1
                if processed % 100 == 0:
                    print(f"   Â∑≤Â§ÑÁêÜ: {processed}/{min(len(audio_files), max_samples)}")
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è Ë∑≥ËøáÊñá‰ª∂ {audio_file}: {e}")
                continue
        
        if sample_latents:
            sample_latents = torch.stack(sample_latents)
            print(f"üìä Êî∂ÈõÜÂà∞ {len(sample_latents)} ‰∏™ÊΩúÂú®Ê†∑Êú¨")
            
            # ‰ΩøÁî® k-means ÂàùÂßãÂåñÁ†ÅÊú¨
            self.codebook.initialize_with_kmeans(sample_latents)
        else:
            print("‚ö†Ô∏è Êú™ËÉΩÊî∂ÈõÜÂà∞ÊúâÊïàÊ†∑Êú¨Ôºå‰ΩøÁî®ÈªòËÆ§È´òÊñØÂàùÂßãÂåñ")
    
    def _encode_audio_to_latent(self, audio: torch.Tensor) -> torch.Tensor:
        """ÁºñÁ†ÅÈü≥È¢ë‰∏∫ÊΩúÂú®Ë°®Á§∫"""
        with torch.no_grad():
            # Á°Æ‰øùÊ≠£Á°ÆÊ†ºÂºè
            if audio.is_cuda:
                audio = audio.cpu()
            if audio.dim() == 2:
                audio = audio.squeeze(0)
                
            audio_numpy = audio.numpy()
            
            # ClapFeatureExtractor
            inputs = self.pipeline.feature_extractor(
                audio_numpy,
                sampling_rate=48000,
                return_tensors="pt"            )
            
            mel_features = inputs["input_features"].to(self.device)
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            # Á°Æ‰øùÊï∞ÊçÆÁ±ªÂûãÂåπÈÖç
            if self.device == "cuda":
                mel_features = mel_features.half()
                
            # VAE ÁºñÁ†Å
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode() if hasattr(latent_dist.latent_dist, 'mode') else latent_dist.latent_dist.sample()
            
            return latent.squeeze(0)  # ÁßªÈô§ batch Áª¥Â∫¶
    
    def compress_audio(self, audio_path: str) -> Dict:
        """
        ÂéãÁº©Èü≥È¢ëÂà∞Á†ÅÊú¨Ë°®Á§∫
        
        Args:
            audio_path: ËæìÂÖ•Èü≥È¢ëË∑ØÂæÑ
            
        Returns:
            compression_result: ÂéãÁº©ÁªìÊûúÂ≠óÂÖ∏
        """
        print(f"üóúÔ∏è ÂéãÁº©Èü≥È¢ë: {Path(audio_path).name}")
        
        # Âä†ËΩΩÂíåÈ¢ÑÂ§ÑÁêÜÈü≥È¢ë
        audio, sr = torchaudio.load(audio_path)
        print(f"   üìä ÂéüÂßãÈü≥È¢ë: {audio.shape}, {sr}Hz")
        
        if sr != 48000:
            resampler = torchaudio.transforms.Resample(sr, 48000)
            audio = resampler(audio)
        
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # ÂàÜÊÆµÂ§ÑÁêÜÈïøÈü≥È¢ë
        segment_length = 48000 * 10  # 10ÁßíÂàÜÊÆµ
        audio_squeezed = audio.squeeze(0)
        segments = []
        
        for i in range(0, len(audio_squeezed), segment_length):
            segment = audio_squeezed[i:i+segment_length]
            if len(segment) < segment_length:
                # Â°´ÂÖÖÊúÄÂêé‰∏ÄÊÆµ
                padding = segment_length - len(segment)
                segment = torch.cat([segment, torch.zeros(padding)])
            segments.append(segment)
        
        print(f"   üì¶ ÂàÜ‰∏∫ {len(segments)} ‰∏™ÊÆµËêΩ")
        
        # ÂéãÁº©ÊØè‰∏™ÊÆµËêΩ
        compressed_data = []
        total_original_size = 0
        total_compressed_size = 0
        
        for i, segment in enumerate(segments):
            # ÁºñÁ†Å‰∏∫ latent
            latent = self._encode_audio_to_latent(segment)
            
            # ÊâæÂà∞ÊúÄËøëÁöÑÁ†ÅÊú¨ÂêëÈáè
            indices, quantized, distances = self.codebook.find_nearest_codes(latent.unsqueeze(0))
            
            # ÂéãÁº©‰ø°ÊÅØ
            segment_info = {
                "segment_id": i,
                "codebook_index": indices[0].item(),
                "distance": distances[0].item(),
                "original_latent_shape": latent.shape
            }
            
            compressed_data.append(segment_info)
            
            # ËÆ°ÁÆóÂéãÁº©ÊØî
            original_size = latent.numel() * 4  # float32 Â≠óËäÇÊï∞
            compressed_size = 4  # Âè™Â≠òÂÇ®Á¥¢Âºï (int32)
            
            total_original_size += original_size
            total_compressed_size += compressed_size
        
        compression_ratio = total_original_size / total_compressed_size
        
        result = {
            "input_file": audio_path,
            "segments": compressed_data,
            "compression_ratio": compression_ratio,
            "original_size_bytes": total_original_size,
            "compressed_size_bytes": total_compressed_size,
            "codebook_size": self.codebook.codebook_size,
            "compression_level": self.compression_level
        }
        
        self.compression_history.append(result)
        
        print(f"   ‚úÖ ÂéãÁº©ÂÆåÊàê")
        print(f"   üìä ÂéãÁº©ÊØî: {compression_ratio:.2f}:1")
        print(f"   üìä ÂéüÂßãÂ§ßÂ∞è: {total_original_size} Â≠óËäÇ")
        print(f"   üìä ÂéãÁº©Â§ßÂ∞è: {total_compressed_size} Â≠óËäÇ")
        
        return result
    
    def decompress_and_generate(self, 
                               compressed_data: Dict,
                               prompt: str = "high quality music",
                               num_inference_steps: int = 20,
                               guidance_scale: float = 7.5) -> torch.Tensor:
        """
        Ëß£ÂéãÁº©Âπ∂ÁîüÊàêÈü≥È¢ë
        
        Args:
            compressed_data: ÂéãÁº©Êï∞ÊçÆ
            prompt: ÊñáÊú¨ÊèêÁ§∫
            num_inference_steps: Êé®ÁêÜÊ≠•Êï∞
            guidance_scale: ÂºïÂØºÂº∫Â∫¶
            
        Returns:
            generated_audio: ÁîüÊàêÁöÑÈü≥È¢ë
        """
        print(f"üéµ Ëß£ÂéãÁº©Âπ∂ÁîüÊàêÈü≥È¢ë")
        print(f"   üìù ÊèêÁ§∫: {prompt}")
        print(f"   üì¶ ÊÆµËêΩÊï∞: {len(compressed_data['segments'])}")
        
        generated_segments = []
        
        for segment_info in compressed_data["segments"]:
            # ‰ªéÁ†ÅÊú¨Ëé∑ÂèñÈáèÂåñÊΩúÂú®Ë°®Á§∫
            codebook_index = segment_info["codebook_index"]
            quantized_latent = self.codebook.get_code_by_index(torch.tensor([codebook_index]).to(self.device))
            
            # ‰ΩøÁî®ÈáèÂåñÁöÑ latent ‰Ωú‰∏∫Ëµ∑ÁÇπËøõË°å diffusion
            # ËøôÈáåÊàë‰ª¨ÂèØ‰ª•ÈÄâÊã©Ôºö
            # 1. Áõ¥Êé•Ëß£Á†Å (Âø´ÈÄü‰ΩÜË¥®ÈáèÂèØËÉΩËæÉ‰Ωé)
            # 2. ‰ΩøÁî® diffusion ÈáçÊñ∞ÁîüÊàê (Ë¥®ÈáèÊõ¥Â•Ω‰ΩÜËæÉÊÖ¢)
            
            if self.compression_level in ["low", "medium"]:
                # Áõ¥Êé•Ëß£Á†ÅÊ®°Âºè
                with torch.no_grad():
                    mel = self.pipeline.vae.decode(quantized_latent).sample
                    # Ê≥®ÊÑèÔºöËøôÈáåÈúÄË¶ÅvocoderÂ∞ÜmelËΩ¨‰∏∫Èü≥È¢ëÔºå‰ΩÜAudioLDM2ÂèØËÉΩÊ≤°ÊúâÁõ¥Êé•ÁöÑvocoderËÆøÈóÆ
                    # Êàë‰ª¨‰ΩøÁî®pipelineÁöÑÂÆåÊï¥ÁîüÊàêËøáÁ®ãÔºå‰ΩÜÁî®ÈáèÂåñlatent‰Ωú‰∏∫Êù°‰ª∂
                    pass
            
            # ‰ΩøÁî® diffusion ÁîüÊàê (Êé®ËçêÊñπÂºè)
            # ËøôÈáåÊàë‰ª¨ÁîüÊàêÂõ∫ÂÆöÈïøÂ∫¶ÁöÑÈü≥È¢ëÊÆµ
            segment_audio = self._generate_audio_segment(
                prompt=prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                audio_length_in_s=10.0
            )
            
            generated_segments.append(segment_audio)
        
        # ÊãºÊé•ÊâÄÊúâÊÆµËêΩ
        full_audio = torch.cat(generated_segments, dim=0)
        
        print(f"   ‚úÖ ÁîüÊàêÂÆåÊàê: {full_audio.shape}")
        return full_audio
    
    def _generate_audio_segment(self, 
                              prompt: str,
                              num_inference_steps: int = 20,
                              guidance_scale: float = 7.5,
                              audio_length_in_s: float = 10.0) -> torch.Tensor:
        """ÁîüÊàêÂçï‰∏™Èü≥È¢ëÊÆµ"""
        with torch.no_grad():
            result = self.pipeline(
                prompt=prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                audio_length_in_s=audio_length_in_s
            )
            return torch.tensor(result.audios[0])
    
    def save_compressed_data(self, compressed_data: Dict, save_path: str):
        """‰øùÂ≠òÂéãÁº©Êï∞ÊçÆ"""
        with open(save_path, 'w') as f:
            json.dump(compressed_data, f, indent=2)
        print(f"üíæ ÂéãÁº©Êï∞ÊçÆ‰øùÂ≠òÂà∞: {save_path}")
    
    def load_compressed_data(self, load_path: str) -> Dict:
        """Âä†ËΩΩÂéãÁº©Êï∞ÊçÆ"""
        with open(load_path, 'r') as f:
            compressed_data = json.load(f)
        print(f"üìÇ ‰ªé {load_path} Âä†ËΩΩÂéãÁº©Êï∞ÊçÆ")
        return compressed_data
    
    def save_codebook(self, save_path: str):
        """‰øùÂ≠òÁ†ÅÊú¨"""
        codebook_data = {
            "codebook": self.codebook.state_dict(),
            "usage_count": self.codebook.usage_count,
            "config": {
                "codebook_size": self.codebook.codebook_size,
                "latent_shape": self.codebook.latent_shape,
                "compression_level": self.compression_level
            }
        }
        torch.save(codebook_data, save_path)
        print(f"üíæ Á†ÅÊú¨‰øùÂ≠òÂà∞: {save_path}")
    
    def load_codebook(self, load_path: str):
        """Âä†ËΩΩÁ†ÅÊú¨"""
        codebook_data = torch.load(load_path, map_location=self.device)
        self.codebook.load_state_dict(codebook_data["codebook"])
        self.codebook.usage_count = codebook_data["usage_count"]
        print(f"üìÇ ‰ªé {load_path} Âä†ËΩΩÁ†ÅÊú¨")
        
        # ÊòæÁ§∫‰ΩøÁî®ÁªüËÆ°
        stats = self.codebook.get_usage_stats()
        print(f"   üìä Á†ÅÊú¨ÁªüËÆ°: {stats}")

def demo_ddcm_workflow():
    """DDCM Â∑•‰ΩúÊµÅÁ®ãÊºîÁ§∫"""
    print("üéØ AudioLDM2 DDCM Â∑•‰ΩúÊµÅÁ®ãÊºîÁ§∫")
    print("=" * 50)
    
    # 1. ÂàùÂßãÂåñ DDCM
    ddcm = AudioLDM2_DDCM(
        model_name="cvssp/audioldm2-music",
        codebook_size=512,
        compression_level="medium"
    )
    
    # 2. Ê£ÄÊü•ËæìÂÖ•Êñá‰ª∂
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"‚ùå Êâæ‰∏çÂà∞ËæìÂÖ•Êñá‰ª∂: {input_file}")
        print("ËØ∑Á°Æ‰øùÊúâÊµãËØïÈü≥È¢ëÊñá‰ª∂")
        return
    
    # 3. ÂéãÁº©Èü≥È¢ë
    print("\nüóúÔ∏è Á¨¨1Ê≠•: ÂéãÁº©Èü≥È¢ë")
    compressed_data = ddcm.compress_audio(input_file)
    
    # 4. ‰øùÂ≠òÂéãÁº©Êï∞ÊçÆ
    compressed_file = "compressed_audio.json"
    ddcm.save_compressed_data(compressed_data, compressed_file)
    
    # 5. ‰øùÂ≠òÁ†ÅÊú¨
    codebook_file = "audio_codebook.pth"
    ddcm.save_codebook(codebook_file)
    
    # 6. Ëß£ÂéãÁº©Âπ∂ÁîüÊàê
    print("\nüéµ Á¨¨2Ê≠•: Ëß£ÂéãÁº©Âπ∂ÁîüÊàêÈü≥È¢ë")
    generated_audio = ddcm.decompress_and_generate(
        compressed_data,
        prompt="high quality instrumental music with rich harmonics",
        num_inference_steps=25,
        guidance_scale=7.5
    )
    
    # 7. ‰øùÂ≠òÁîüÊàêÁöÑÈü≥È¢ë
    output_dir = Path("ddcm_output")
    output_dir.mkdir(exist_ok=True)
    
    output_file = output_dir / "ddcm_generated.wav"
    if generated_audio.dim() == 1:
        generated_audio = generated_audio.unsqueeze(0)
    
    torchaudio.save(str(output_file), generated_audio.cpu(), 16000)
    print(f"üíæ ÁîüÊàêÈü≥È¢ë‰øùÂ≠òÂà∞: {output_file}")
    
    # 8. ÊòæÁ§∫ÁªüËÆ°‰ø°ÊÅØ
    print("\nüìä DDCM ÁªüËÆ°‰ø°ÊÅØ")
    print("-" * 30)
    print(f"ÂéãÁº©ÊØî: {compressed_data['compression_ratio']:.2f}:1")
    print(f"Á†ÅÊú¨‰ΩøÁî®Áéá: {ddcm.codebook.get_usage_stats()['usage_rate']*100:.1f}%")
    print(f"ÂéãÁº©Á∫ßÂà´: {ddcm.compression_level}")
    
    print("\n‚úÖ DDCM Â∑•‰ΩúÊµÅÁ®ãÊºîÁ§∫ÂÆåÊàêÔºÅ")

if __name__ == "__main__":
    demo_ddcm_workflow()
