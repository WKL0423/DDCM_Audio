"""
AudioLDM2 DDCM (Denoising Diffusion Codebook Model) å®ç°
åŸºäº DDCM è®ºæ–‡æ€æƒ³ï¼Œä¸º AudioLDM2 åˆ›å»ºç æœ¬åŒ–çš„ diffusion è¿‡ç¨‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from diffusers import AudioLDM2Pipeline
import torchaudio
from pathlib import Path
import json
import pickle
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings("ignore")

class AudioCodebook(nn.Module):
    """
    éŸ³é¢‘æ‰©æ•£ç æœ¬
    ä¸º AudioLDM2 çš„ latent space è®¾è®¡çš„ç æœ¬ç»“æ„
    """
    
    def __init__(self, 
                 codebook_size: int = 1024,
                 latent_dim: int = 8,
                 latent_height: int = 250, 
                 latent_width: int = 16,
                 init_method: str = "gaussian"):
        """
        åˆå§‹åŒ–éŸ³é¢‘ç æœ¬
        
        Args:
            codebook_size: ç æœ¬å¤§å°
            latent_dim: æ½œåœ¨ç©ºé—´ç»´åº¦ (AudioLDM2 VAE latent channels)
            latent_height: æ½œåœ¨ç©ºé—´é«˜åº¦ (æ—¶é—´ç»´åº¦)
            latent_width: æ½œåœ¨ç©ºé—´å®½åº¦ (é¢‘ç‡ç»´åº¦)
            init_method: åˆå§‹åŒ–æ–¹æ³• ("gaussian", "uniform", "kmeans")
        """
        super().__init__()
        
        self.codebook_size = codebook_size
        self.latent_dim = latent_dim
        self.latent_height = latent_height
        self.latent_width = latent_width
        self.latent_shape = (latent_dim, latent_height, latent_width)
        
        # ç æœ¬å‘é‡ï¼š[codebook_size, latent_dim, latent_height, latent_width]
        if init_method == "gaussian":
            self.codebook = nn.Parameter(
                torch.randn(codebook_size, latent_dim, latent_height, latent_width)
            )
        elif init_method == "uniform":
            self.codebook = nn.Parameter(
                torch.rand(codebook_size, latent_dim, latent_height, latent_width) * 2 - 1
            )
        else:
            # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œç”¨äº k-means
            self.register_parameter('codebook', None)
        
        self.usage_count = torch.zeros(codebook_size)
        
    def initialize_with_kmeans(self, sample_latents: torch.Tensor, n_samples: int = 10000):
        """
        ä½¿ç”¨ k-means åˆå§‹åŒ–ç æœ¬
        
        Args:
            sample_latents: æ ·æœ¬æ½œåœ¨è¡¨ç¤º [n_samples, latent_dim, latent_height, latent_width]
            n_samples: ç”¨äºèšç±»çš„æ ·æœ¬æ•°é‡
        """
        print(f"ğŸ”§ ä½¿ç”¨ K-means åˆå§‹åŒ–ç æœ¬ (æ ·æœ¬æ•°: {min(n_samples, len(sample_latents))})")
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        if len(sample_latents) > n_samples:
            indices = torch.randperm(len(sample_latents))[:n_samples]
            sample_latents = sample_latents[indices]
        
        # å±•å¹³æ ·æœ¬
        flattened = sample_latents.view(len(sample_latents), -1).cpu().numpy()
        
        # K-means èšç±»
        kmeans = KMeans(n_clusters=self.codebook_size, random_state=42, n_init=10)
        kmeans.fit(flattened)
        
        # é‡å¡‘ä¸ºç æœ¬å½¢çŠ¶
        centroids = torch.tensor(kmeans.cluster_centers_).float()
        centroids = centroids.view(self.codebook_size, *self.latent_shape)        
        self.codebook = nn.Parameter(centroids)
        print(f"âœ… K-means ç æœ¬åˆå§‹åŒ–å®Œæˆ")
    
    def find_nearest_codes(self, latents: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ä¸ºç»™å®šçš„æ½œåœ¨è¡¨ç¤ºæ‰¾åˆ°æœ€è¿‘çš„ç æœ¬å‘é‡
        
        Args:
            latents: æ½œåœ¨è¡¨ç¤º [batch_size, latent_dim, latent_height, latent_width]
            
        Returns:
            indices: ç æœ¬ç´¢å¼• [batch_size]
            quantized: é‡åŒ–åçš„æ½œåœ¨è¡¨ç¤º [batch_size, latent_dim, latent_height, latent_width]
            distances: è·ç¦» [batch_size]
        """
        batch_size = latents.shape[0]
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        latents = latents.float()
        codebook_float = self.codebook.float()
        
        # å±•å¹³ç”¨äºè®¡ç®—è·ç¦»
        latents_flat = latents.view(batch_size, -1)  # [batch, latent_dim*height*width]
        codebook_flat = codebook_float.view(self.codebook_size, -1)  # [codebook_size, latent_dim*height*width]
        
        # è®¡ç®— L2 è·ç¦»
        distances = torch.cdist(latents_flat, codebook_flat, p=2)  # [batch, codebook_size]
        
        # æ‰¾åˆ°æœ€è¿‘çš„ç æœ¬å‘é‡
        min_distances, indices = torch.min(distances, dim=1)  # [batch]
        
        # è·å–é‡åŒ–åçš„å‘é‡
        quantized = codebook_float[indices]  # [batch, latent_dim, latent_height, latent_width]
        
        # æ›´æ–°ä½¿ç”¨è®¡æ•°
        with torch.no_grad():
            for idx in indices:
                self.usage_count[idx] += 1
        
        return indices, quantized, min_distances
    
    def get_code_by_index(self, indices: torch.Tensor) -> torch.Tensor:
        """
        æ ¹æ®ç´¢å¼•è·å–ç æœ¬å‘é‡
        
        Args:
            indices: ç æœ¬ç´¢å¼• [batch_size]
            
        Returns:
            codes: ç æœ¬å‘é‡ [batch_size, latent_dim, latent_height, latent_width]
        """
        return self.codebook[indices]
    
    def get_usage_stats(self) -> Dict:
        """è·å–ç æœ¬ä½¿ç”¨ç»Ÿè®¡"""
        used_codes = (self.usage_count > 0).sum().item()
        total_usage = self.usage_count.sum().item()
        
        return {
            "used_codes": used_codes,
            "total_codes": self.codebook_size,
            "usage_rate": used_codes / self.codebook_size,
            "total_usage": total_usage,
            "avg_usage": total_usage / max(used_codes, 1)
        }

class AudioLDM2_DDCM(nn.Module):
    """
    AudioLDM2 DDCM ä¸»ç±»
    é›†æˆç æœ¬åŒ–æ‰©æ•£æ¨¡å‹åˆ° AudioLDM2 pipeline
    """
    
    def __init__(self, 
                 model_name: str = "cvssp/audioldm2-music",
                 codebook_size: int = 1024,
                 compression_level: str = "medium"):
        """
        åˆå§‹åŒ– AudioLDM2 DDCM
        
        Args:
            model_name: AudioLDM2 æ¨¡å‹åç§°
            codebook_size: ç æœ¬å¤§å°
            compression_level: å‹ç¼©çº§åˆ« ("low", "medium", "high", "extreme")
        """
        super().__init__()
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸµ åˆå§‹åŒ– AudioLDM2 DDCM...")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {codebook_size}")
        print(f"   ğŸ—œï¸ å‹ç¼©çº§åˆ«: {compression_level}")
        
        # åŠ è½½åŸºç¡€ AudioLDM2 pipeline
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
          # è·å– VAE latent ç©ºé—´ç»´åº¦
        # AudioLDM2 VAE latent shape: [batch, 8, 250, 16] for audio segments
        self.latent_channels = self.pipeline.vae.config.latent_channels  # 8
        
        # æ ¹æ®å‹ç¼©çº§åˆ«è°ƒæ•´ç æœ¬é…ç½®
        compression_configs = {
            "low": {"codebook_size": codebook_size * 4, "selection_strategy": "nearest"},
            "medium": {"codebook_size": codebook_size, "selection_strategy": "nearest"},  
            "high": {"codebook_size": codebook_size // 2, "selection_strategy": "nearest"},
            "extreme": {"codebook_size": codebook_size // 4, "selection_strategy": "probabilistic"}
        }
        
        config = compression_configs[compression_level]
        self.compression_level = compression_level
        self.selection_strategy = config["selection_strategy"]
        
        # åˆ›å»ºç æœ¬ (ä½¿ç”¨å®é™…çš„ latent ç»´åº¦)
        self.codebook = AudioCodebook(
            codebook_size=config["codebook_size"],
            latent_dim=self.latent_channels,
            latent_height=250,  # AudioLDM2 å®é™…æ—¶é—´ç»´åº¦
            latent_width=16,    # AudioLDM2 å®é™…é¢‘ç‡ç»´åº¦
            init_method="gaussian"  # å…ˆç”¨é«˜æ–¯åˆå§‹åŒ–ï¼Œåç»­å¯ç”¨ k-means
        ).to(self.device)
        
        # å‹ç¼©å†å²
        self.compression_history = []
        
        print(f"âœ… AudioLDM2 DDCM åˆå§‹åŒ–å®Œæˆ")
    
    def prepare_codebook_with_dataset(self, audio_files: List[str], max_samples: int = 1000):
        """
        ä½¿ç”¨éŸ³é¢‘æ•°æ®é›†å‡†å¤‡ç æœ¬
        
        Args:
            audio_files: éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡
        """
        print(f"ğŸ“š ä½¿ç”¨æ•°æ®é›†å‡†å¤‡ç æœ¬ ({len(audio_files)} æ–‡ä»¶, æœ€å¤§æ ·æœ¬: {max_samples})")
        
        sample_latents = []
        processed = 0
        
        for audio_file in audio_files:
            if processed >= max_samples:
                break
                
            try:
                # åŠ è½½éŸ³é¢‘
                audio, sr = torchaudio.load(audio_file)
                
                # é¢„å¤„ç†
                if sr != 48000:
                    resampler = torchaudio.transforms.Resample(sr, 48000)
                    audio = resampler(audio)
                
                if audio.shape[0] > 1:
                    audio = audio.mean(dim=0, keepdim=True)
                
                # é™åˆ¶é•¿åº¦
                max_length = 48000 * 10  # 10ç§’
                if audio.shape[-1] > max_length:
                    audio = audio[..., :max_length]
                
                # ç¼–ç ä¸º latent
                latent = self._encode_audio_to_latent(audio.squeeze(0))
                sample_latents.append(latent)
                
                processed += 1
                if processed % 100 == 0:
                    print(f"   å·²å¤„ç†: {processed}/{min(len(audio_files), max_samples)}")
                    
            except Exception as e:
                print(f"   âš ï¸ è·³è¿‡æ–‡ä»¶ {audio_file}: {e}")
                continue
        
        if sample_latents:
            sample_latents = torch.stack(sample_latents)
            print(f"ğŸ“Š æ”¶é›†åˆ° {len(sample_latents)} ä¸ªæ½œåœ¨æ ·æœ¬")
            
            # ä½¿ç”¨ k-means åˆå§‹åŒ–ç æœ¬
            self.codebook.initialize_with_kmeans(sample_latents)
        else:
            print("âš ï¸ æœªèƒ½æ”¶é›†åˆ°æœ‰æ•ˆæ ·æœ¬ï¼Œä½¿ç”¨é»˜è®¤é«˜æ–¯åˆå§‹åŒ–")
    
    def _encode_audio_to_latent(self, audio: torch.Tensor) -> torch.Tensor:
        """ç¼–ç éŸ³é¢‘ä¸ºæ½œåœ¨è¡¨ç¤º"""
        with torch.no_grad():
            # ç¡®ä¿æ­£ç¡®æ ¼å¼
            if audio.is_cuda:
                audio = audio.cpu()
            if audio.dim() == 2:
                audio = audio.squeeze(0)
                
            audio_numpy = audio.numpy()
            
            # ClapFeatureExtractor
            inputs = self.pipeline.feature_extractor(
                audio_numpy,
                sampling_rate=48000,
                return_tensors="pt"            )
            
            mel_features = inputs["input_features"].to(self.device)
            if mel_features.dim() == 3:
                mel_features = mel_features.unsqueeze(1)
            
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            if self.device == "cuda":
                mel_features = mel_features.half()
                
            # VAE ç¼–ç 
            latent_dist = self.pipeline.vae.encode(mel_features)
            latent = latent_dist.latent_dist.mode() if hasattr(latent_dist.latent_dist, 'mode') else latent_dist.latent_dist.sample()
            
            return latent.squeeze(0)  # ç§»é™¤ batch ç»´åº¦
    
    def compress_audio(self, audio_path: str) -> Dict:
        """
        å‹ç¼©éŸ³é¢‘åˆ°ç æœ¬è¡¨ç¤º
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            
        Returns:
            compression_result: å‹ç¼©ç»“æœå­—å…¸
        """
        print(f"ğŸ—œï¸ å‹ç¼©éŸ³é¢‘: {Path(audio_path).name}")
        
        # åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘
        audio, sr = torchaudio.load(audio_path)
        print(f"   ğŸ“Š åŸå§‹éŸ³é¢‘: {audio.shape}, {sr}Hz")
        
        if sr != 48000:
            resampler = torchaudio.transforms.Resample(sr, 48000)
            audio = resampler(audio)
        
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # åˆ†æ®µå¤„ç†é•¿éŸ³é¢‘
        segment_length = 48000 * 10  # 10ç§’åˆ†æ®µ
        audio_squeezed = audio.squeeze(0)
        segments = []
        
        for i in range(0, len(audio_squeezed), segment_length):
            segment = audio_squeezed[i:i+segment_length]
            if len(segment) < segment_length:
                # å¡«å……æœ€åä¸€æ®µ
                padding = segment_length - len(segment)
                segment = torch.cat([segment, torch.zeros(padding)])
            segments.append(segment)
        
        print(f"   ğŸ“¦ åˆ†ä¸º {len(segments)} ä¸ªæ®µè½")
        
        # å‹ç¼©æ¯ä¸ªæ®µè½
        compressed_data = []
        total_original_size = 0
        total_compressed_size = 0
        
        for i, segment in enumerate(segments):
            # ç¼–ç ä¸º latent
            latent = self._encode_audio_to_latent(segment)
            
            # æ‰¾åˆ°æœ€è¿‘çš„ç æœ¬å‘é‡
            indices, quantized, distances = self.codebook.find_nearest_codes(latent.unsqueeze(0))
            
            # å‹ç¼©ä¿¡æ¯
            segment_info = {
                "segment_id": i,
                "codebook_index": indices[0].item(),
                "distance": distances[0].item(),
                "original_latent_shape": latent.shape
            }
            
            compressed_data.append(segment_info)
            
            # è®¡ç®—å‹ç¼©æ¯”
            original_size = latent.numel() * 4  # float32 å­—èŠ‚æ•°
            compressed_size = 4  # åªå­˜å‚¨ç´¢å¼• (int32)
            
            total_original_size += original_size
            total_compressed_size += compressed_size
        
        compression_ratio = total_original_size / total_compressed_size
        
        result = {
            "input_file": audio_path,
            "segments": compressed_data,
            "compression_ratio": compression_ratio,
            "original_size_bytes": total_original_size,
            "compressed_size_bytes": total_compressed_size,
            "codebook_size": self.codebook.codebook_size,
            "compression_level": self.compression_level
        }
        
        self.compression_history.append(result)
        
        print(f"   âœ… å‹ç¼©å®Œæˆ")
        print(f"   ğŸ“Š å‹ç¼©æ¯”: {compression_ratio:.2f}:1")
        print(f"   ğŸ“Š åŸå§‹å¤§å°: {total_original_size} å­—èŠ‚")
        print(f"   ğŸ“Š å‹ç¼©å¤§å°: {total_compressed_size} å­—èŠ‚")
        
        return result
    
    def decompress_and_generate(self, 
                               compressed_data: Dict,
                               prompt: str = "high quality music",
                               num_inference_steps: int = 20,
                               guidance_scale: float = 7.5) -> torch.Tensor:
        """
        è§£å‹ç¼©å¹¶ç”ŸæˆéŸ³é¢‘
        
        Args:
            compressed_data: å‹ç¼©æ•°æ®
            prompt: æ–‡æœ¬æç¤º
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            
        Returns:
            generated_audio: ç”Ÿæˆçš„éŸ³é¢‘
        """
        print(f"ğŸµ è§£å‹ç¼©å¹¶ç”ŸæˆéŸ³é¢‘")
        print(f"   ğŸ“ æç¤º: {prompt}")
        print(f"   ğŸ“¦ æ®µè½æ•°: {len(compressed_data['segments'])}")
        
        generated_segments = []
        
        for segment_info in compressed_data["segments"]:
            # ä»ç æœ¬è·å–é‡åŒ–æ½œåœ¨è¡¨ç¤º
            codebook_index = segment_info["codebook_index"]
            quantized_latent = self.codebook.get_code_by_index(torch.tensor([codebook_index]).to(self.device))
            
            # ä½¿ç”¨é‡åŒ–çš„ latent ä½œä¸ºèµ·ç‚¹è¿›è¡Œ diffusion
            # è¿™é‡Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ï¼š
            # 1. ç›´æ¥è§£ç  (å¿«é€Ÿä½†è´¨é‡å¯èƒ½è¾ƒä½)
            # 2. ä½¿ç”¨ diffusion é‡æ–°ç”Ÿæˆ (è´¨é‡æ›´å¥½ä½†è¾ƒæ…¢)
            
            if self.compression_level in ["low", "medium"]:
                # ç›´æ¥è§£ç æ¨¡å¼
                with torch.no_grad():
                    mel = self.pipeline.vae.decode(quantized_latent).sample
                    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦vocoderå°†melè½¬ä¸ºéŸ³é¢‘ï¼Œä½†AudioLDM2å¯èƒ½æ²¡æœ‰ç›´æ¥çš„vocoderè®¿é—®
                    # æˆ‘ä»¬ä½¿ç”¨pipelineçš„å®Œæ•´ç”Ÿæˆè¿‡ç¨‹ï¼Œä½†ç”¨é‡åŒ–latentä½œä¸ºæ¡ä»¶
                    pass
            
            # ä½¿ç”¨ diffusion ç”Ÿæˆ (æ¨èæ–¹å¼)
            # è¿™é‡Œæˆ‘ä»¬ç”Ÿæˆå›ºå®šé•¿åº¦çš„éŸ³é¢‘æ®µ
            segment_audio = self._generate_audio_segment(
                prompt=prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                audio_length_in_s=10.0
            )
            
            generated_segments.append(segment_audio)
        
        # æ‹¼æ¥æ‰€æœ‰æ®µè½
        full_audio = torch.cat(generated_segments, dim=0)
        
        print(f"   âœ… ç”Ÿæˆå®Œæˆ: {full_audio.shape}")
        return full_audio
    
    def _generate_audio_segment(self, 
                              prompt: str,
                              num_inference_steps: int = 20,
                              guidance_scale: float = 7.5,
                              audio_length_in_s: float = 10.0) -> torch.Tensor:
        """ç”Ÿæˆå•ä¸ªéŸ³é¢‘æ®µ"""
        with torch.no_grad():
            result = self.pipeline(
                prompt=prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                audio_length_in_s=audio_length_in_s
            )
            return torch.tensor(result.audios[0])
    
    def save_compressed_data(self, compressed_data: Dict, save_path: str):
        """ä¿å­˜å‹ç¼©æ•°æ®"""
        with open(save_path, 'w') as f:
            json.dump(compressed_data, f, indent=2)
        print(f"ğŸ’¾ å‹ç¼©æ•°æ®ä¿å­˜åˆ°: {save_path}")
    
    def load_compressed_data(self, load_path: str) -> Dict:
        """åŠ è½½å‹ç¼©æ•°æ®"""
        with open(load_path, 'r') as f:
            compressed_data = json.load(f)
        print(f"ğŸ“‚ ä» {load_path} åŠ è½½å‹ç¼©æ•°æ®")
        return compressed_data
    
    def save_codebook(self, save_path: str):
        """ä¿å­˜ç æœ¬"""
        codebook_data = {
            "codebook": self.codebook.state_dict(),
            "usage_count": self.codebook.usage_count,
            "config": {
                "codebook_size": self.codebook.codebook_size,
                "latent_shape": self.codebook.latent_shape,
                "compression_level": self.compression_level
            }
        }
        torch.save(codebook_data, save_path)
        print(f"ğŸ’¾ ç æœ¬ä¿å­˜åˆ°: {save_path}")
    
    def load_codebook(self, load_path: str):
        """åŠ è½½ç æœ¬"""
        codebook_data = torch.load(load_path, map_location=self.device)
        self.codebook.load_state_dict(codebook_data["codebook"])
        self.codebook.usage_count = codebook_data["usage_count"]
        print(f"ğŸ“‚ ä» {load_path} åŠ è½½ç æœ¬")
        
        # æ˜¾ç¤ºä½¿ç”¨ç»Ÿè®¡
        stats = self.codebook.get_usage_stats()
        print(f"   ğŸ“Š ç æœ¬ç»Ÿè®¡: {stats}")

def demo_ddcm_workflow():
    """DDCM å·¥ä½œæµç¨‹æ¼”ç¤º"""
    print("ğŸ¯ AudioLDM2 DDCM å·¥ä½œæµç¨‹æ¼”ç¤º")
    print("=" * 50)
    
    # 1. åˆå§‹åŒ– DDCM
    ddcm = AudioLDM2_DDCM(
        model_name="cvssp/audioldm2-music",
        codebook_size=512,
        compression_level="medium"
    )
    
    # 2. æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "AudioLDM2_Music_output.wav"
    if not Path(input_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        print("è¯·ç¡®ä¿æœ‰æµ‹è¯•éŸ³é¢‘æ–‡ä»¶")
        return
    
    # 3. å‹ç¼©éŸ³é¢‘
    print("\nğŸ—œï¸ ç¬¬1æ­¥: å‹ç¼©éŸ³é¢‘")
    compressed_data = ddcm.compress_audio(input_file)
    
    # 4. ä¿å­˜å‹ç¼©æ•°æ®
    compressed_file = "compressed_audio.json"
    ddcm.save_compressed_data(compressed_data, compressed_file)
    
    # 5. ä¿å­˜ç æœ¬
    codebook_file = "audio_codebook.pth"
    ddcm.save_codebook(codebook_file)
    
    # 6. è§£å‹ç¼©å¹¶ç”Ÿæˆ
    print("\nğŸµ ç¬¬2æ­¥: è§£å‹ç¼©å¹¶ç”ŸæˆéŸ³é¢‘")
    generated_audio = ddcm.decompress_and_generate(
        compressed_data,
        prompt="high quality instrumental music with rich harmonics",
        num_inference_steps=25,
        guidance_scale=7.5
    )
    
    # 7. ä¿å­˜ç”Ÿæˆçš„éŸ³é¢‘
    output_dir = Path("ddcm_output")
    output_dir.mkdir(exist_ok=True)
    
    output_file = output_dir / "ddcm_generated.wav"
    if generated_audio.dim() == 1:
        generated_audio = generated_audio.unsqueeze(0)
    
    torchaudio.save(str(output_file), generated_audio.cpu(), 16000)
    print(f"ğŸ’¾ ç”ŸæˆéŸ³é¢‘ä¿å­˜åˆ°: {output_file}")
    
    # 8. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š DDCM ç»Ÿè®¡ä¿¡æ¯")
    print("-" * 30)
    print(f"å‹ç¼©æ¯”: {compressed_data['compression_ratio']:.2f}:1")
    print(f"ç æœ¬ä½¿ç”¨ç‡: {ddcm.codebook.get_usage_stats()['usage_rate']*100:.1f}%")
    print(f"å‹ç¼©çº§åˆ«: {ddcm.compression_level}")
    
    print("\nâœ… DDCM å·¥ä½œæµç¨‹æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    demo_ddcm_workflow()
