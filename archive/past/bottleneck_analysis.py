"""
AudioLDM2 VAEé‡å»ºè´¨é‡ç“¶é¢ˆåˆ†æä¸æ”¹è¿›æ–¹æ¡ˆ

å½“å‰é—®é¢˜ï¼š
- SNRåªæœ‰çº¦-0.01dBï¼Œè™½ç„¶æŠ€æœ¯ä¸Šå¯è¡Œï¼Œä½†è´¨é‡ä¸€èˆ¬
- ç›¸å…³ç³»æ•°å¾ˆä½(çº¦0.006)ï¼Œè¯´æ˜æ—¶åŸŸç›¸ä¼¼æ€§ä¸é«˜
- è™½ç„¶èƒ½å¬å‡ºè”ç³»ï¼Œä½†æ˜æ˜¾æ„ŸçŸ¥è´¨é‡ä¸ä½³

ç“¶é¢ˆåˆ†æï¼š
1. ğŸ¯ VAEä¸æ˜¯ä¸ºé‡å»ºä¼˜åŒ–çš„ - æœ€å¤§ç“¶é¢ˆ
2. ğŸ”Š Mel-spectrogramä¿¡æ¯æŸå¤±ä¸¥é‡
3. ğŸµ Griffin-Limç›¸ä½é‡å»ºé—®é¢˜
4. ğŸ“Š å½’ä¸€åŒ–å’Œå‚æ•°é…ç½®ä¸å½“
5. ğŸ§  ç¼ºå°‘æ„ŸçŸ¥æŸå¤±å‡½æ•°

æœ¬è„šæœ¬å®ç°é’ˆå¯¹æ€§çš„æ”¹è¿›æªæ–½ã€‚
"""

import torch
import librosa
import numpy as np
import os
import time
from pathlib import Path
import torchaudio
import torch.nn.functional as F
from scipy.signal import butter, filtfilt

from diffusers import AudioLDM2Pipeline


def analyze_bottlenecks(audio_path, model_id="cvssp/audioldm2-music"):
    """
    æ·±åº¦åˆ†æVAEé‡å»ºçš„ç“¶é¢ˆé—®é¢˜
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ” å¼€å§‹ç“¶é¢ˆåˆ†æ: {audio_path}")
    
    # åŠ è½½æ¨¡å‹
    pipeline = AudioLDM2Pipeline.from_pretrained(model_id, torch_dtype=torch.float32).to(device)
    vae = pipeline.vae
    sample_rate = 16000
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    if len(audio) > 5 * sample_rate:
        audio = audio[:5 * sample_rate]
    
    print(f"\\nğŸ“Š åŸå§‹éŸ³é¢‘åˆ†æ:")
    print(f"   é•¿åº¦: {len(audio)/sample_rate:.2f}ç§’")
    print(f"   åŠ¨æ€èŒƒå›´: {audio.max():.3f} åˆ° {audio.min():.3f}")
    print(f"   RMSåŠŸç‡: {np.sqrt(np.mean(audio**2)):.6f}")
    
    # åˆ†æ1: Mel-spectrogramä¿¡æ¯æŸå¤±
    print(f"\\nğŸ”¬ ç“¶é¢ˆåˆ†æ1: Mel-spectrogramä¿¡æ¯æŸå¤±")
    
    # åŸå§‹é¢‘è°±
    stft_original = librosa.stft(audio, n_fft=1024, hop_length=160)
    magnitude_original = np.abs(stft_original)
    
    # Melå˜æ¢
    mel_spec = librosa.feature.melspectrogram(
        y=audio, sr=sample_rate, n_mels=64, n_fft=1024, hop_length=160
    )
    
    # å°è¯•ä»melé‡å»ºçº¿æ€§é¢‘è°±
    mel_to_linear_matrix = librosa.filters.mel(sr=sample_rate, n_fft=1024, n_mels=64)
    reconstructed_magnitude = np.dot(mel_to_linear_matrix.T, mel_spec)
    
    # è®¡ç®—é¢‘è°±åŸŸæŸå¤±
    spectral_loss = np.mean((magnitude_original - reconstructed_magnitude) ** 2)
    spectral_correlation = np.corrcoef(magnitude_original.flatten(), reconstructed_magnitude.flatten())[0,1]
    
    print(f"   é¢‘è°±åŸŸMSEæŸå¤±: {spectral_loss:.6f}")
    print(f"   é¢‘è°±åŸŸç›¸å…³æ€§: {spectral_correlation:.4f}")
    print(f"   âŒ ç“¶é¢ˆ: Melå˜æ¢æŸå¤±äº† {(1-spectral_correlation)*100:.1f}% çš„é¢‘è°±ä¿¡æ¯")
    
    # åˆ†æ2: VAEæ½œåœ¨ç©ºé—´å‹ç¼©æŸå¤±
    print(f"\\nğŸ”¬ ç“¶é¢ˆåˆ†æ2: VAEæ½œåœ¨ç©ºé—´å‹ç¼©æŸå¤±")
    
    # å‡†å¤‡VAEè¾“å…¥
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_normalized = 2.0 * (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min()) - 1.0
    mel_tensor = torch.from_numpy(mel_normalized).unsqueeze(0).unsqueeze(0).float().to(device)
      with torch.no_grad():
        # VAEç¼–ç è§£ç 
        latent = vae.encode(mel_tensor).latent_dist.sample()
        reconstructed_mel_tensor = vae.decode(latent).sample
        
    reconstructed_mel_np = reconstructed_mel_tensor.squeeze().cpu().numpy()
    
    # ç¡®ä¿å½¢çŠ¶åŒ¹é…ï¼ˆå¤„ç†VAEå¯èƒ½çš„è½»å¾®å°ºå¯¸å˜åŒ–ï¼‰
    min_width = min(mel_normalized.shape[1], reconstructed_mel_np.shape[1])
    mel_normalized_aligned = mel_normalized[:, :min_width]
    reconstructed_mel_aligned = reconstructed_mel_np[:, :min_width]
    
    # è®¡ç®—VAEé‡å»ºæŸå¤±
    vae_mse = np.mean((mel_normalized_aligned - reconstructed_mel_aligned) ** 2)
    vae_correlation = np.corrcoef(mel_normalized_aligned.flatten(), reconstructed_mel_aligned.flatten())[0,1]
    
    print(f"   VAEé‡å»ºMSE: {vae_mse:.6f}")
    print(f"   VAEé‡å»ºç›¸å…³æ€§: {vae_correlation:.4f}")
    print(f"   å‹ç¼©æ¯”: {mel_tensor.numel() / latent.numel():.1f}:1")
    print(f"   âŒ ç“¶é¢ˆ: VAEå‹ç¼©æŸå¤±äº† {(1-vae_correlation)*100:.1f}% çš„melä¿¡æ¯")
    
    # åˆ†æ3: Griffin-Limç›¸ä½é‡å»ºé—®é¢˜
    print(f"\\nğŸ”¬ ç“¶é¢ˆåˆ†æ3: Griffin-Limç›¸ä½é‡å»ºé—®é¢˜")
    
    # ä½¿ç”¨åŸå§‹å¹…åº¦è°± + Griffin-Lim
    gl_audio_perfect = librosa.griffinlim(magnitude_original, hop_length=160, n_iter=32)
    
    # ä½¿ç”¨é‡å»ºå¹…åº¦è°± + Griffin-Lim  
    reconstructed_mel_power = librosa.db_to_power((reconstructed_mel_np + 1) / 2 * 80 - 80)
    gl_audio_degraded = librosa.feature.inverse.mel_to_audio(
        reconstructed_mel_power, sr=sample_rate, hop_length=160, n_fft=1024, n_iter=32
    )
    
    # å¯¹æ¯”åˆ†æ
    if len(gl_audio_perfect) > len(audio):
        gl_audio_perfect = gl_audio_perfect[:len(audio)]
    if len(gl_audio_degraded) > len(audio):
        gl_audio_degraded = gl_audio_degraded[:len(audio)]
    
    perfect_correlation = np.corrcoef(audio, gl_audio_perfect)[0,1] if len(audio) > 1 else 0
    degraded_correlation = np.corrcoef(audio, gl_audio_degraded)[0,1] if len(audio) > 1 else 0
    
    print(f"   ç†æƒ³Griffin-Limç›¸å…³æ€§: {perfect_correlation:.4f}")
    print(f"   å®é™…Griffin-Limç›¸å…³æ€§: {degraded_correlation:.4f}")
    print(f"   âŒ ç“¶é¢ˆ: ç›¸ä½é‡å»ºè´¨é‡å·®å¼‚ {(perfect_correlation-degraded_correlation)*100:.1f}%")
    
    # åˆ†æ4: æ•´ä½“ä¿¡æ¯æµæŸå¤±
    print(f"\\nğŸ”¬ ç“¶é¢ˆåˆ†æ4: æ•´ä½“ä¿¡æ¯æµæŸå¤±")
    print(f"   åŸå§‹éŸ³é¢‘ â†’ Melå˜æ¢: ä¿ç•™ {spectral_correlation*100:.1f}%")
    print(f"   Mel â†’ VAEé‡å»º: ä¿ç•™ {vae_correlation*100:.1f}%")
    print(f"   VAE â†’ éŸ³é¢‘é‡å»º: ä¿ç•™ {degraded_correlation*100:.1f}%")
    
    total_retention = spectral_correlation * vae_correlation * abs(degraded_correlation)
    print(f"   ğŸ“‰ æ€»ä½“ä¿¡æ¯ä¿ç•™ç‡: {total_retention*100:.1f}%")
    print(f"   ğŸ“‰ æ€»ä½“ä¿¡æ¯æŸå¤±: {(1-total_retention)*100:.1f}%")
    
    return {
        'spectral_loss': spectral_loss,
        'spectral_correlation': spectral_correlation,
        'vae_mse': vae_mse,
        'vae_correlation': vae_correlation,
        'perfect_gl_correlation': perfect_correlation,
        'degraded_gl_correlation': degraded_correlation,
        'total_retention': total_retention
    }


def improved_reconstruction_v2(audio_path, model_id="cvssp/audioldm2-music"):
    """
    å®æ–½é’ˆå¯¹æ€§æ”¹è¿›çš„é‡å»ºæ–¹æ³•
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\\nğŸš€ å¯åŠ¨æ”¹è¿›ç‰ˆé‡å»ºæ–¹æ¡ˆ")
    
    # åŠ è½½æ¨¡å‹
    pipeline = AudioLDM2Pipeline.from_pretrained(model_id, torch_dtype=torch.float32).to(device)
    vae = pipeline.vae
    vocoder = pipeline.vocoder
    sample_rate = 16000
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    if len(audio) > 5 * sample_rate:
        audio = audio[:5 * sample_rate]
    
    output_dir = "vae_improved_v2_test"
    os.makedirs(output_dir, exist_ok=True)
    input_name = Path(audio_path).stem
    timestamp = int(time.time())
    
    results = []
    
    # æ”¹è¿›æ–¹æ¡ˆ1: é«˜åˆ†è¾¨ç‡Mel + å¤šæ¬¡è¿­ä»£Griffin-Lim
    print(f"\\nğŸ“ˆ æ”¹è¿›æ–¹æ¡ˆ1: é«˜åˆ†è¾¨ç‡Mel + å¢å¼ºGriffin-Lim")
    
    # ä½¿ç”¨æ›´é«˜åˆ†è¾¨ç‡çš„mel
    mel_spec_hires = librosa.feature.melspectrogram(
        y=audio, sr=sample_rate, 
        n_mels=128,  # ä»64å¢åŠ åˆ°128
        n_fft=2048,  # ä»1024å¢åŠ åˆ°2048  
        hop_length=128,  # ä»160å‡å°‘åˆ°128
        fmax=sample_rate//2  # ä½¿ç”¨å…¨é¢‘å¸¦
    )
    
    # æ”¹è¿›çš„å½’ä¸€åŒ–ï¼šä¿ç•™æ›´å¤šåŠ¨æ€èŒƒå›´
    mel_spec_db = librosa.power_to_db(mel_spec_hires, ref=np.max)
    # ä½¿ç”¨æ›´ä¿å®ˆçš„å½’ä¸€åŒ–ï¼Œä¿ç•™åŸå§‹åˆ†å¸ƒç‰¹å¾
    mel_mean = mel_spec_db.mean()
    mel_std = mel_spec_db.std()
    mel_normalized = (mel_spec_db - mel_mean) / (mel_std + 1e-8)
    mel_normalized = np.clip(mel_normalized, -3, 3)  # 3-sigmaå‰ªè£
    
    # è°ƒæ•´å°ºå¯¸ä»¥é€‚åº”VAE (éœ€è¦èƒ½è¢«8æ•´é™¤)
    target_height = 64  # VAEæœŸæœ›çš„mel bins
    target_width = (mel_normalized.shape[1] // 8) * 8
    
    # é™é‡‡æ ·åˆ°VAEæœŸæœ›çš„å°ºå¯¸
    mel_resized = F.interpolate(
        torch.from_numpy(mel_normalized).unsqueeze(0).unsqueeze(0),
        size=(target_height, target_width),
        mode='bilinear',
        align_corners=False
    ).squeeze().numpy()
    
    mel_tensor = torch.from_numpy(mel_resized).unsqueeze(0).unsqueeze(0).float().to(device)
    
    with torch.no_grad():
        latent = vae.encode(mel_tensor).latent_dist.sample()
        reconstructed_mel_tensor = vae.decode(latent).sample
    
    reconstructed_mel = reconstructed_mel_tensor.squeeze().cpu().numpy()
    
    # åå½’ä¸€åŒ–
    reconstructed_mel_denorm = reconstructed_mel * (mel_std + 1e-8) + mel_mean
    
    # ä¸Šé‡‡æ ·å›åŸå§‹åˆ†è¾¨ç‡
    reconstructed_mel_upsampled = F.interpolate(
        torch.from_numpy(reconstructed_mel_denorm).unsqueeze(0).unsqueeze(0),
        size=mel_spec_hires.shape,
        mode='bilinear',
        align_corners=False
    ).squeeze().numpy()
    
    # è½¬æ¢å›åŠŸç‡åŸŸå¹¶é‡å»ºéŸ³é¢‘
    reconstructed_power = librosa.db_to_power(reconstructed_mel_upsampled)
    
    # ä½¿ç”¨æ›´å¤šè¿­ä»£çš„Griffin-Lim
    audio_improved_1 = librosa.feature.inverse.mel_to_audio(
        reconstructed_power,
        sr=sample_rate,
        n_fft=2048,
        hop_length=128,
        n_iter=64,  # å¢åŠ è¿­ä»£æ¬¡æ•°
        fmax=sample_rate//2
    )
    
    # é•¿åº¦å¯¹é½
    if len(audio_improved_1) > len(audio):
        audio_improved_1 = audio_improved_1[:len(audio)]
    elif len(audio_improved_1) < len(audio):
        audio_improved_1 = np.pad(audio_improved_1, (0, len(audio) - len(audio_improved_1)))
    
    # è®¡ç®—æŒ‡æ ‡
    snr_1 = 10 * np.log10(np.mean(audio**2) / (np.mean((audio - audio_improved_1)**2) + 1e-10))
    corr_1 = np.corrcoef(audio, audio_improved_1)[0,1]
    
    # ä¿å­˜ç»“æœ
    path_1 = os.path.join(output_dir, f"{input_name}_hires_griffin_{timestamp}.wav")
    torchaudio.save(path_1, torch.from_numpy(audio_improved_1 / (np.max(np.abs(audio_improved_1)) + 1e-8)).unsqueeze(0), sample_rate)
    
    results.append({
        'method': 'é«˜åˆ†è¾¨ç‡Mel + å¢å¼ºGriffin-Lim',
        'snr': snr_1,
        'correlation': corr_1,
        'path': path_1
    })
    
    print(f"   âœ… SNR: {snr_1:.2f}dB, ç›¸å…³æ€§: {corr_1:.4f}")
    
    # æ”¹è¿›æ–¹æ¡ˆ2: å›ºå®šVocoder + é¢„åå¤„ç†
    print(f"\\nğŸ“ˆ æ”¹è¿›æ–¹æ¡ˆ2: ä¿®æ­£Vocoder + éŸ³é¢‘åå¤„ç†")
    
    try:
        # ä½¿ç”¨ä¿®æ­£çš„Vocoder
        mel_for_vocoder = torch.from_numpy(reconstructed_mel).unsqueeze(0).float().to(device)
        mel_for_vocoder_transposed = mel_for_vocoder.transpose(-2, -1)
        
        with torch.no_grad():
            audio_vocoder_raw = vocoder(mel_for_vocoder_transposed).squeeze().cpu().numpy()
        
        # åå¤„ç†ï¼šé¢‘åŸŸæ»¤æ³¢å’ŒåŠ¨æ€èŒƒå›´è°ƒæ•´
        # è®¾è®¡ä½é€šæ»¤æ³¢å™¨ç§»é™¤é«˜é¢‘å™ªå£°
        nyquist = sample_rate // 2
        cutoff = min(8000, nyquist * 0.8)  # æˆªæ­¢é¢‘ç‡
        b, a = butter(5, cutoff / nyquist, btype='low')
        audio_vocoder_filtered = filtfilt(b, a, audio_vocoder_raw)
        
        # åŠ¨æ€èŒƒå›´å‹ç¼©ï¼ˆè½»å¾®ï¼‰
        audio_compressed = np.sign(audio_vocoder_filtered) * np.power(np.abs(audio_vocoder_filtered), 0.8)
        
        # é•¿åº¦å¯¹é½
        if len(audio_compressed) > len(audio):
            audio_compressed = audio_compressed[:len(audio)]
        elif len(audio_compressed) < len(audio):
            audio_compressed = np.pad(audio_compressed, (0, len(audio) - len(audio_compressed)))
        
        # è®¡ç®—æŒ‡æ ‡
        snr_2 = 10 * np.log10(np.mean(audio**2) / (np.mean((audio - audio_compressed)**2) + 1e-10))
        corr_2 = np.corrcoef(audio, audio_compressed)[0,1]
        
        # ä¿å­˜ç»“æœ
        path_2 = os.path.join(output_dir, f"{input_name}_vocoder_enhanced_{timestamp}.wav")
        torchaudio.save(path_2, torch.from_numpy(audio_compressed / (np.max(np.abs(audio_compressed)) + 1e-8)).unsqueeze(0), sample_rate)
        
        results.append({
            'method': 'ä¿®æ­£Vocoder + éŸ³é¢‘åå¤„ç†',
            'snr': snr_2,
            'correlation': corr_2,
            'path': path_2
        })
        
        print(f"   âœ… SNR: {snr_2:.2f}dB, ç›¸å…³æ€§: {corr_2:.4f}")
        
    except Exception as e:
        print(f"   âŒ Vocoderæ–¹æ¡ˆå¤±è´¥: {e}")
    
    # æ”¹è¿›æ–¹æ¡ˆ3: å¤šå°ºåº¦èåˆé‡å»º
    print(f"\\nğŸ“ˆ æ”¹è¿›æ–¹æ¡ˆ3: å¤šå°ºåº¦èåˆé‡å»º")
    
    # ä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡é‡å»ºå¹¶èåˆ
    scales = [(64, 1024, 160), (80, 1536, 120), (96, 2048, 128)]
    reconstructed_audios = []
    
    for n_mels, n_fft, hop_length in scales:
        # ç”Ÿæˆè¯¥å°ºåº¦çš„mel
        mel_scale = librosa.feature.melspectrogram(
            y=audio, sr=sample_rate, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length
        )
        
        # ç®€åŒ–å¤„ç†ï¼šç›´æ¥é‡å»ºï¼ˆè·³è¿‡VAEä»¥è¯„ä¼°ä¸Šé™ï¼‰
        audio_scale = librosa.feature.inverse.mel_to_audio(
            mel_scale, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_iter=32
        )
        
        # é•¿åº¦å¯¹é½
        if len(audio_scale) > len(audio):
            audio_scale = audio_scale[:len(audio)]
        elif len(audio_scale) < len(audio):
            audio_scale = np.pad(audio_scale, (0, len(audio) - len(audio_scale)))
            
        reconstructed_audios.append(audio_scale)
    
    # åŠ æƒèåˆï¼ˆåŸºäºé¢‘åŸŸèƒ½é‡åˆ†å¸ƒï¼‰
    weights = [0.5, 0.3, 0.2]  # ç»™ä½åˆ†è¾¨ç‡æ›´é«˜æƒé‡ï¼ˆæ›´ç¨³å®šï¼‰
    audio_fused = np.zeros_like(audio)
    for i, (audio_scale, weight) in enumerate(zip(reconstructed_audios, weights)):
        audio_fused += weight * audio_scale
    
    # è®¡ç®—æŒ‡æ ‡
    snr_3 = 10 * np.log10(np.mean(audio**2) / (np.mean((audio - audio_fused)**2) + 1e-10))
    corr_3 = np.corrcoef(audio, audio_fused)[0,1]
    
    # ä¿å­˜ç»“æœ
    path_3 = os.path.join(output_dir, f"{input_name}_multiscale_fusion_{timestamp}.wav")
    torchaudio.save(path_3, torch.from_numpy(audio_fused / (np.max(np.abs(audio_fused)) + 1e-8)).unsqueeze(0), sample_rate)
    
    results.append({
        'method': 'å¤šå°ºåº¦èåˆé‡å»º',
        'snr': snr_3,
        'correlation': corr_3,
        'path': path_3
    })
    
    print(f"   âœ… SNR: {snr_3:.2f}dB, ç›¸å…³æ€§: {corr_3:.4f}")
    
    # ä¿å­˜åŸå§‹éŸ³é¢‘ä½œä¸ºå¯¹æ¯”
    original_path = os.path.join(output_dir, f"{input_name}_original_{timestamp}.wav")
    torchaudio.save(original_path, torch.from_numpy(audio / (np.max(np.abs(audio)) + 1e-8)).unsqueeze(0), sample_rate)
    
    # ç»“æœåˆ†æ
    print(f"\\n{'='*60}")
    print(f"ğŸ¯ æ”¹è¿›æ–¹æ¡ˆæ•ˆæœå¯¹æ¯”")
    print(f"{'='*60}")
    
    results.sort(key=lambda x: x['snr'], reverse=True)
    
    for i, result in enumerate(results, 1):
        print(f"#{i} {result['method']}:")
        print(f"    ğŸ“ˆ SNR: {result['snr']:.2f} dB")
        print(f"    ğŸ”— ç›¸å…³æ€§: {result['correlation']:.4f}")
        print(f"    ğŸ“„ æ–‡ä»¶: {result['path']}")
        print()
    
    best_improvement = results[0]['snr'] - (-0.01)  # ä¸ä¹‹å‰æœ€ä½³ç»“æœå¯¹æ¯”
    print(f"ğŸš€ æœ€ä½³æ”¹è¿›æ•ˆæœ: {best_improvement:+.2f} dB")
    print(f"ğŸ† æ¨èæ–¹æ³•: {results[0]['method']}")
    
    print(f"\\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}/")
    print(f"ğŸ§ å»ºè®®æ’­æ”¾éŸ³é¢‘æ–‡ä»¶è¿›è¡Œä¸»è§‚è¯„ä¼°")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    audio_path = sys.argv[1] if len(sys.argv) > 1 else "AudioLDM2_Music_output.wav"
    
    print(f"ğŸ” AudioLDM2 VAEé‡å»ºç“¶é¢ˆåˆ†æä¸æ”¹è¿›")
    print(f"=" * 60)
    
    # æ­¥éª¤1: ç“¶é¢ˆåˆ†æ
    print(f"\\nğŸ”¬ æ­¥éª¤1: æ·±åº¦ç“¶é¢ˆåˆ†æ")
    bottleneck_analysis = analyze_bottlenecks(audio_path)
    
    # æ­¥éª¤2: é’ˆå¯¹æ€§æ”¹è¿›
    print(f"\\nğŸš€ æ­¥éª¤2: å®æ–½æ”¹è¿›æ–¹æ¡ˆ")
    improvement_results = improved_reconstruction_v2(audio_path)
    
    # æ­¥éª¤3: æ€»ç»“å»ºè®®
    print(f"\\nğŸ’¡ å…³é”®ç“¶é¢ˆæ€»ç»“:")
    print(f"   1ï¸âƒ£ Melå˜æ¢ä¿¡æ¯æŸå¤±: {(1-bottleneck_analysis['spectral_correlation'])*100:.1f}%")
    print(f"   2ï¸âƒ£ VAEå‹ç¼©ä¿¡æ¯æŸå¤±: {(1-bottleneck_analysis['vae_correlation'])*100:.1f}%") 
    print(f"   3ï¸âƒ£ ç›¸ä½é‡å»ºè´¨é‡å·®: Griffin-Limæœ¬èº«é™åˆ¶")
    print(f"   4ï¸âƒ£ æ€»ä½“ä¿¡æ¯ä¿ç•™ç‡: ä»… {bottleneck_analysis['total_retention']*100:.1f}%")
    
    print(f"\\nğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"   ğŸ”§ çŸ­æœŸ: ä¼˜åŒ–melå‚æ•°é…ç½®å’Œåå¤„ç†")
    print(f"   ğŸ§  ä¸­æœŸ: ä½¿ç”¨é¢„è®­ç»ƒçš„é«˜è´¨é‡vocoder")
    print(f"   ğŸš€ é•¿æœŸ: ç«¯åˆ°ç«¯è®­ç»ƒä¸“ç”¨é‡å»ºæ¨¡å‹")


if __name__ == "__main__":
    main()
