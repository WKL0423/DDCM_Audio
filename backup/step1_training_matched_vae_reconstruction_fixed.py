#!/usr/bin/env python3
"""
Step 1: ä¸è®­ç»ƒç®¡é“å®Œå…¨åŒ¹é…çš„VAEé‡å»ºæµ‹è¯•ï¼ˆä¿®æ­£ç‰ˆï¼‰
åŸºäºè®­ç»ƒä»£ç çš„autoencoder.pyå’Œé…ç½®æ–‡ä»¶16k_64.yamlåˆ›å»º
ç¡®ä¿ï¼š
1. ä½¿ç”¨16kHzé‡‡æ ·ç‡ï¼ˆè®­ç»ƒé…ç½®ï¼‰
2. ä½¿ç”¨ä¸è®­ç»ƒå®Œå…¨ä¸€è‡´çš„melé¢‘è°±å›¾æå–æ–¹æ³•
3. ä¸ä½¿ç”¨å­é¢‘å¸¦å¤„ç†ï¼ˆsubband=1ï¼‰
4. æ­£ç¡®çš„æ•°æ®æ ¼å¼ã€paddingå’Œç»´åº¦å¤„ç†
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Tuple, Optional
import torchaudio
from pathlib import Path
import time
import soundfile as sf
import librosa
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

# å°è¯•å¯¼å…¥AudioLDM2Pipeline
try:
    from New_pipeline_audioldm2 import AudioLDM2Pipeline
except ImportError:
    print("Warning: æ— æ³•å¯¼å…¥New_pipeline_audioldm2ï¼Œå°è¯•ä½¿ç”¨æ ‡å‡†ç®¡é“")
    from diffusers import AudioLDM2Pipeline

class TrainingMatchedVAEReconstructor:
    """
    ä¸è®­ç»ƒç®¡é“å®Œå…¨åŒ¹é…çš„VAEé‡å»ºå™¨
    ä¸¥æ ¼æŒ‰ç…§è®­ç»ƒä»£ç çš„æ•°æ®å¤„ç†æµç¨‹
    """
    
    def __init__(self, model_name: str = "cvssp/audioldm2-music"):
        """åˆå§‹åŒ–VAEé‡å»ºå™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸµ åˆå§‹åŒ–è®­ç»ƒåŒ¹é…VAEé‡å»ºå™¨")
        print(f"   ğŸ“± è®¾å¤‡: {self.device}")
        
        # è®­ç»ƒé…ç½®å‚æ•°ï¼ˆæ¥è‡ª16k_64.yamlï¼‰
        self.sampling_rate = 16000  # è®­ç»ƒé‡‡æ ·ç‡
        self.mel_bins = 64          # melé¢‘æ®µæ•°
        self.mel_fmin = 0           # melæœ€å°é¢‘ç‡
        self.mel_fmax = 8000        # melæœ€å¤§é¢‘ç‡
        self.n_fft = 1024           # FFTçª—å£å¤§å°
        self.hop_length = 160       # è·³è·ƒé•¿åº¦
        self.win_length = 1024      # çª—å£é•¿åº¦
        self.subband = 1            # ä¸ä½¿ç”¨å­é¢‘å¸¦å¤„ç†
        self.duration = 10.24       # è®­ç»ƒéŸ³é¢‘æŒç»­æ—¶é—´
        
        print(f"   ğŸ“Š è®­ç»ƒé…ç½®å‚æ•°:")
        print(f"      é‡‡æ ·ç‡: {self.sampling_rate}Hz")
        print(f"      Mel bins: {self.mel_bins}")
        print(f"      Melé¢‘ç‡èŒƒå›´: {self.mel_fmin}-{self.mel_fmax}Hz")
        print(f"      å­é¢‘å¸¦æ•°: {self.subband}")
        print(f"      éŸ³é¢‘æŒç»­æ—¶é—´: {self.duration}s")
        
        # åŠ è½½AudioLDM2ç®¡é“
        print("ğŸ“¦ åŠ è½½AudioLDM2æ¨¡å‹...")
        self.pipeline = AudioLDM2Pipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)
        
        print(f"âœ… VAEé‡å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š VAE scaling_factor: {self.pipeline.vae.config.scaling_factor}")
    
    def extract_mel_spectrogram(self, audio: np.ndarray) -> torch.Tensor:
        """
        ä½¿ç”¨ä¸è®­ç»ƒå®Œå…¨ä¸€è‡´çš„melé¢‘è°±å›¾æå–æ–¹æ³•
        """
        # ç¡®ä¿éŸ³é¢‘æ˜¯1Dæ•°ç»„
        if audio.ndim > 1:
            audio = audio.flatten()
        
        # è½¬ä¸ºtorchå¼ é‡
        waveform = torch.FloatTensor(audio).unsqueeze(0)  # [1, samples]
        
        # è®¡ç®—ç›®æ ‡é•¿åº¦ï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
        target_length = int(self.duration * self.sampling_rate / self.hop_length)
        
        # ä½¿ç”¨ä¸è®­ç»ƒä»£ç ä¸€è‡´çš„melé¢‘è°±å›¾æå–
        mel_spec, stft_spec = self._mel_spectrogram_train(waveform)
        
        # è½¬ç½®å¹¶paddingåˆ°ç›®æ ‡é•¿åº¦ï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
        log_mel = mel_spec.transpose(-1, -2)  # [batch, time, freq]
        log_mel = self._pad_spec(log_mel, target_length)
        
        # è°ƒæ•´ç»´åº¦ä¸º[batch, channel, time, freq]
        log_mel_tensor = log_mel.unsqueeze(1)  # [1, 1, time, freq]
        
        return log_mel_tensor
    
    def _mel_spectrogram_train(self, y):
        """
        ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´çš„melé¢‘è°±å›¾æå–
        """
        # åˆ›å»ºmelåŸºå’Œhannçª—å£ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not hasattr(self, 'mel_basis'):
            mel = librosa.filters.mel(
                sr=self.sampling_rate,
                n_fft=self.n_fft,
                n_mels=self.mel_bins,
                fmin=self.mel_fmin,
                fmax=self.mel_fmax,
            )
            self.mel_basis = torch.from_numpy(mel).float()
            self.hann_window = torch.hann_window(self.win_length)
        
        # Paddingï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
        y = torch.nn.functional.pad(
            y.unsqueeze(1),
            (
                int((self.n_fft - self.hop_length) / 2),
                int((self.n_fft - self.hop_length) / 2),
            ),
            mode="reflect",
        )
        y = y.squeeze(1)
        
        # STFTï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
        stft_spec = torch.stft(
            y,
            self.n_fft,
            hop_length=self.hop_length,
            win_length=self.win_length,
            window=self.hann_window,
            center=False,
            pad_mode="reflect",
            normalized=False,
            onesided=True,
            return_complex=True,
        )
        
        stft_spec = torch.abs(stft_spec)
        
        # è®¡ç®—melé¢‘è°±å›¾
        mel = torch.matmul(self.mel_basis, stft_spec)
        
        # Spectral normalizationï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
        mel = torch.log(torch.clamp(mel, min=1e-5))
        
        return mel, stft_spec
    
    def _pad_spec(self, log_mel_spec, target_length):
        """
        ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´çš„paddingæ–¹æ³•
        """
        n_frames = log_mel_spec.shape[1]  # timeç»´åº¦
        p = target_length - n_frames
        
        # cut and pad
        if p > 0:
            # padding
            m = torch.nn.ZeroPad2d((0, 0, 0, p))  # (left, right, top, bottom)
            log_mel_spec = m(log_mel_spec)
        elif p < 0:
            # cut
            log_mel_spec = log_mel_spec[:, :target_length, :]
        
        # ç¡®ä¿é¢‘ç‡ç»´åº¦æ˜¯å¶æ•°ï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
        if log_mel_spec.size(-1) % 2 != 0:
            log_mel_spec = log_mel_spec[..., :-1]
        
        return log_mel_spec
    
    def reconstruct_audio(self, audio_path: str) -> Dict:
        """
        å®Œæ•´çš„VAEé‡å»ºæµç¨‹ï¼Œä¸¥æ ¼åŒ¹é…è®­ç»ƒç®¡é“
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            
        Returns:
            result: åŒ…å«é‡å»ºç»“æœå’Œåˆ†æçš„å­—å…¸
        """
        print(f"\nğŸµ è®­ç»ƒåŒ¹é…VAEé‡å»º: {Path(audio_path).name}")
        
        # 1. åŠ è½½å’Œé¢„å¤„ç†è¾“å…¥éŸ³é¢‘
        original_audio, mel_features, input_latent = self._load_and_encode_audio(audio_path)
        
        # 2. VAEè§£ç é‡å»º
        reconstructed_mel, reconstructed_audio = self._decode_latent_to_audio(input_latent)
        
        # 3. ä¿å­˜ç»“æœå’Œåˆ†æ
        result = self._save_and_analyze(
            original_audio, reconstructed_audio, mel_features, reconstructed_mel, audio_path
        )
        
        # 4. æ˜¾ç¤ºç»“æœ
        self._display_results(result)
        
        return result
    
    def _load_and_encode_audio(self, audio_path: str) -> Tuple[np.ndarray, torch.Tensor, torch.Tensor]:
        """åŠ è½½éŸ³é¢‘å¹¶ç¼–ç ä¸ºlatentï¼Œä¸¥æ ¼åŒ¹é…è®­ç»ƒæµç¨‹"""
        print(f"   ğŸ“ åŠ è½½éŸ³é¢‘...")
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = torchaudio.load(audio_path)
        
        # é‡é‡‡æ ·åˆ°è®­ç»ƒé‡‡æ ·ç‡ (16kHz)
        if sr != self.sampling_rate:
            print(f"   ğŸ”„ é‡é‡‡æ ·: {sr}Hz -> {self.sampling_rate}Hz")
            resampler = torchaudio.transforms.Resample(sr, self.sampling_rate)
            audio = resampler(audio)
        
        # è½¬ä¸ºå•å£°é“
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # é™åˆ¶é•¿åº¦åˆ°è®­ç»ƒé…ç½®æ—¶é•¿
        max_length = int(self.duration * self.sampling_rate)
        if audio.shape[-1] > max_length:
            audio = audio[..., :max_length]
            print(f"   âœ‚ï¸ æˆªå–åˆ°{self.duration}ç§’")
        
        # è½¬ä¸ºnumpyç”¨äºmelæå–
        audio_np = audio.squeeze(0).numpy()
        
        print(f"   âœ… éŸ³é¢‘é¢„å¤„ç†å®Œæˆ: {len(audio_np)}æ ·æœ¬ ({len(audio_np)/self.sampling_rate:.2f}ç§’)")
        
        # ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„melé¢‘è°±å›¾æå–
        print(f"   ğŸ”„ æå–melé¢‘è°±å›¾...")
        mel_features = self.extract_mel_spectrogram(audio_np)
        
        print(f"   ğŸ“Š Melç‰¹å¾: {mel_features.shape}")
        print(f"   ğŸ“Š MelèŒƒå›´: [{mel_features.min():.3f}, {mel_features.max():.3f}]")
        
        # VAEç¼–ç 
        print(f"   ğŸ”„ VAEç¼–ç ...")
        with torch.no_grad():
            # ç§»åˆ°è®¾å¤‡å¹¶è½¬æ¢æ•°æ®ç±»å‹
            mel_features = mel_features.to(self.device)
            if self.device == "cuda":
                mel_features = mel_features.half()
            
            # VAEç¼–ç ï¼ˆä¸éœ€è¦å­é¢‘å¸¦å¤„ç†ï¼Œå› ä¸ºsubband=1ï¼‰
            latent_dist = self.pipeline.vae.encode(mel_features)
            
            # è·å–latentï¼ˆä½¿ç”¨modeè€Œä¸æ˜¯sampleï¼‰
            if hasattr(latent_dist, 'latent_dist'):
                latent = latent_dist.latent_dist.mode()
            else:
                latent = latent_dist.mode()
            
            # åº”ç”¨scaling factor
            latent = latent * self.pipeline.vae.config.scaling_factor
            
            print(f"   âœ… VAEç¼–ç å®Œæˆ: {latent.shape}")
            print(f"   ğŸ“Š LatentèŒƒå›´: [{latent.min():.3f}, {latent.max():.3f}]")
            print(f"   ğŸ“Š Latent std: {latent.std():.3f}")
        
        return audio_np, mel_features, latent
    
    def _decode_latent_to_audio(self, latent: torch.Tensor) -> Tuple[torch.Tensor, np.ndarray]:
        """å°†latentè§£ç ä¸ºéŸ³é¢‘ï¼Œä¸¥æ ¼åŒ¹é…è®­ç»ƒæµç¨‹"""
        print(f"   ğŸ”„ VAEè§£ç ...")
        
        with torch.no_grad():
            # å‡†å¤‡è§£ç 
            latent_for_decode = latent / self.pipeline.vae.config.scaling_factor
            
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            vae_dtype = next(self.pipeline.vae.parameters()).dtype
            latent_for_decode = latent_for_decode.to(vae_dtype)
            
            print(f"   ğŸ“Š è§£ç latent: {latent_for_decode.shape}, {latent_for_decode.dtype}")
            print(f"   ğŸ“Š è§£ç latentèŒƒå›´: [{latent_for_decode.min():.3f}, {latent_for_decode.max():.3f}]")
            
            # VAEè§£ç 
            reconstructed_mel = self.pipeline.vae.decode(latent_for_decode).sample
            
            print(f"   ğŸ“Š é‡å»ºmel: {reconstructed_mel.shape}")
            print(f"   ğŸ“Š é‡å»ºmelèŒƒå›´: [{reconstructed_mel.min():.3f}, {reconstructed_mel.max():.3f}]")
            
            # ä½¿ç”¨vocoderè½¬æ¢ä¸ºéŸ³é¢‘
            audio_tensor = self.pipeline.mel_spectrogram_to_waveform(reconstructed_mel)
            audio = audio_tensor.squeeze().cpu().numpy()
            
            print(f"   âœ… VAEè§£ç å®Œæˆ: {len(audio)}æ ·æœ¬ ({len(audio)/16000:.2f}ç§’)")
            
        return reconstructed_mel, audio
    
    def _save_and_analyze(self, original_audio: np.ndarray, reconstructed_audio: np.ndarray,
                         original_mel: torch.Tensor, reconstructed_mel: torch.Tensor,
                         audio_path: str) -> Dict:
        """ä¿å­˜ç»“æœå¹¶è¿›è¡Œå…¨é¢è´¨é‡åˆ†æ"""
        print(f"   ğŸ’¾ ä¿å­˜ç»“æœå’Œè´¨é‡åˆ†æ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("step_1_training_matched_vae_reconstruction_fixed")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = int(time.time())
        input_name = Path(audio_path).stem
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(original_audio), len(reconstructed_audio))
        original_audio_trimmed = original_audio[:min_len]
        reconstructed_audio_trimmed = reconstructed_audio[:min_len]
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        original_path = output_dir / f"{input_name}_original_{timestamp}.wav"
        reconstructed_path = output_dir / f"{input_name}_training_matched_reconstructed_{timestamp}.wav"
        
        sf.write(str(original_path), original_audio_trimmed, self.sampling_rate)
        sf.write(str(reconstructed_path), reconstructed_audio_trimmed, self.sampling_rate)
        
        # è®¡ç®—éŸ³é¢‘è´¨é‡æŒ‡æ ‡
        snr = self._calculate_snr(original_audio_trimmed, reconstructed_audio_trimmed)
        mse = np.mean((original_audio_trimmed - reconstructed_audio_trimmed) ** 2)
        
        # è®¡ç®—é¢‘åŸŸåˆ†æ
        freq_analysis = self._analyze_frequency_content(original_audio_trimmed, reconstructed_audio_trimmed)
        
        # è®¡ç®—melé¢‘è°±å›¾å·®å¼‚ï¼ˆä½¿ç”¨è®­ç»ƒæ–¹å¼çš„paddingï¼‰
        mel_analysis = self._analyze_mel_difference_training_way(original_mel, reconstructed_mel)
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        plot_path = self._generate_analysis_plots(
            original_audio_trimmed, reconstructed_audio_trimmed,
            original_mel, reconstructed_mel,
            output_dir, input_name, timestamp
        )
        
        # ç»„ç»‡ç»“æœ
        result = {
            "input_path": audio_path,
            "output_dir": str(output_dir),
            "original_path": str(original_path),
            "reconstructed_path": str(reconstructed_path),
            "plot_path": str(plot_path),
            "audio_metrics": {
                "snr_db": snr,
                "mse": mse,
                "original_length": len(original_audio_trimmed),
                "reconstructed_length": len(reconstructed_audio_trimmed),
                "sampling_rate": self.sampling_rate
            },
            "frequency_analysis": freq_analysis,
            "mel_analysis": mel_analysis
        }
        
        return result
    
    def _calculate_snr(self, original: np.ndarray, reconstructed: np.ndarray) -> float:
        """è®¡ç®—ä¿¡å™ªæ¯”"""
        signal_power = np.mean(original ** 2)
        noise_power = np.mean((original - reconstructed) ** 2)
        if noise_power == 0:
            return float('inf')
        snr = 10 * np.log10(signal_power / noise_power)
        return snr
    
    def _analyze_frequency_content(self, original: np.ndarray, reconstructed: np.ndarray) -> Dict:
        """åˆ†æé¢‘åŸŸå†…å®¹"""
        # è®¡ç®—é¢‘è°±
        orig_fft = np.abs(np.fft.rfft(original))
        recon_fft = np.abs(np.fft.rfft(reconstructed))
        
        # é¢‘ç‡è½´
        freqs = np.fft.rfftfreq(len(original), 1/self.sampling_rate)
        
        # è®¡ç®—ä¸åŒé¢‘æ®µçš„èƒ½é‡æŸå¤±
        nyquist = self.sampling_rate // 2
        low_freq_mask = freqs <= nyquist * 0.25  # 0-2kHz
        mid_freq_mask = (freqs > nyquist * 0.25) & (freqs <= nyquist * 0.5)  # 2-4kHz
        high_freq_mask = freqs > nyquist * 0.5  # 4-8kHz
        
        def energy_ratio(mask):
            orig_energy = np.sum(orig_fft[mask] ** 2)
            recon_energy = np.sum(recon_fft[mask] ** 2)
            return recon_energy / orig_energy if orig_energy > 0 else 0
        
        return {
            "low_freq_ratio": energy_ratio(low_freq_mask),
            "mid_freq_ratio": energy_ratio(mid_freq_mask),
            "high_freq_ratio": energy_ratio(high_freq_mask),
            "spectral_correlation": np.corrcoef(orig_fft, recon_fft)[0, 1]
        }
    
    def _analyze_mel_difference_training_way(self, original_mel: torch.Tensor, reconstructed_mel: torch.Tensor) -> Dict:
        """
        ä½¿ç”¨è®­ç»ƒä»£ç çš„æ–¹å¼åˆ†æmelé¢‘è°±å›¾å·®å¼‚
        éƒ½åº”è¯¥å·²ç»paddingåˆ°ç›¸åŒçš„target_length
        """
        # è½¬ä¸ºnumpy
        orig_mel_np = original_mel.squeeze().cpu().numpy()
        recon_mel_np = reconstructed_mel.squeeze().cpu().numpy()
        
        print(f"   ğŸ“Š Melåˆ†æç»´åº¦: åŸå§‹{orig_mel_np.shape}, é‡å»º{recon_mel_np.shape}")
        
        # å¦‚æœç»´åº¦ä»ç„¶ä¸åŒ¹é…ï¼Œä½¿ç”¨è®­ç»ƒä»£ç çš„paddingæ–¹æ³•
        if orig_mel_np.shape != recon_mel_np.shape:
            print(f"   âš ï¸ Melç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨è®­ç»ƒæ–¹å¼padding...")
            target_length = int(self.duration * self.sampling_rate / self.hop_length)
            
            # å¯¹åŸå§‹melåº”ç”¨ç›¸åŒçš„padding
            orig_mel_tensor = torch.from_numpy(orig_mel_np).unsqueeze(0)  # [1, time, freq]
            orig_mel_tensor = self._pad_spec(orig_mel_tensor, target_length)
            orig_mel_np = orig_mel_tensor.squeeze(0).numpy()
            
            # å¯¹é‡å»ºmelä¹Ÿåº”ç”¨paddingï¼ˆå¦‚æœéœ€è¦ï¼‰
            recon_mel_tensor = torch.from_numpy(recon_mel_np).unsqueeze(0)  # [1, time, freq]  
            recon_mel_tensor = self._pad_spec(recon_mel_tensor, target_length)
            recon_mel_np = recon_mel_tensor.squeeze(0).numpy()
            
            print(f"   ğŸ“Š Paddingåç»´åº¦: åŸå§‹{orig_mel_np.shape}, é‡å»º{recon_mel_np.shape}")
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        mse = np.mean((orig_mel_np - recon_mel_np) ** 2)
        mae = np.mean(np.abs(orig_mel_np - recon_mel_np))
        correlation = np.corrcoef(orig_mel_np.flatten(), recon_mel_np.flatten())[0, 1]
        
        # è®¡ç®—ä¸åŒé¢‘æ®µçš„è¯¯å·®
        n_mels = orig_mel_np.shape[-1]
        low_mel_error = np.mean((orig_mel_np[..., :n_mels//3] - recon_mel_np[..., :n_mels//3]) ** 2)
        mid_mel_error = np.mean((orig_mel_np[..., n_mels//3:2*n_mels//3] - recon_mel_np[..., n_mels//3:2*n_mels//3]) ** 2)
        high_mel_error = np.mean((orig_mel_np[..., 2*n_mels//3:] - recon_mel_np[..., 2*n_mels//3:]) ** 2)
        
        return {
            "mel_mse": mse,
            "mel_mae": mae,
            "mel_correlation": correlation,
            "low_mel_error": low_mel_error,
            "mid_mel_error": mid_mel_error,
            "high_mel_error": high_mel_error,
            "time_frames_original": orig_mel_np.shape[0],
            "time_frames_reconstructed": recon_mel_np.shape[0],
            "padding_applied": orig_mel_np.shape != recon_mel_np.shape
        }
    
    def _generate_analysis_plots(self, original_audio: np.ndarray, reconstructed_audio: np.ndarray,
                               original_mel: torch.Tensor, reconstructed_mel: torch.Tensor,
                               output_dir: Path, input_name: str, timestamp: int) -> str:
        """ç”Ÿæˆç»¼åˆåˆ†æå›¾è¡¨"""
        
        fig, axes = plt.subplots(3, 2, figsize=(15, 12))
        fig.suptitle(f'analyzing graph - {input_name}', fontsize=16)
        
        # 1. æ³¢å½¢å¯¹æ¯”
        time_axis = np.arange(len(original_audio)) / self.sampling_rate
        axes[0, 0].plot(time_axis, original_audio, label='original', alpha=0.7)
        axes[0, 0].plot(time_axis, reconstructed_audio, label='reconstructed', alpha=0.7)
        axes[0, 0].set_title('waveform comparison')
        axes[0, 0].set_xlabel('time (s)')
        axes[0, 0].set_ylabel('amplitude')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # 2. è¯¯å·®æ³¢å½¢
        error = original_audio - reconstructed_audio
        axes[0, 1].plot(time_axis, error)
        axes[0, 1].set_title('waveform error')
        axes[0, 1].set_xlabel('time (s)')
        axes[0, 1].set_ylabel('error amplitude')
        axes[0, 1].grid(True)
        
        # 3. é¢‘è°±å¯¹æ¯”
        orig_fft = np.abs(np.fft.rfft(original_audio))
        recon_fft = np.abs(np.fft.rfft(reconstructed_audio))
        freqs = np.fft.rfftfreq(len(original_audio), 1/self.sampling_rate)
        
        axes[1, 0].semilogy(freqs, orig_fft, label='original', alpha=0.7)
        axes[1, 0].semilogy(freqs, recon_fft, label='reconstructed', alpha=0.7)
        axes[1, 0].set_title('spectrum comparison')
        axes[1, 0].set_xlabel('frequency (Hz)')
        axes[1, 0].set_ylabel('magnitude')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # 4. é¢‘è°±è¯¯å·®
        freq_error = np.abs(orig_fft - recon_fft)
        axes[1, 1].semilogy(freqs, freq_error)
        axes[1, 1].set_title('spectrum error')
        axes[1, 1].set_xlabel('frequency (Hz)')
        axes[1, 1].set_ylabel('error magnitude')
        axes[1, 1].grid(True)
        
        # 5. & 6. Melé¢‘è°±å›¾å¯¹æ¯”ï¼ˆä½¿ç”¨è®­ç»ƒæ–¹å¼å¤„ç†ç»´åº¦ï¼‰
        orig_mel_np = original_mel.squeeze().cpu().numpy()
        recon_mel_np = reconstructed_mel.squeeze().cpu().numpy()
        
        # ç¡®ä¿ç»´åº¦åŒ¹é…ï¼ˆä½¿ç”¨è®­ç»ƒæ–¹å¼çš„paddingï¼‰
        if orig_mel_np.shape != recon_mel_np.shape:
            target_length = int(self.duration * self.sampling_rate / self.hop_length)
            
            orig_mel_tensor = torch.from_numpy(orig_mel_np).unsqueeze(0)
            orig_mel_tensor = self._pad_spec(orig_mel_tensor, target_length)
            orig_mel_np = orig_mel_tensor.squeeze(0).numpy()
            
            recon_mel_tensor = torch.from_numpy(recon_mel_np).unsqueeze(0)
            recon_mel_tensor = self._pad_spec(recon_mel_tensor, target_length)
            recon_mel_np = recon_mel_tensor.squeeze(0).numpy()
        
        im1 = axes[2, 0].imshow(orig_mel_np.T, aspect='auto', origin='lower', cmap='viridis')
        axes[2, 0].set_title('original Mel spectrogram')
        axes[2, 0].set_xlabel('time frames')
        axes[2, 0].set_ylabel('Mel frequency bins')
        plt.colorbar(im1, ax=axes[2, 0])
        
        im2 = axes[2, 1].imshow(recon_mel_np.T, aspect='auto', origin='lower', cmap='viridis')
        axes[2, 1].set_title('reconstructed Mel spectrogram')
        axes[2, 1].set_xlabel('time frames')
        axes[2, 1].set_ylabel('Mel frequency bins')
        plt.colorbar(im2, ax=axes[2, 1])
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = output_dir / f"{input_name}_training_matched_analysis_fixed_{timestamp}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return plot_path
    
    def _display_results(self, result: Dict):
        """æ˜¾ç¤ºåˆ†æç»“æœ"""
        print(f"\nğŸ“Š è®­ç»ƒåŒ¹é…VAEé‡å»ºåˆ†æç»“æœï¼ˆä¿®æ­£ç‰ˆï¼‰:")
        print(f"   ğŸ“ è¾“å‡ºç›®å½•: {result['output_dir']}")
        
        metrics = result["audio_metrics"]
        print(f"   ğŸµ éŸ³é¢‘è´¨é‡:")
        print(f"      SNR: {metrics['snr_db']:.2f} dB")
        print(f"      MSE: {metrics['mse']:.6f}")
        
        freq_analysis = result["frequency_analysis"]
        print(f"   ğŸ“Š é¢‘åŸŸåˆ†æ:")
        print(f"      ä½é¢‘ä¿æŒ: {freq_analysis['low_freq_ratio']:.3f}")
        print(f"      ä¸­é¢‘ä¿æŒ: {freq_analysis['mid_freq_ratio']:.3f}")
        print(f"      é«˜é¢‘ä¿æŒ: {freq_analysis['high_freq_ratio']:.3f}")
        print(f"      é¢‘è°±ç›¸å…³æ€§: {freq_analysis['spectral_correlation']:.3f}")
        
        mel_analysis = result["mel_analysis"]
        print(f"   ğŸ¼ Melé¢‘è°±åˆ†æ:")
        print(f"      Mel MSE: {mel_analysis['mel_mse']:.6f}")
        print(f"      Melç›¸å…³æ€§: {mel_analysis['mel_correlation']:.3f}")
        print(f"      ä½é¢‘æ®µè¯¯å·®: {mel_analysis['low_mel_error']:.6f}")
        print(f"      ä¸­é¢‘æ®µè¯¯å·®: {mel_analysis['mid_mel_error']:.6f}")
        print(f"      é«˜é¢‘æ®µè¯¯å·®: {mel_analysis['high_mel_error']:.6f}")
        print(f"      æ—¶é—´å¸§æ•°: åŸå§‹{mel_analysis['time_frames_original']}, é‡å»º{mel_analysis['time_frames_reconstructed']}")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œè®­ç»ƒåŒ¹é…çš„VAEé‡å»ºæµ‹è¯•"""
    
    # åˆå§‹åŒ–é‡å»ºå™¨
    reconstructor = TrainingMatchedVAEReconstructor()
    
    # æµ‹è¯•éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    test_audio_path = "AudioLDM2_Music_output.wav"  # ä½¿ç”¨ç°æœ‰çš„æµ‹è¯•éŸ³é¢‘
    
    if not Path(test_audio_path).exists():
        print(f"âŒ æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {test_audio_path}")
        print("è¯·å°†æµ‹è¯•éŸ³é¢‘æ–‡ä»¶æ”¾åœ¨å½“å‰ç›®å½•ä¸‹ï¼Œæˆ–ä¿®æ”¹test_audio_pathå˜é‡")
        return
    
    try:
        # è¿è¡Œé‡å»ºæµ‹è¯•
        result = reconstructor.reconstruct_audio(test_audio_path)
        
        print(f"\nâœ… è®­ç»ƒåŒ¹é…VAEé‡å»ºæµ‹è¯•å®Œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰!")
        print(f"ğŸ“ æŸ¥çœ‹ç»“æœ: {result['output_dir']}")
        print(f"ğŸµ åŸå§‹éŸ³é¢‘: {result['original_path']}")
        print(f"ğŸµ é‡å»ºéŸ³é¢‘: {result['reconstructed_path']}")
        print(f"ğŸ“Š åˆ†æå›¾è¡¨: {result['plot_path']}")
        
        # å…³é”®æ”¹è¿›è¯´æ˜
        print(f"\nğŸ”§ å…³é”®ä¿®æ­£:")
        print(f"   âœ… ä½¿ç”¨è®­ç»ƒä»£ç çš„melé¢‘è°±å›¾æå–æ–¹æ³•")
        print(f"   âœ… ä½¿ç”¨è®­ç»ƒä»£ç çš„paddingç­–ç•¥")
        print(f"   âœ… ä¸¥æ ¼åŒ¹é…è®­ç»ƒé…ç½®å‚æ•°")
        print(f"   âœ… ä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜")
        
    except Exception as e:
        print(f"âŒ é‡å»ºè¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
